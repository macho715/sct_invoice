"""Risk Score Weight Testing Tool

Ïó¨Îü¨ Í∞ÄÏ§ëÏπò ÏÑ§Ï†ïÏùÑ ÌÖåÏä§Ìä∏ÌïòÍ≥† Í≤∞Í≥ºÎ•º ÎπÑÍµêÌïòÎäî ÎèÑÍµ¨ÏûÖÎãàÎã§.
"""

import pandas as pd
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
import os
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)


class RiskWeightTester:
    """Risk Score Weight ÌÖåÏä§Ìä∏ ÌÅ¥ÎûòÏä§"""

    def __init__(self, results_dir: str = "Results/Sept_2025/CSV"):
        self.results_dir = results_dir
        self.test_results = {}

    def test_weight_configurations(
        self, validation_data: pd.DataFrame, weight_configs: List[Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        Ïó¨Îü¨ Í∞ÄÏ§ëÏπò ÏÑ§Ï†ïÏùÑ ÌÖåÏä§Ìä∏ÌïòÍ≥† Í≤∞Í≥º ÎπÑÍµê

        Args:
            validation_data: Í≤ÄÏ¶ù Í≤∞Í≥º DataFrame
            weight_configs: ÌÖåÏä§Ìä∏Ìï† Í∞ÄÏ§ëÏπò ÏÑ§Ï†ï Î¶¨Ïä§Ìä∏

        Returns:
            ÎπÑÍµê Í≤∞Í≥º DataFrame (ÏÑ§Ï†ïÎ≥Ñ Ï†ïÌôïÎèÑ, FP/FN rate)
        """

        logger.info(f"Testing {len(weight_configs)} weight configurations")

        comparison_results = []

        for i, config in enumerate(weight_configs):
            config_name = config.get("name", f"config_{i+1}")
            weights = config.get("weights", {})
            threshold = config.get("trigger_threshold", 0.8)

            logger.info(f"Testing configuration: {config_name}")

            # Ìï¥Îãπ ÏÑ§Ï†ïÏúºÎ°ú Î¶¨Ïä§ÌÅ¨ Ï†êÏàò Ïû¨Í≥ÑÏÇ∞
            recalculated_scores = self._recalculate_risk_scores(
                validation_data, weights, threshold
            )

            # ÏÑ±Îä• ÏßÄÌëú Í≥ÑÏÇ∞
            performance_metrics = self._calculate_performance_metrics(
                validation_data, recalculated_scores, threshold
            )

            result = {
                "config_name": config_name,
                "weights": weights,
                "trigger_threshold": threshold,
                "performance_metrics": performance_metrics,
                "recalculated_scores": recalculated_scores,
            }

            comparison_results.append(result)

        # Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò
        comparison_df = self._create_comparison_dataframe(comparison_results)

        return comparison_df

    def _recalculate_risk_scores(
        self, df: pd.DataFrame, weights: Dict[str, float], threshold: float
    ) -> pd.Series:
        """Í∞ÄÏ§ëÏπò ÏÑ§Ï†ïÏúºÎ°ú Î¶¨Ïä§ÌÅ¨ Ï†êÏàò Ïû¨Í≥ÑÏÇ∞"""

        # Í∏∞Î≥∏ Í∞ÄÏ§ëÏπò (ÌòÑÏû¨ ÏÑ§Ï†ï)
        default_weights = {
            "delta": 0.4,
            "anomaly": 0.3,
            "certification": 0.2,
            "signature": 0.1,
        }

        # Í∞ÄÏ§ëÏπò ÏóÖÎç∞Ïù¥Ìä∏
        final_weights = {**default_weights, **weights}

        # Í∞ÄÏ§ëÏπò Ìï©Í≥Ñ Í≤ÄÏ¶ù
        weight_sum = sum(final_weights.values())
        if abs(weight_sum - 1.0) > 0.001:
            logger.warning(f"Weight sum is {weight_sum}, normalizing to 1.0")
            final_weights = {k: v / weight_sum for k, v in final_weights.items()}

        # Î¶¨Ïä§ÌÅ¨ Ï†êÏàò Ïû¨Í≥ÑÏÇ∞
        risk_scores = []

        for _, row in df.iterrows():
            # Í∞Å Íµ¨ÏÑ± ÏöîÏÜå Ï†êÏàò Ï∂îÏ∂ú
            delta_score = self._extract_component_score(row, "delta")
            anomaly_score = self._extract_component_score(row, "anomaly")
            cert_score = self._extract_component_score(row, "certification")
            sig_score = self._extract_component_score(row, "signature")

            # Í∞ÄÏ§ë ÌèâÍ∑† Í≥ÑÏÇ∞
            risk_score = (
                final_weights["delta"] * delta_score
                + final_weights["anomaly"] * anomaly_score
                + final_weights["certification"] * cert_score
                + final_weights["signature"] * sig_score
            )

            risk_scores.append(risk_score)

        return pd.Series(risk_scores, index=df.index)

    def _extract_component_score(self, row: pd.Series, component: str) -> float:
        """risk_componentsÏóêÏÑú Í∞úÎ≥Ñ Íµ¨ÏÑ± ÏöîÏÜå Ï†êÏàò Ï∂îÏ∂ú"""

        try:
            risk_components = row.get("risk_components", {})

            if isinstance(risk_components, str):
                components = json.loads(risk_components)
            elif isinstance(risk_components, dict):
                components = risk_components
            else:
                return 0.0

            return float(components.get(component, 0.0))

        except (json.JSONDecodeError, TypeError, ValueError, KeyError):
            return 0.0

    def _calculate_performance_metrics(
        self, df: pd.DataFrame, new_risk_scores: pd.Series, threshold: float
    ) -> Dict[str, float]:
        """ÏÑ±Îä• ÏßÄÌëú Í≥ÑÏÇ∞"""

        # ÌòÑÏû¨ Î¶¨Ïä§ÌÅ¨ Ìä∏Î¶¨Í±∞ ÏÉÅÌÉú
        current_triggered = df["risk_triggered"].fillna(False)

        # ÏÉàÎ°úÏö¥ Î¶¨Ïä§ÌÅ¨ Ìä∏Î¶¨Í±∞ ÏÉÅÌÉú (threshold Í∏∞Î∞ò)
        new_triggered = new_risk_scores >= threshold

        # Ïã§Ï†ú ÏÉÅÌÉú (Í≤ÄÏ¶ù Í≤∞Í≥º Í∏∞Î∞ò)
        actual_status = df["status"].fillna("UNKNOWN")
        is_high_risk = actual_status.isin(["ERROR", "FAIL", "REVIEW_NEEDED"])

        # True Positive: Ïò¨Î∞îÎ•¥Í≤å ÏúÑÌóòÏúºÎ°ú ÏãùÎ≥Ñ
        tp = ((new_triggered == True) & (is_high_risk == True)).sum()

        # False Positive: ÏûòÎ™ª ÏúÑÌóòÏúºÎ°ú ÏãùÎ≥Ñ
        fp = ((new_triggered == True) & (is_high_risk == False)).sum()

        # True Negative: Ïò¨Î∞îÎ•¥Í≤å ÏïàÏ†ÑÏúºÎ°ú ÏãùÎ≥Ñ
        tn = ((new_triggered == False) & (is_high_risk == False)).sum()

        # False Negative: ÏûòÎ™ª ÏïàÏ†ÑÏúºÎ°ú ÏãùÎ≥Ñ
        fn = ((new_triggered == False) & (is_high_risk == True)).sum()

        # ÏÑ±Îä• ÏßÄÌëú Í≥ÑÏÇ∞
        total = len(df)

        metrics = {
            "total_items": total,
            "true_positive": int(tp),
            "false_positive": int(fp),
            "true_negative": int(tn),
            "false_negative": int(fn),
            "false_positive_rate": float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0,
            "false_negative_rate": float(fn / (fn + tp)) if (fn + tp) > 0 else 0.0,
            "precision": float(tp / (tp + fp)) if (tp + fp) > 0 else 0.0,
            "recall": float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0,
            "accuracy": float((tp + tn) / total) if total > 0 else 0.0,
            "f1_score": 0.0,
        }

        # F1 Score Í≥ÑÏÇ∞
        if metrics["precision"] > 0 and metrics["recall"] > 0:
            metrics["f1_score"] = (
                2
                * (metrics["precision"] * metrics["recall"])
                / (metrics["precision"] + metrics["recall"])
            )

        return metrics

    def _create_comparison_dataframe(
        self, comparison_results: List[Dict]
    ) -> pd.DataFrame:
        """ÎπÑÍµê Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò"""

        comparison_data = []

        for result in comparison_results:
            config_name = result["config_name"]
            weights = result["weights"]
            threshold = result["trigger_threshold"]
            metrics = result["performance_metrics"]

            row = {
                "config_name": config_name,
                "delta_weight": weights.get("delta", 0.4),
                "anomaly_weight": weights.get("anomaly", 0.3),
                "certification_weight": weights.get("certification", 0.2),
                "signature_weight": weights.get("signature", 0.1),
                "trigger_threshold": threshold,
                "total_items": metrics["total_items"],
                "true_positive": metrics["true_positive"],
                "false_positive": metrics["false_positive"],
                "true_negative": metrics["true_negative"],
                "false_negative": metrics["false_negative"],
                "false_positive_rate": metrics["false_positive_rate"],
                "false_negative_rate": metrics["false_negative_rate"],
                "precision": metrics["precision"],
                "recall": metrics["recall"],
                "accuracy": metrics["accuracy"],
                "f1_score": metrics["f1_score"],
            }

            comparison_data.append(row)

        return pd.DataFrame(comparison_data)

    def generate_comparison_report(
        self, comparison_df: pd.DataFrame, output_path: str = None
    ) -> str:
        """ÎπÑÍµê Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""

        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"RISK_WEIGHT_COMPARISON_REPORT_{timestamp}.md"

        # Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        report_content = self._create_comparison_report_content(comparison_df)

        # ÌååÏùº Ï†ÄÏû•
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info(f"Comparison report saved: {output_path}")
        return output_path

    def _create_comparison_report_content(self, comparison_df: pd.DataFrame) -> str:
        """ÎπÑÍµê Î≥¥Í≥†ÏÑú ÎÇ¥Ïö© ÏÉùÏÑ±"""

        report = f"""# Risk Score Weight Configuration Comparison Report

**ÏÉùÏÑ± ÏùºÏãú**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï Ïàò**: {len(comparison_df)}
**ÌÖåÏä§Ìä∏ Ìï≠Î™© Ïàò**: {comparison_df['total_items'].iloc[0] if len(comparison_df) > 0 else 0}

---

## üìä ÏÑ§Ï†ïÎ≥Ñ ÏÑ±Îä• ÎπÑÍµê

| ÏÑ§Ï†ïÎ™Ö | Delta | Anomaly | Cert | Sig | Threshold | FPR | FNR | Precision | Recall | F1 | Accuracy |
|--------|-------|---------|------|-----|-----------|-----|-----|-----------|--------|----|-----------|
"""

        for _, row in comparison_df.iterrows():
            report += f"| {row['config_name']} | {row['delta_weight']:.2f} | {row['anomaly_weight']:.2f} | {row['certification_weight']:.2f} | {row['signature_weight']:.2f} | {row['trigger_threshold']:.2f} | {row['false_positive_rate']:.1%} | {row['false_negative_rate']:.1%} | {row['precision']:.3f} | {row['recall']:.3f} | {row['f1_score']:.3f} | {row['accuracy']:.3f} |\n"

        # ÏµúÍ≥† ÏÑ±Îä• ÏÑ§Ï†ï Ï∞æÍ∏∞
        best_f1 = comparison_df.loc[comparison_df["f1_score"].idxmax()]
        best_accuracy = comparison_df.loc[comparison_df["accuracy"].idxmax()]
        best_low_fpr = comparison_df.loc[comparison_df["false_positive_rate"].idxmin()]

        report += f"""
---

## üèÜ ÏµúÍ≥† ÏÑ±Îä• ÏÑ§Ï†ï

### ÏµúÍ≥† F1 Score
- **ÏÑ§Ï†ï**: {best_f1['config_name']}
- **F1 Score**: {best_f1['f1_score']:.3f}
- **Í∞ÄÏ§ëÏπò**: Delta={best_f1['delta_weight']:.2f}, Anomaly={best_f1['anomaly_weight']:.2f}, Cert={best_f1['certification_weight']:.2f}, Sig={best_f1['signature_weight']:.2f}
- **Threshold**: {best_f1['trigger_threshold']:.2f}

### ÏµúÍ≥† Accuracy
- **ÏÑ§Ï†ï**: {best_accuracy['config_name']}
- **Accuracy**: {best_accuracy['accuracy']:.3f}
- **FPR**: {best_accuracy['false_positive_rate']:.1%}
- **FNR**: {best_accuracy['false_negative_rate']:.1%}

### ÏµúÏ†Ä False Positive Rate
- **ÏÑ§Ï†ï**: {best_low_fpr['config_name']}
- **FPR**: {best_low_fpr['false_positive_rate']:.1%}
- **FNR**: {best_low_fpr['false_negative_rate']:.1%}
- **Trade-off**: FPR Í∞êÏÜåÎ°ú Ïù∏Ìïú FNR Ï¶ùÍ∞Ä

---

## üìà ÏÉÅÏÑ∏ Î∂ÑÏÑù

### ÏÑ±Îä• ÏßÄÌëú Î∂ÑÌè¨

"""

        # ÏÑ±Îä• ÏßÄÌëú ÌÜµÍ≥Ñ
        metrics_stats = comparison_df[
            [
                "false_positive_rate",
                "false_negative_rate",
                "precision",
                "recall",
                "accuracy",
                "f1_score",
            ]
        ].describe()

        for metric in [
            "false_positive_rate",
            "false_negative_rate",
            "precision",
            "recall",
            "accuracy",
            "f1_score",
        ]:
            mean_val = metrics_stats.loc["mean", metric]
            std_val = metrics_stats.loc["std", metric]
            min_val = metrics_stats.loc["min", metric]
            max_val = metrics_stats.loc["max", metric]

            report += f"- **{metric.replace('_', ' ').title()}**: ÌèâÍ∑† {mean_val:.3f} (¬±{std_val:.3f}), Î≤îÏúÑ {min_val:.3f}-{max_val:.3f}\n"

        # Í∂åÏû•ÏÇ¨Ìï≠
        report += f"""
---

## üéØ Í∂åÏû•ÏÇ¨Ìï≠

### Ï¶âÏãú Ï†ÅÏö© Í∞ÄÎä•Ìïú ÏÑ§Ï†ï
"""

        # F1 Score Í∏∞Ï§Ä ÏÉÅÏúÑ 3Í∞ú ÏÑ§Ï†ï
        top_configs = comparison_df.nlargest(3, "f1_score")

        for i, (_, config) in enumerate(top_configs.iterrows(), 1):
            report += f"""
#### {i}ÏàúÏúÑ: {config['config_name']}
- **F1 Score**: {config['f1_score']:.3f}
- **Accuracy**: {config['accuracy']:.3f}
- **FPR**: {config['false_positive_rate']:.1%}
- **FNR**: {config['false_negative_rate']:.1%}
- **Í∂åÏû• Ïù¥Ïú†**: Í∑†ÌòïÏû°Ìûå ÏÑ±Îä•Í≥º ÎÇÆÏùÄ Ïò§Î•òÏú®
"""

        # ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ Ï∂îÏ≤ú
        report += f"""
### ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ Ï∂îÏ≤ú

#### Í≥ÑÏïΩ Ï§ÄÏàò Ï§ëÏãú
- **Ï∂îÏ≤ú ÏÑ§Ï†ï**: Delta Í∞ÄÏ§ëÏπòÍ∞Ä ÎÜíÏùÄ ÏÑ§Ï†ï
- **Í∏∞Ï§Ä**: Delta weight > 0.4, FPR < 5%

#### Ïù¥ÏÉÅ Ìå®ÌÑ¥ ÌÉêÏßÄ Ï§ëÏãú
- **Ï∂îÏ≤ú ÏÑ§Ï†ï**: Anomaly Í∞ÄÏ§ëÏπòÍ∞Ä ÎÜíÏùÄ ÏÑ§Ï†ï
- **Í∏∞Ï§Ä**: Anomaly weight > 0.35, FNR < 10%

#### Í∑†ÌòïÌòï Ïö¥ÏòÅ
- **Ï∂îÏ≤ú ÏÑ§Ï†ï**: F1 ScoreÍ∞Ä ÎÜíÏùÄ ÏÑ§Ï†ï
- **Í∏∞Ï§Ä**: F1 Score > 0.8, FPR < 5%, FNR < 10%

---

## üìã Îã§Ïùå Îã®Í≥Ñ

1. **ÎèÑÎ©îÏù∏ Ï†ÑÎ¨∏Í∞Ä Í≤ÄÌÜ†**: Ï∂îÏ≤ú ÏÑ§Ï†ïÏóê ÎåÄÌïú ÎπÑÏ¶àÎãàÏä§ Í≤ÄÌÜ†
2. **A/B ÌÖåÏä§Ìä∏**: Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑú Ï†úÌïúÏ†Å ÌÖåÏä§Ìä∏
3. **ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ**: 1Ï£ºÏùº Î™®ÎãàÌÑ∞ÎßÅ ÌõÑ ÏÑ±Îä• ÌèâÍ∞Ä
4. **ÏµúÏ¢Ö Ï†ÅÏö©**: ÏÑ±Îä• Í≤ÄÏ¶ù ÌõÑ Ï†ÑÏ≤¥ Ï†ÅÏö©

---

**Î≥¥Í≥†ÏÑú ÏÉùÏÑ±Ïûê**: RiskWeightTester
**Î∂ÑÏÑù ÏùºÏãú**: {datetime.now().isoformat()}
"""

        return report


def test_risk_weights(
    results_dir: str = "Results/Sept_2025/CSV",
    config_files: List[str] = None,
    output_path: str = None,
) -> str:
    """
    Risk Score Í∞ÄÏ§ëÏπò ÌÖåÏä§Ìä∏ Ïã§Ìñâ (Ìé∏Ïùò Ìï®Ïàò)

    Args:
        results_dir: Í≤∞Í≥º CSV ÌååÏùºÎì§Ïù¥ ÏûàÎäî ÎîîÎ†âÌÜ†Î¶¨
        config_files: ÌÖåÏä§Ìä∏Ìï† ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏
        output_path: ÎπÑÍµê Î≥¥Í≥†ÏÑú Ï∂úÎ†• Í≤ΩÎ°ú

    Returns:
        ÏÉùÏÑ±Îêú Î≥¥Í≥†ÏÑú ÌååÏùº Í≤ΩÎ°ú
    """

    tester = RiskWeightTester(results_dir)

    # Í∏∞Î≥∏ ÌÖåÏä§Ìä∏ ÏÑ§Ï†ï (ÏÑ§Ï†ï ÌååÏùºÏù¥ ÏóÜÎäî Í≤ΩÏö∞)
    if not config_files:
        default_configs = [
            {
                "name": "current",
                "weights": {
                    "delta": 0.4,
                    "anomaly": 0.3,
                    "certification": 0.2,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.8,
            },
            {
                "name": "contract_focus",
                "weights": {
                    "delta": 0.5,
                    "anomaly": 0.25,
                    "certification": 0.15,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.75,
            },
            {
                "name": "anomaly_focus",
                "weights": {
                    "delta": 0.3,
                    "anomaly": 0.45,
                    "certification": 0.15,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.7,
            },
            {
                "name": "compliance_focus",
                "weights": {
                    "delta": 0.3,
                    "anomaly": 0.2,
                    "certification": 0.4,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.8,
            },
            {
                "name": "balanced",
                "weights": {
                    "delta": 0.35,
                    "anomaly": 0.3,
                    "certification": 0.25,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.75,
            },
        ]

        weight_configs = default_configs
    else:
        # ÏÑ§Ï†ï ÌååÏùºÏóêÏÑú Î°úÎìú
        weight_configs = []
        for config_file in config_files:
            try:
                with open(config_file, "r") as f:
                    config = json.load(f)
                    weight_configs.append(config)
            except Exception as e:
                logger.warning(f"Failed to load config file {config_file}: {e}")

    # ÏµúÏã† CSV ÌååÏùº Î°úÎìú
    csv_files = list(Path(results_dir).glob("*.csv"))
    if not csv_files:
        logger.error(f"No CSV files found in {results_dir}")
        return ""

    latest_csv = max(csv_files, key=os.path.getctime)
    validation_data = pd.read_csv(latest_csv)

    logger.info(f"Loaded {len(validation_data)} rows from {latest_csv}")

    # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    comparison_df = tester.test_weight_configurations(validation_data, weight_configs)

    # Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
    report_path = tester.generate_comparison_report(comparison_df, output_path)

    return report_path


if __name__ == "__main__":
    # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    report_path = test_risk_weights()
    print(f"Risk weight comparison report created: {report_path}")
