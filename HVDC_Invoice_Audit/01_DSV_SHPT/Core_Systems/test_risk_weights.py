"""Risk Score Weight Testing Tool

ì—¬ëŸ¬ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
"""

import pandas as pd
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
import os
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)


class RiskWeightTester:
    """Risk Score Weight í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, results_dir: str = "Results/Sept_2025/CSV"):
        self.results_dir = results_dir
        self.test_results = {}

    def test_weight_configurations(
        self, validation_data: pd.DataFrame, weight_configs: List[Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ ë¹„êµ

        Args:
            validation_data: ê²€ì¦ ê²°ê³¼ DataFrame
            weight_configs: í…ŒìŠ¤íŠ¸í•  ê°€ì¤‘ì¹˜ ì„¤ì • ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¹„êµ ê²°ê³¼ DataFrame (ì„¤ì •ë³„ ì •í™•ë„, FP/FN rate)
        """

        logger.info(f"Testing {len(weight_configs)} weight configurations")

        comparison_results = []

        for i, config in enumerate(weight_configs):
            config_name = config.get("name", f"config_{i+1}")
            weights = config.get("weights", {})
            threshold = config.get("trigger_threshold", 0.8)

            logger.info(f"Testing configuration: {config_name}")

            # í•´ë‹¹ ì„¤ì •ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ì ìˆ˜ ì¬ê³„ì‚°
            recalculated_scores = self._recalculate_risk_scores(
                validation_data, weights, threshold
            )

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            performance_metrics = self._calculate_performance_metrics(
                validation_data, recalculated_scores, threshold
            )

            result = {
                "config_name": config_name,
                "weights": weights,
                "trigger_threshold": threshold,
                "performance_metrics": performance_metrics,
                "recalculated_scores": recalculated_scores,
            }

            comparison_results.append(result)

        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        comparison_df = self._create_comparison_dataframe(comparison_results)

        return comparison_df

    def _recalculate_risk_scores(
        self, df: pd.DataFrame, weights: Dict[str, float], threshold: float
    ) -> pd.Series:
        """ê°€ì¤‘ì¹˜ ì„¤ì •ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ì ìˆ˜ ì¬ê³„ì‚°"""

        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (í˜„ì¬ ì„¤ì •)
        default_weights = {
            "delta": 0.4,
            "anomaly": 0.3,
            "certification": 0.2,
            "signature": 0.1,
        }

        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        final_weights = {**default_weights, **weights}

        # ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì¦
        weight_sum = sum(final_weights.values())
        if abs(weight_sum - 1.0) > 0.001:
            logger.warning(f"Weight sum is {weight_sum}, normalizing to 1.0")
            final_weights = {k: v / weight_sum for k, v in final_weights.items()}

        # ë¦¬ìŠ¤í¬ ì ìˆ˜ ì¬ê³„ì‚°
        risk_scores = []

        for _, row in df.iterrows():
            # ê° êµ¬ì„± ìš”ì†Œ ì ìˆ˜ ì¶”ì¶œ
            delta_score = self._extract_component_score(row, "delta")
            anomaly_score = self._extract_component_score(row, "anomaly")
            cert_score = self._extract_component_score(row, "certification")
            sig_score = self._extract_component_score(row, "signature")

            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            risk_score = (
                final_weights["delta"] * delta_score
                + final_weights["anomaly"] * anomaly_score
                + final_weights["certification"] * cert_score
                + final_weights["signature"] * sig_score
            )

            risk_scores.append(risk_score)

        return pd.Series(risk_scores, index=df.index)

    def _extract_component_score(self, row: pd.Series, component: str) -> float:
        """risk_componentsì—ì„œ ê°œë³„ êµ¬ì„± ìš”ì†Œ ì ìˆ˜ ì¶”ì¶œ"""

        try:
            risk_components = row.get("risk_components", {})

            if isinstance(risk_components, str):
                components = json.loads(risk_components)
            elif isinstance(risk_components, dict):
                components = risk_components
            else:
                return 0.0

            return float(components.get(component, 0.0))

        except (json.JSONDecodeError, TypeError, ValueError, KeyError):
            return 0.0

    def _calculate_performance_metrics(
        self, df: pd.DataFrame, new_risk_scores: pd.Series, threshold: float
    ) -> Dict[str, float]:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""

        # í˜„ì¬ ë¦¬ìŠ¤í¬ íŠ¸ë¦¬ê±° ìƒíƒœ
        current_triggered = df["risk_triggered"].fillna(False)

        # ìƒˆë¡œìš´ ë¦¬ìŠ¤í¬ íŠ¸ë¦¬ê±° ìƒíƒœ (threshold ê¸°ë°˜)
        new_triggered = new_risk_scores >= threshold

        # ì‹¤ì œ ìƒíƒœ (ê²€ì¦ ê²°ê³¼ ê¸°ë°˜)
        actual_status = df["status"].fillna("UNKNOWN")
        is_high_risk = actual_status.isin(["ERROR", "FAIL", "REVIEW_NEEDED"])

        # True Positive: ì˜¬ë°”ë¥´ê²Œ ìœ„í—˜ìœ¼ë¡œ ì‹ë³„
        tp = ((new_triggered == True) & (is_high_risk == True)).sum()

        # False Positive: ì˜ëª» ìœ„í—˜ìœ¼ë¡œ ì‹ë³„
        fp = ((new_triggered == True) & (is_high_risk == False)).sum()

        # True Negative: ì˜¬ë°”ë¥´ê²Œ ì•ˆì „ìœ¼ë¡œ ì‹ë³„
        tn = ((new_triggered == False) & (is_high_risk == False)).sum()

        # False Negative: ì˜ëª» ì•ˆì „ìœ¼ë¡œ ì‹ë³„
        fn = ((new_triggered == False) & (is_high_risk == True)).sum()

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        total = len(df)

        metrics = {
            "total_items": total,
            "true_positive": int(tp),
            "false_positive": int(fp),
            "true_negative": int(tn),
            "false_negative": int(fn),
            "false_positive_rate": float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0,
            "false_negative_rate": float(fn / (fn + tp)) if (fn + tp) > 0 else 0.0,
            "precision": float(tp / (tp + fp)) if (tp + fp) > 0 else 0.0,
            "recall": float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0,
            "accuracy": float((tp + tn) / total) if total > 0 else 0.0,
            "f1_score": 0.0,
        }

        # F1 Score ê³„ì‚°
        if metrics["precision"] > 0 and metrics["recall"] > 0:
            metrics["f1_score"] = (
                2
                * (metrics["precision"] * metrics["recall"])
                / (metrics["precision"] + metrics["recall"])
            )

        return metrics

    def _create_comparison_dataframe(
        self, comparison_results: List[Dict]
    ) -> pd.DataFrame:
        """ë¹„êµ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""

        comparison_data = []

        for result in comparison_results:
            config_name = result["config_name"]
            weights = result["weights"]
            threshold = result["trigger_threshold"]
            metrics = result["performance_metrics"]

            row = {
                "config_name": config_name,
                "delta_weight": weights.get("delta", 0.4),
                "anomaly_weight": weights.get("anomaly", 0.3),
                "certification_weight": weights.get("certification", 0.2),
                "signature_weight": weights.get("signature", 0.1),
                "trigger_threshold": threshold,
                "total_items": metrics["total_items"],
                "true_positive": metrics["true_positive"],
                "false_positive": metrics["false_positive"],
                "true_negative": metrics["true_negative"],
                "false_negative": metrics["false_negative"],
                "false_positive_rate": metrics["false_positive_rate"],
                "false_negative_rate": metrics["false_negative_rate"],
                "precision": metrics["precision"],
                "recall": metrics["recall"],
                "accuracy": metrics["accuracy"],
                "f1_score": metrics["f1_score"],
            }

            comparison_data.append(row)

        return pd.DataFrame(comparison_data)

    def generate_comparison_report(
        self, comparison_df: pd.DataFrame, output_path: str = None
    ) -> str:
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""

        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"RISK_WEIGHT_COMPARISON_REPORT_{timestamp}.md"

        # ë³´ê³ ì„œ ìƒì„±
        report_content = self._create_comparison_report_content(comparison_df)

        # íŒŒì¼ ì €ì¥
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info(f"Comparison report saved: {output_path}")
        return output_path

    def _create_comparison_report_content(self, comparison_df: pd.DataFrame) -> str:
        """ë¹„êµ ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""

        report = f"""# Risk Score Weight Configuration Comparison Report

**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**í…ŒìŠ¤íŠ¸ ì„¤ì • ìˆ˜**: {len(comparison_df)}
**í…ŒìŠ¤íŠ¸ í•­ëª© ìˆ˜**: {comparison_df['total_items'].iloc[0] if len(comparison_df) > 0 else 0}

---

## ğŸ“Š ì„¤ì •ë³„ ì„±ëŠ¥ ë¹„êµ

| ì„¤ì •ëª… | Delta | Anomaly | Cert | Sig | Threshold | FPR | FNR | Precision | Recall | F1 | Accuracy |
|--------|-------|---------|------|-----|-----------|-----|-----|-----------|--------|----|-----------|
"""

        for _, row in comparison_df.iterrows():
            report += f"| {row['config_name']} | {row['delta_weight']:.2f} | {row['anomaly_weight']:.2f} | {row['certification_weight']:.2f} | {row['signature_weight']:.2f} | {row['trigger_threshold']:.2f} | {row['false_positive_rate']:.1%} | {row['false_negative_rate']:.1%} | {row['precision']:.3f} | {row['recall']:.3f} | {row['f1_score']:.3f} | {row['accuracy']:.3f} |\n"

        # ìµœê³  ì„±ëŠ¥ ì„¤ì • ì°¾ê¸°
        best_f1 = comparison_df.loc[comparison_df["f1_score"].idxmax()]
        best_accuracy = comparison_df.loc[comparison_df["accuracy"].idxmax()]
        best_low_fpr = comparison_df.loc[comparison_df["false_positive_rate"].idxmin()]

        report += f"""
---

## ğŸ† ìµœê³  ì„±ëŠ¥ ì„¤ì •

### ìµœê³  F1 Score
- **ì„¤ì •**: {best_f1['config_name']}
- **F1 Score**: {best_f1['f1_score']:.3f}
- **ê°€ì¤‘ì¹˜**: Delta={best_f1['delta_weight']:.2f}, Anomaly={best_f1['anomaly_weight']:.2f}, Cert={best_f1['certification_weight']:.2f}, Sig={best_f1['signature_weight']:.2f}
- **Threshold**: {best_f1['trigger_threshold']:.2f}

### ìµœê³  Accuracy
- **ì„¤ì •**: {best_accuracy['config_name']}
- **Accuracy**: {best_accuracy['accuracy']:.3f}
- **FPR**: {best_accuracy['false_positive_rate']:.1%}
- **FNR**: {best_accuracy['false_negative_rate']:.1%}

### ìµœì € False Positive Rate
- **ì„¤ì •**: {best_low_fpr['config_name']}
- **FPR**: {best_low_fpr['false_positive_rate']:.1%}
- **FNR**: {best_low_fpr['false_negative_rate']:.1%}
- **Trade-off**: FPR ê°ì†Œë¡œ ì¸í•œ FNR ì¦ê°€

---

## ğŸ“ˆ ìƒì„¸ ë¶„ì„

### ì„±ëŠ¥ ì§€í‘œ ë¶„í¬

"""

        # ì„±ëŠ¥ ì§€í‘œ í†µê³„
        metrics_stats = comparison_df[
            [
                "false_positive_rate",
                "false_negative_rate",
                "precision",
                "recall",
                "accuracy",
                "f1_score",
            ]
        ].describe()

        for metric in [
            "false_positive_rate",
            "false_negative_rate",
            "precision",
            "recall",
            "accuracy",
            "f1_score",
        ]:
            mean_val = metrics_stats.loc["mean", metric]
            std_val = metrics_stats.loc["std", metric]
            min_val = metrics_stats.loc["min", metric]
            max_val = metrics_stats.loc["max", metric]

            report += f"- **{metric.replace('_', ' ').title()}**: í‰ê·  {mean_val:.3f} (Â±{std_val:.3f}), ë²”ìœ„ {min_val:.3f}-{max_val:.3f}\n"

        # ê¶Œì¥ì‚¬í•­
        report += f"""
---

## ğŸ¯ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì„¤ì •
"""

        # F1 Score ê¸°ì¤€ ìƒìœ„ 3ê°œ ì„¤ì •
        top_configs = comparison_df.nlargest(3, "f1_score")

        for i, (_, config) in enumerate(top_configs.iterrows(), 1):
            report += f"""
#### {i}ìˆœìœ„: {config['config_name']}
- **F1 Score**: {config['f1_score']:.3f}
- **Accuracy**: {config['accuracy']:.3f}
- **FPR**: {config['false_positive_rate']:.1%}
- **FNR**: {config['false_negative_rate']:.1%}
- **ê¶Œì¥ ì´ìœ **: ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ ë‚®ì€ ì˜¤ë¥˜ìœ¨
"""

        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ
        report += f"""
### ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ

#### ê³„ì•½ ì¤€ìˆ˜ ì¤‘ì‹œ
- **ì¶”ì²œ ì„¤ì •**: Delta ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì„¤ì •
- **ê¸°ì¤€**: Delta weight > 0.4, FPR < 5%

#### ì´ìƒ íŒ¨í„´ íƒì§€ ì¤‘ì‹œ
- **ì¶”ì²œ ì„¤ì •**: Anomaly ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì„¤ì •
- **ê¸°ì¤€**: Anomaly weight > 0.35, FNR < 10%

#### ê· í˜•í˜• ìš´ì˜
- **ì¶”ì²œ ì„¤ì •**: F1 Scoreê°€ ë†’ì€ ì„¤ì •
- **ê¸°ì¤€**: F1 Score > 0.8, FPR < 5%, FNR < 10%

---

## ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„

1. **ë„ë©”ì¸ ì „ë¬¸ê°€ ê²€í† **: ì¶”ì²œ ì„¤ì •ì— ëŒ€í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê²€í† 
2. **A/B í…ŒìŠ¤íŠ¸**: ìš´ì˜ í™˜ê²½ì—ì„œ ì œí•œì  í…ŒìŠ¤íŠ¸
3. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: 1ì£¼ì¼ ëª¨ë‹ˆí„°ë§ í›„ ì„±ëŠ¥ í‰ê°€
4. **ìµœì¢… ì ìš©**: ì„±ëŠ¥ ê²€ì¦ í›„ ì „ì²´ ì ìš©

---

**ë³´ê³ ì„œ ìƒì„±ì**: RiskWeightTester
**ë¶„ì„ ì¼ì‹œ**: {datetime.now().isoformat()}
"""

        return report


def test_risk_weights(
    results_dir: str = "Results/Sept_2025/CSV",
    config_files: List[str] = None,
    output_path: str = None,
) -> str:
    """
    Risk Score ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í¸ì˜ í•¨ìˆ˜)

    Args:
        results_dir: ê²°ê³¼ CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        config_files: í…ŒìŠ¤íŠ¸í•  ì„¤ì • íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        output_path: ë¹„êµ ë³´ê³ ì„œ ì¶œë ¥ ê²½ë¡œ

    Returns:
        ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ
    """

    tester = RiskWeightTester(results_dir)

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì„¤ì • (ì„¤ì • íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
    if not config_files:
        default_configs = [
            {
                "name": "current",
                "weights": {
                    "delta": 0.4,
                    "anomaly": 0.3,
                    "certification": 0.2,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.8,
            },
            {
                "name": "contract_focus",
                "weights": {
                    "delta": 0.5,
                    "anomaly": 0.25,
                    "certification": 0.15,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.75,
            },
            {
                "name": "anomaly_focus",
                "weights": {
                    "delta": 0.3,
                    "anomaly": 0.45,
                    "certification": 0.15,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.7,
            },
            {
                "name": "compliance_focus",
                "weights": {
                    "delta": 0.3,
                    "anomaly": 0.2,
                    "certification": 0.4,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.8,
            },
            {
                "name": "balanced",
                "weights": {
                    "delta": 0.35,
                    "anomaly": 0.3,
                    "certification": 0.25,
                    "signature": 0.1,
                },
                "trigger_threshold": 0.75,
            },
        ]

        weight_configs = default_configs
    else:
        # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ
        weight_configs = []
        for config_file in config_files:
            try:
                with open(config_file, "r") as f:
                    config = json.load(f)
                    weight_configs.append(config)
            except Exception as e:
                logger.warning(f"Failed to load config file {config_file}: {e}")

    # ìµœì‹  CSV íŒŒì¼ ë¡œë“œ
    csv_files = list(Path(results_dir).glob("*.csv"))
    if not csv_files:
        logger.error(f"No CSV files found in {results_dir}")
        return ""

    latest_csv = max(csv_files, key=os.path.getctime)
    validation_data = pd.read_csv(latest_csv)

    logger.info(f"Loaded {len(validation_data)} rows from {latest_csv}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    comparison_df = tester.test_weight_configurations(validation_data, weight_configs)

    # ë³´ê³ ì„œ ìƒì„±
    report_path = tester.generate_comparison_report(comparison_df, output_path)

    return report_path


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    report_path = test_risk_weights()
    print(f"Risk weight comparison report created: {report_path}")
