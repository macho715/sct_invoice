"""Anomaly Detection Tuning Module

ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë ˆì¸ë³„ ìµœì  thresholdë¥¼ ê³„ì‚°í•˜ê³  íŠœë‹ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
import os
from pathlib import Path

logger = logging.getLogger(__name__)


class AnomalyDetectionTuner:
    """Anomaly Detection íŠœë‹ í´ë˜ìŠ¤"""

    def __init__(self, results_dir: str = "Results/Sept_2025/CSV"):
        self.results_dir = results_dir
        self.analysis_results = {}

    def analyze_historical_data(self, csv_files: List[str]) -> Dict[str, Any]:
        """
        ê³¼ê±° ë°ì´í„° ë¶„ì„ ë° ìµœì  threshold ë„ì¶œ

        Args:
            csv_files: ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"Analyzing historical data from {len(csv_files)} files")

        all_data = []

        # ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                all_data.append(df)
                logger.info(f"Loaded {len(df)} rows from {csv_file}")
            except Exception as e:
                logger.warning(f"Failed to load {csv_file}: {e}")

        if not all_data:
            logger.error("No data loaded for analysis")
            return {}

        # ë°ì´í„° ë³‘í•©
        combined_df = pd.concat(all_data, ignore_index=True)
        logger.info(f"Combined dataset: {len(combined_df)} rows")

        # ë ˆì¸ë³„ ë¶„ì„
        lane_analysis = {}

        for lane in combined_df["sheet_name"].unique():
            if pd.isna(lane):
                continue

            lane_data = combined_df[combined_df["sheet_name"] == lane]
            lane_analysis[lane] = self._analyze_lane_data(lane_data, lane)

        # ì „ì²´ ë¶„ì„
        overall_analysis = self._analyze_overall_data(combined_df)

        self.analysis_results = {
            "overall": overall_analysis,
            "lanes": lane_analysis,
            "total_samples": len(combined_df),
            "analysis_timestamp": datetime.now().isoformat(),
        }

        return self.analysis_results

    def _analyze_lane_data(
        self, lane_df: pd.DataFrame, lane_name: str
    ) -> Dict[str, Any]:
        """ê°œë³„ ë ˆì¸ ë°ì´í„° ë¶„ì„"""

        # Delta ë¶„í¬ ë¶„ì„
        delta_values = lane_df["delta_pct"].dropna()

        if len(delta_values) == 0:
            return {
                "lane": lane_name,
                "sample_count": len(lane_df),
                "delta_stats": {},
                "recommendations": {
                    "threshold": 3.0,
                    "min_samples": 10,
                    "confidence": "low",
                },
            }

        # í†µê³„ ê³„ì‚°
        delta_stats = {
            "mean": float(delta_values.mean()),
            "std": float(delta_values.std()),
            "median": float(delta_values.median()),
            "q25": float(delta_values.quantile(0.25)),
            "q75": float(delta_values.quantile(0.75)),
            "q90": float(delta_values.quantile(0.90)),
            "q95": float(delta_values.quantile(0.95)),
            "q99": float(delta_values.quantile(0.99)),
            "min": float(delta_values.min()),
            "max": float(delta_values.max()),
            "count": len(delta_values),
        }

        # Anomaly Score ë¶„ì„
        anomaly_scores = []
        for _, row in lane_df.iterrows():
            if pd.notna(row["anomaly_detection"]):
                try:
                    if isinstance(row["anomaly_detection"], str):
                        anomaly_data = json.loads(row["anomaly_detection"])
                    else:
                        anomaly_data = row["anomaly_detection"]

                    if isinstance(anomaly_data, dict) and "score" in anomaly_data:
                        anomaly_scores.append(float(anomaly_data["score"]))
                except (json.JSONDecodeError, TypeError, ValueError):
                    continue

        anomaly_stats = {}
        if anomaly_scores:
            anomaly_stats = {
                "mean": float(np.mean(anomaly_scores)),
                "std": float(np.std(anomaly_scores)),
                "median": float(np.median(anomaly_scores)),
                "q95": float(np.percentile(anomaly_scores, 95)),
                "max": float(np.max(anomaly_scores)),
                "count": len(anomaly_scores),
            }

        # ê¶Œì¥ threshold ê³„ì‚°
        recommendations = self._calculate_optimal_thresholds(
            delta_stats, anomaly_stats, len(lane_df)
        )

        return {
            "lane": lane_name,
            "sample_count": len(lane_df),
            "delta_stats": delta_stats,
            "anomaly_stats": anomaly_stats,
            "recommendations": recommendations,
        }

    def _analyze_overall_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ë¶„ì„"""

        # Delta ë¶„í¬ ë¶„ì„
        delta_values = df["delta_pct"].dropna()

        overall_stats = {
            "total_items": len(df),
            "lanes_count": df["sheet_name"].nunique(),
            "delta_distribution": {
                "mean": float(delta_values.mean()) if len(delta_values) > 0 else 0,
                "std": float(delta_values.std()) if len(delta_values) > 0 else 0,
                "median": float(delta_values.median()) if len(delta_values) > 0 else 0,
                "q95": (
                    float(delta_values.quantile(0.95)) if len(delta_values) > 0 else 0
                ),
                "q99": (
                    float(delta_values.quantile(0.99)) if len(delta_values) > 0 else 0
                ),
            },
        }

        # Charge Groupë³„ ë¶„ì„
        charge_groups = df["charge_group"].value_counts().to_dict()
        overall_stats["charge_groups"] = charge_groups

        # Statusë³„ ë¶„ì„
        status_counts = df["status"].value_counts().to_dict()
        overall_stats["status_distribution"] = status_counts

        return overall_stats

    def _calculate_optimal_thresholds(
        self,
        delta_stats: Dict[str, float],
        anomaly_stats: Dict[str, float],
        sample_count: int,
    ) -> Dict[str, Any]:
        """ìµœì  threshold ê³„ì‚°"""

        # ê¸°ë³¸ threshold
        base_threshold = 3.0
        min_samples = 10

        # ìƒ˜í”Œ ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        if sample_count < 5:
            confidence = "very_low"
            threshold = 4.0  # ë” ë³´ìˆ˜ì 
            min_samples = 5
        elif sample_count < 15:
            confidence = "low"
            threshold = 3.5
            min_samples = 8
        elif sample_count < 30:
            confidence = "medium"
            threshold = 3.0
            min_samples = 10
        else:
            confidence = "high"
            threshold = 2.8  # ë” ë¯¼ê°í•˜ê²Œ
            min_samples = 12

        # Delta ë¶„í¬ ê¸°ë°˜ ì¡°ì •
        if delta_stats.get("std", 0) > 20:  # ë†’ì€ ë³€ë™ì„±
            threshold += 0.5
        elif delta_stats.get("std", 0) < 5:  # ë‚®ì€ ë³€ë™ì„±
            threshold -= 0.3

        # Anomaly Score ê¸°ë°˜ ì¡°ì •
        if anomaly_stats and anomaly_stats.get("mean", 0) > 50:
            threshold -= 0.2  # ë” ë¯¼ê°í•˜ê²Œ
        elif anomaly_stats and anomaly_stats.get("mean", 0) < 20:
            threshold += 0.3  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ

        # ìµœì¢… ë²”ìœ„ ì œí•œ
        threshold = max(2.0, min(4.5, threshold))

        return {
            "threshold": round(threshold, 2),
            "min_samples": min_samples,
            "confidence": confidence,
            "rationale": self._generate_rationale(
                delta_stats, anomaly_stats, sample_count
            ),
        }

    def _generate_rationale(
        self,
        delta_stats: Dict[str, float],
        anomaly_stats: Dict[str, float],
        sample_count: int,
    ) -> str:
        """ê¶Œì¥ì‚¬í•­ ê·¼ê±° ìƒì„±"""

        rationale_parts = []

        # ìƒ˜í”Œ ìˆ˜ ê¸°ë°˜
        if sample_count < 10:
            rationale_parts.append(
                f"Small sample size ({sample_count}) - using conservative threshold"
            )
        elif sample_count > 25:
            rationale_parts.append(
                f"Large sample size ({sample_count}) - can use more sensitive threshold"
            )

        # Delta ë³€ë™ì„± ê¸°ë°˜
        if delta_stats.get("std", 0) > 20:
            rationale_parts.append(
                "High delta variability - increased threshold for stability"
            )
        elif delta_stats.get("std", 0) < 5:
            rationale_parts.append(
                "Low delta variability - can use more sensitive threshold"
            )

        # Anomaly Score ê¸°ë°˜
        if anomaly_stats and anomaly_stats.get("mean", 0) > 50:
            rationale_parts.append(
                "High anomaly scores detected - more sensitive threshold recommended"
            )

        return (
            "; ".join(rationale_parts)
            if rationale_parts
            else "Standard threshold based on sample size"
        )

    def recommend_thresholds(
        self, analysis: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """ë ˆì¸ë³„ ìµœì  threshold ì¶”ì²œ"""

        if not analysis or "lanes" not in analysis:
            logger.warning("No analysis data available for recommendations")
            return {}

        recommendations = {}

        for lane_name, lane_analysis in analysis["lanes"].items():
            rec = lane_analysis.get("recommendations", {})
            recommendations[lane_name] = {
                "threshold": rec.get("threshold", 3.0),
                "min_samples": rec.get("min_samples", 10),
                "confidence": rec.get("confidence", "medium"),
                "rationale": rec.get("rationale", "Standard recommendation"),
            }

        return recommendations

    def generate_tuning_report(self, output_path: str = None) -> str:
        """íŠœë‹ ë³´ê³ ì„œ ìƒì„±"""

        if not self.analysis_results:
            logger.error(
                "No analysis results available. Run analyze_historical_data() first."
            )
            return ""

        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"ANOMALY_TUNING_REPORT_{timestamp}.md"

        # ë³´ê³ ì„œ ìƒì„±
        report_content = self._create_report_content()

        # íŒŒì¼ ì €ì¥
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info(f"Tuning report saved: {output_path}")
        return output_path

    def _create_report_content(self) -> str:
        """ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""

        analysis = self.analysis_results

        report = f"""# Anomaly Detection Tuning Report

**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ë¶„ì„ ë°ì´í„°**: {analysis.get('total_samples', 0)}ê°œ í•­ëª©
**ë ˆì¸ ìˆ˜**: {len(analysis.get('lanes', {}))}ê°œ

---

## ğŸ“Š ì „ì²´ ë¶„ì„ ê²°ê³¼

### ê¸°ë³¸ í†µê³„
- **ì´ í•­ëª© ìˆ˜**: {analysis.get('total_samples', 0)}
- **ë ˆì¸ ìˆ˜**: {analysis.get('total_samples', 0)}

### Delta ë¶„í¬ (ì „ì²´)
"""

        overall = analysis.get("overall", {})
        delta_dist = overall.get("delta_distribution", {})

        if delta_dist:
            report += f"""
- **í‰ê· **: {delta_dist.get('mean', 0):.2f}%
- **í‘œì¤€í¸ì°¨**: {delta_dist.get('std', 0):.2f}%
- **ì¤‘ì•™ê°’**: {delta_dist.get('median', 0):.2f}%
- **95th Percentile**: {delta_dist.get('q95', 0):.2f}%
- **99th Percentile**: {delta_dist.get('q99', 0):.2f}%
"""

        # Charge Group ë¶„í¬
        charge_groups = overall.get("charge_groups", {})
        if charge_groups:
            report += f"""
### Charge Group ë¶„í¬
"""
            for group, count in charge_groups.items():
                report += f"- **{group}**: {count}ê°œ\n"

        # Status ë¶„í¬
        status_dist = overall.get("status_distribution", {})
        if status_dist:
            report += f"""
### Status ë¶„í¬
"""
            for status, count in status_dist.items():
                report += f"- **{status}**: {count}ê°œ\n"

        # ë ˆì¸ë³„ ë¶„ì„
        report += f"""
---

## ğŸ¯ ë ˆì¸ë³„ ìµœì  Threshold ì¶”ì²œ

| ë ˆì¸ | ìƒ˜í”Œ ìˆ˜ | í˜„ì¬ Threshold | ê¶Œì¥ Threshold | ì‹ ë¢°ë„ | ê·¼ê±° |
|------|---------|----------------|----------------|--------|------|
"""

        lanes = analysis.get("lanes", {})
        for lane_name, lane_data in lanes.items():
            recommendations = lane_data.get("recommendations", {})
            sample_count = lane_data.get("sample_count", 0)

            threshold = recommendations.get("threshold", 3.0)
            confidence = recommendations.get("confidence", "medium")
            rationale = recommendations.get("rationale", "Standard")

            # ì‹ ë¢°ë„ ì´ëª¨ì§€
            confidence_emoji = {
                "very_low": "ğŸ”´",
                "low": "ğŸŸ¡",
                "medium": "ğŸŸ¢",
                "high": "âœ…",
            }.get(confidence, "â“")

            report += f"| {lane_name} | {sample_count} | 3.0 | **{threshold}** | {confidence_emoji} {confidence} | {rationale[:50]}... |\n"

        # ìƒì„¸ ë¶„ì„
        report += f"""
---

## ğŸ“ˆ ìƒì„¸ ë¶„ì„

### High-Volume Lanes (ìƒ˜í”Œ > 20)
"""

        high_volume_lanes = {
            k: v for k, v in lanes.items() if v.get("sample_count", 0) > 20
        }

        for lane_name, lane_data in high_volume_lanes.items():
            delta_stats = lane_data.get("delta_stats", {})
            recommendations = lane_data.get("recommendations", {})

            report += f"""
#### {lane_name}
- **ìƒ˜í”Œ ìˆ˜**: {lane_data.get('sample_count', 0)}
- **Delta í‰ê· **: {delta_stats.get('mean', 0):.2f}%
- **Delta í‘œì¤€í¸ì°¨**: {delta_stats.get('std', 0):.2f}%
- **ê¶Œì¥ Threshold**: {recommendations.get('threshold', 3.0)}
- **ì‹ ë¢°ë„**: {recommendations.get('confidence', 'medium')}
"""

        report += f"""
### Medium-Volume Lanes (ìƒ˜í”Œ 10-20)
"""

        medium_volume_lanes = {
            k: v for k, v in lanes.items() if 10 <= v.get("sample_count", 0) <= 20
        }

        for lane_name, lane_data in medium_volume_lanes.items():
            recommendations = lane_data.get("recommendations", {})
            report += f"- **{lane_name}**: {lane_data.get('sample_count', 0)}ê°œ ìƒ˜í”Œ, ê¶Œì¥ threshold {recommendations.get('threshold', 3.0)}\n"

        report += f"""
### Low-Volume Lanes (ìƒ˜í”Œ < 10)
"""

        low_volume_lanes = {
            k: v for k, v in lanes.items() if v.get("sample_count", 0) < 10
        }

        for lane_name, lane_data in low_volume_lanes.items():
            recommendations = lane_data.get("recommendations", {})
            report += f"- **{lane_name}**: {lane_data.get('sample_count', 0)}ê°œ ìƒ˜í”Œ, ê¶Œì¥ threshold {recommendations.get('threshold', 3.0)} (ë³´ìˆ˜ì )\n"

        # ê¶Œì¥ì‚¬í•­
        report += f"""
---

## ğŸ¯ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
1. **High-Volume Lanes**: ë” ë¯¼ê°í•œ threshold ì ìš© (2.5-2.8)
2. **Medium-Volume Lanes**: í˜„ì¬ threshold ìœ ì§€ (3.0-3.2)
3. **Low-Volume Lanes**: ë³´ìˆ˜ì  threshold ì ìš© (3.5-4.0)

### ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
```json
{{
  "lanes": {{
"""

        # JSON ì„¤ì • ì˜ˆì‹œ ìƒì„±
        for lane_name, lane_data in lanes.items():
            recommendations = lane_data.get("recommendations", {})
            threshold = recommendations.get("threshold", 3.0)
            min_samples = recommendations.get("min_samples", 10)

            report += f"""    "{lane_name}": {{
      "anomaly_detection": {{
        "enabled": true,
        "model": {{
          "type": "robust_zscore",
          "params": {{
            "threshold": {threshold},
            "min_samples": {min_samples}
          }}
        }}
      }}
    }},
"""

        report += f"""  }}
}}
```

### ëª¨ë‹ˆí„°ë§ ì§€í‘œ
1. **False Positive Rate**: < 5%
2. **False Negative Rate**: < 10%
3. **Detection Accuracy**: > 85%

### ë‹¤ìŒ ë‹¨ê³„
1. ê¶Œì¥ thresholdë¡œ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
2. 1ì£¼ì¼ ëª¨ë‹ˆí„°ë§ í›„ ì„±ëŠ¥ í‰ê°€
3. í•„ìš”ì‹œ ì¶”ê°€ ì¡°ì •

---

**ë³´ê³ ì„œ ìƒì„±ì**: AnomalyDetectionTuner
**ë¶„ì„ ì¼ì‹œ**: {analysis.get('analysis_timestamp', 'N/A')}
"""

        return report


def tune_anomaly_detection(
    results_dir: str = "Results/Sept_2025/CSV", output_path: str = None
) -> str:
    """
    Anomaly Detection íŠœë‹ ì‹¤í–‰ (í¸ì˜ í•¨ìˆ˜)

    Args:
        results_dir: ê²°ê³¼ CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_path: íŠœë‹ ë³´ê³ ì„œ ì¶œë ¥ ê²½ë¡œ

    Returns:
        ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ
    """

    tuner = AnomalyDetectionTuner(results_dir)

    # CSV íŒŒì¼ ì°¾ê¸°
    csv_files = []
    if os.path.exists(results_dir):
        csv_files = list(Path(results_dir).glob("*.csv"))

    if not csv_files:
        logger.error(f"No CSV files found in {results_dir}")
        return ""

    # ë¶„ì„ ì‹¤í–‰
    analysis = tuner.analyze_historical_data([str(f) for f in csv_files])

    if not analysis:
        logger.error("Analysis failed")
        return ""

    # ë³´ê³ ì„œ ìƒì„±
    report_path = tuner.generate_tuning_report(output_path)

    return report_path


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    report_path = tune_anomaly_detection()
    print(f"Tuning report created: {report_path}")
