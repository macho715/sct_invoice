#!/usr/bin/env python3
"""
Excel Processor
Excel ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ ìœ í‹¸ë¦¬í‹°

Version: 2.0.0
Created: 2025-10-13
Author: MACHO-GPT v3.4-mini HVDC Project Enhancement
"""

import pandas as pd
import json
import ast
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ExcelDataProcessor:
    """
    PDF í†µí•© ê²°ê³¼ë¥¼ Excel í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ê¸°

    Features:
    - JSON ê²°ê³¼ë¥¼ Excel ì¹œí™”ì  í˜•íƒœë¡œ ë³€í™˜
    - PDF ê²€ì¦ ê²°ê³¼ ì •ê·œí™”
    - Gate ì ìˆ˜ ê³„ì‚° ë° ì‹œê°í™” ë°ì´í„° ì¤€ë¹„
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.raw_data = None
        self.processed_data = None

    def load_json_data(self, json_path: str) -> bool:
        """
        JSON ë°ì´í„° ë¡œë“œ

        Args:
            json_path: JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"Loading JSON data from: {json_path}")

            with open(json_path, "r", encoding="utf-8") as f:
                self.raw_data = json.load(f)

            logger.info("JSON data loaded successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to load JSON data: {str(e)}")
            return False

    def extract_pdf_integration_stats(self) -> Dict[str, Any]:
        """
        PDF í†µí•© í†µê³„ ì¶”ì¶œ (Documentation ê¸°ì¤€)

        Returns:
            Dict: PDF í†µí•© ê´€ë ¨ í†µê³„
        """
        if not self.raw_data:
            return {}

        stats = {}

        # ê¸°ë³¸ í†µê³„
        audit_info = self.raw_data.get("audit_info", {})
        statistics = self.raw_data.get("statistics", {})

        stats["total_supporting_docs"] = audit_info.get("total_supporting_docs", 0)
        stats["total_items"] = statistics.get("total_items", 0)
        stats["pass_rate"] = statistics.get("pass_rate", "0%")
        stats["total_amount_usd"] = statistics.get("total_amount_usd", 0)

        # PDF ê²€ì¦ í†µê³„ (Documentation ê¸°ì¤€)
        pdf_validation = statistics.get("pdf_validation", {})
        stats["pdf_stats"] = {
            "total_parsed": pdf_validation.get("total_parsed", 0),
            "shipments_with_pdfs": pdf_validation.get("shipments_with_pdfs", 0),
            "avg_pdfs_per_shipment": pdf_validation.get("avg_pdfs_per_shipment", 0),
            "cross_doc_pass": pdf_validation.get("cross_doc_pass", 0),
            "cross_doc_fail": pdf_validation.get("cross_doc_fail", 0),
            "demurrage_risks_found": pdf_validation.get("demurrage_risks_found", 0),
        }

        # Gate ê²€ì¦ í†µê³„
        gate_validation = statistics.get("gate_validation", {})
        stats["gate_stats"] = {
            "avg_gate_score": gate_validation.get("avg_gate_score", 0),
            "gate_pass_rate": gate_validation.get("gate_pass_rate", "0%"),
            "gate_statistics": gate_validation.get("gate_statistics", {}),
        }

        return stats

    def process_supporting_docs_data(self) -> pd.DataFrame:
        """
        Supporting Documents ë°ì´í„° ì²˜ë¦¬

        Returns:
            DataFrame: ì¦ë¹™ë¬¸ì„œ ìƒì„¸ ë°ì´í„°
        """
        if not self.raw_data:
            return pd.DataFrame()

        supporting_docs = self.raw_data.get("supporting_docs", {})
        docs_data = []

        for shipment_id, docs in supporting_docs.items():
            for doc in docs:
                docs_data.append(
                    {
                        "shipment_id": shipment_id,
                        "file_name": doc.get("file_name", ""),
                        "doc_type": doc.get("doc_type", ""),
                        "file_size_kb": doc.get("file_size", 0) / 1024,
                        "file_path": doc.get("file_path", ""),
                        "status": "Parsed",
                    }
                )

        return pd.DataFrame(docs_data)

    def extract_gate_analysis_data(self) -> pd.DataFrame:
        """
        Gate ë¶„ì„ ë°ì´í„° ì¶”ì¶œ (Gate-11~14)

        Returns:
            DataFrame: Gate ë¶„ì„ ê²°ê³¼
        """
        if not self.raw_data:
            return pd.DataFrame()

        gate_stats = (
            self.raw_data.get("statistics", {})
            .get("gate_validation", {})
            .get("gate_statistics", {})
        )

        gate_data = []
        gate_definitions = {
            "Gate-11": {
                "description": "MBL Consistency Check",
                "rule": "Validates MBL numbers across documents",
            },
            "Gate-12": {
                "description": "Container Consistency Check",
                "rule": "Validates container numbers across documents",
            },
            "Gate-13": {
                "description": "Weight Consistency Check",
                "rule": "Validates weight data (Â±3% tolerance)",
            },
            "Gate-14": {
                "description": "Certificate Validation",
                "rule": "Validates required certificates (FANR/MOIAT)",
            },
        }

        for gate_id, definition in gate_definitions.items():
            gate_stat = gate_stats.get(gate_id, {"pass": 0, "fail": 0, "skip": 0})

            total = (
                gate_stat.get("pass", 0)
                + gate_stat.get("fail", 0)
                + gate_stat.get("skip", 0)
            )
            pass_rate = (gate_stat.get("pass", 0) / total * 100) if total > 0 else 0

            gate_data.append(
                {
                    "gate": gate_id,
                    "description": definition["description"],
                    "validation_rule": definition["rule"],
                    "pass_count": gate_stat.get("pass", 0),
                    "fail_count": gate_stat.get("fail", 0),
                    "skip_count": gate_stat.get("skip", 0),
                    "pass_rate": f"{pass_rate:.1f}%",
                }
            )

        return pd.DataFrame(gate_data)

    def extract_cross_document_issues(self) -> pd.DataFrame:
        """
        Cross-Document ì´ìŠˆ ì¶”ì¶œ

        Returns:
            DataFrame: Cross-Document ê²€ì¦ ì´ìŠˆ
        """
        if not self.raw_data:
            return pd.DataFrame()

        # all_resultsì—ì„œ cross-document ì´ìŠˆ ì¶”ì¶œ
        all_results = self.raw_data.get("all_results", [])
        issues_data = []

        for result in all_results:
            pdf_validation = result.get("pdf_validation", {})
            cross_doc_validation = pdf_validation.get("cross_doc_validation", {})

            if cross_doc_validation.get("status") == "FAIL":
                issues = cross_doc_validation.get("issues", [])

                for issue in issues:
                    issues_data.append(
                        {
                            "shipment_id": result.get("shipment_id", ""),
                            "issue_type": issue.get("type", ""),
                            "description": issue.get("description", ""),
                            "severity": issue.get("severity", "MEDIUM"),
                            "affected_documents": ", ".join(
                                issue.get("affected_documents", [])
                            ),
                            "recommendation": issue.get("recommendation", ""),
                        }
                    )

        return pd.DataFrame(issues_data)

    def extract_demurrage_risk_data(self) -> pd.DataFrame:
        """
        Demurrage Risk ë°ì´í„° ì¶”ì¶œ

        Returns:
            DataFrame: Demurrage ë¦¬ìŠ¤í¬ ë¶„ì„
        """
        if not self.raw_data:
            return pd.DataFrame()

        all_results = self.raw_data.get("all_results", [])
        demurrage_data = []

        for result in all_results:
            pdf_validation = result.get("pdf_validation", {})
            demurrage_risk = pdf_validation.get("demurrage_risk", {})

            if demurrage_risk:
                demurrage_data.append(
                    {
                        "shipment_id": result.get("shipment_id", ""),
                        "risk_level": demurrage_risk.get("risk_level", "LOW"),
                        "days_remaining": demurrage_risk.get("days_remaining", 0),
                        "days_overdue": demurrage_risk.get("days_overdue", 0),
                        "estimated_cost_usd": demurrage_risk.get(
                            "estimated_cost_usd", 0
                        ),
                        "do_validity_date": demurrage_risk.get("do_validity_date", ""),
                        "current_date": demurrage_risk.get("current_date", ""),
                    }
                )

        return pd.DataFrame(demurrage_data)

    def create_executive_summary_data(self) -> Dict[str, Any]:
        """
        Executive Summary ë°ì´í„° ìƒì„±

        Returns:
            Dict: Executive Summaryìš© êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        stats = self.extract_pdf_integration_stats()

        summary = {
            "overview": {
                "report_title": "SHPT September 2025 - PDF Integrated Audit Report",
                "generation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_items": stats.get("total_items", 0),
                "total_amount_usd": stats.get("total_amount_usd", 0),
                "pass_rate": stats.get("pass_rate", "0%"),
                "pdf_integration_enabled": True,
            },
            "pdf_integration": stats.get("pdf_stats", {}),
            "gate_validation": stats.get("gate_stats", {}),
            "key_metrics": {
                "documents_processed": stats.get("total_supporting_docs", 0),
                "shipments_analyzed": stats.get("pdf_stats", {}).get(
                    "shipments_with_pdfs", 0
                ),
                "cross_doc_success_rate": self._calculate_cross_doc_success_rate(stats),
                "avg_gate_score": stats.get("gate_stats", {}).get("avg_gate_score", 0),
            },
        }

        return summary

    def _calculate_cross_doc_success_rate(self, stats: Dict) -> str:
        """Cross-Document ì„±ê³µë¥  ê³„ì‚°"""
        pdf_stats = stats.get("pdf_stats", {})
        total_pass = pdf_stats.get("cross_doc_pass", 0)
        total_fail = pdf_stats.get("cross_doc_fail", 0)
        total = total_pass + total_fail

        if total == 0:
            return "N/A"

        success_rate = (total_pass / total) * 100
        return f"{success_rate:.1f}%"

    def process_all_data(self) -> Dict[str, Any]:
        """
        ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ë° êµ¬ì¡°í™”

        Returns:
            Dict: ì²˜ë¦¬ëœ ëª¨ë“  ë°ì´í„°
        """
        if not self.raw_data:
            return {}

        logger.info("Processing all JSON data for Excel conversion...")

        processed = {
            "statistics": self.extract_pdf_integration_stats(),
            "supporting_docs": self.process_supporting_docs_data(),
            "gate_analysis": self.extract_gate_analysis_data(),
            "cross_doc_issues": self.extract_cross_document_issues(),
            "demurrage_risks": self.extract_demurrage_risk_data(),
            "executive_summary": self.create_executive_summary_data(),
        }

        self.processed_data = processed

        logger.info("Data processing completed successfully")
        return processed

    def save_processed_data(self, output_path: str) -> bool:
        """
        ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥

        Args:
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ

        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        if not self.processed_data:
            logger.error("No processed data to save")
            return False

        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, "w", encoding="utf-8") as f:
                # DataFrameì€ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                save_data = {}
                for key, value in self.processed_data.items():
                    if isinstance(value, pd.DataFrame):
                        save_data[key] = value.to_dict("records")
                    else:
                        save_data[key] = value

                json.dump(save_data, f, ensure_ascii=False, indent=2)

            logger.info(f"Processed data saved to: {output_file}")
            return True

        except Exception as e:
            logger.error(f"Failed to save processed data: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ"""

    # JSON íŒŒì¼ ê²½ë¡œ
    json_path = "HVDC_Invoice_Audit-20251012T195441Z-1-001/HVDC_Invoice_Audit/01_DSV_SHPT/Results/Sept_2025/shpt_sept_2025_enhanced_result_20251012_123701.json"

    # ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = ExcelDataProcessor()

    # JSON ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
    if processor.load_json_data(json_path):
        processed_data = processor.process_all_data()

        # ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
        print("âœ… JSON Data Processing Complete!")
        print(f"   ğŸ“Š Statistics: {len(processed_data.get('statistics', {}))}")
        print(
            f"   ğŸ“„ Supporting Docs: {len(processed_data.get('supporting_docs', []))}"
        )
        print(f"   ğŸ¯ Gate Analysis: {len(processed_data.get('gate_analysis', []))}")
        print(
            f"   âš ï¸ Cross-Doc Issues: {len(processed_data.get('cross_doc_issues', []))}"
        )
        print(
            f"   ğŸš¨ Demurrage Risks: {len(processed_data.get('demurrage_risks', []))}"
        )

        # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (ì„ íƒì‚¬í•­)
        output_path = "Results/Sept_2025/processed_excel_data.json"
        processor.save_processed_data(output_path)
    else:
        print("âŒ Failed to load JSON data")


if __name__ == "__main__":
    main()
