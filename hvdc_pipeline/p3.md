Updated the Stage 2 derived column processor to load configured input/output paths, create missing directories, and optionally copy cell fills from the source workbook so color highlighting survives the export.

Bootstrapped Stage 3 report generation with pipeline configuration paths, resolving Stage 2 outputs automatically and persisting CSV/Excel artifacts into the configured reports directory.

Surfaced the derived/report locations through the pipeline runner and documented the workflow changes in the changelog for easier operator guidance.

Testing

⚠️ pytest -q (fails: missing fixture PDF required by At Cost PDF Parsing Test)

✅ black --check hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py hvdc_pipeline/scripts/stage3_report/report_generator.py hvdc_pipeline/run_pipeline.py

⚠️ flake8 hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py hvdc_pipeline/scripts/stage3_report/report_generator.py hvdc_pipeline/run_pipeline.py (fails because of extensive pre-existing style violations in the legacy Stage 3 report generator)


파일 (4)개

CHANGELOG.md
신규

hvdc_pipeline/run_pipeline.py
+37
-10

hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
+108
-15

hvdc_pipeline/scripts/stage3_report/report_generator.py
+87
-20

diff --git a/CHANGELOG.md b/CHANGELOG.md
new file mode 100644
index 0000000000000000000000000000000000000000..8b2ab391032158a44270db24da902a23f07d5c54
--- /dev/null
+++ b/CHANGELOG.md
@@ -0,0 +1,9 @@
+# Changelog
+
+## [Unreleased]
+### Added
+- structural(pipeline): Stage 2 산출물 출력 경로 및 Stage 3 보고서 디렉터리 구성을 설정하고 색상 보존 전략을
+  통합했습니다. / Configured Stage 2 derived output paths and Stage 3 report directories with color preservation
+  support.
+- behavioral(pipeline): 파이프라인 실행 안내 메시지를 업데이트하여 사용자가 보고서 출력 위치를 쉽게 확인할 수
+  있도록 했습니다. / Updated pipeline execution guidance to point users to the correct report output location.
diff --git a/hvdc_pipeline/run_pipeline.py b/hvdc_pipeline/run_pipeline.py
index 3c29950c91e3c2d6a457114d531bb72e9d8286c0..c54a0b0c8ea0343872a3805d78249e7f0a800598 100644
--- a/hvdc_pipeline/run_pipeline.py
+++ b/hvdc_pipeline/run_pipeline.py
@@ -1,38 +1,38 @@
 #!/usr/bin/env python3
 """
 HVDC 파이프라인 통합 실행 스크립트
 HVDC Pipeline Integrated Execution Script

 전체 파이프라인을 하나의 명령으로 실행할 수 있는 통합 스크립트입니다.
 """

 import argparse
 import sys
 import time
 from pathlib import Path
-from typing import List, Optional
+from typing import List

 import yaml

 # 프로젝트 루트 경로 추가
 sys.path.append(str(Path(__file__).parent))

 # 각 Stage 임포트
 try:
     from scripts.stage2_derived.derived_columns_processor import process_derived_columns

     # 다른 모듈들은 필요시 개별적으로 임포트
 except ImportError as e:
     print(f"ERROR: 모듈 임포트 실패: {e}")
     print("requirements.txt의 패키지들이 설치되었는지 확인하세요.")
     sys.exit(1)


 def load_config() -> dict:
     """파이프라인 설정을 로드합니다."""
     config_path = Path(__file__).parent / "config" / "pipeline_config.yaml"
     try:
         with open(config_path, "r", encoding="utf-8") as f:
             return yaml.safe_load(f)
     except FileNotFoundError:
         print(f"WARNING: 설정 파일을 찾을 수 없습니다: {config_path}")
@@ -43,74 +43,101 @@ def print_banner():
     """파이프라인 시작 배너를 출력합니다."""
     print("\n" + "=" * 80)
     print("HVDC PIPELINE v2.0 - Integration Execution")
     print("   Samsung C&T Logistics | ADNOC-DSV Partnership")
     print("=" * 80)
     print("Execution Stages:")
     print("   Stage 1: Data Synchronization")
     print("   Stage 2: Derived Columns")
     print("   Stage 3: Report Generation")
     print("   Stage 4: Anomaly Detection")
     print("=" * 80 + "\n")


 def run_stage(stage_num: int, config: dict) -> bool:
     """특정 Stage를 실행합니다."""
     stage_start_time = time.time()

     try:
         if stage_num == 1:
             print("[Stage 1] Data Synchronization...")
             print("INFO: Stage 1 requires separate script execution.")
             print("      python scripts/stage1_sync/data_synchronizer.py")

         elif stage_num == 2:
             print("[Stage 2] Derived Columns Generation...")
-            # 설정에서 입력 파일 경로 가져오기
             stage2_config_path = (
                 Path(__file__).parent / "config" / "stage2_derived_config.yaml"
             )
             if stage2_config_path.exists():
                 with open(stage2_config_path, "r", encoding="utf-8") as f:
                     stage2_config = yaml.safe_load(f)
-                input_file = stage2_config.get("input", {}).get(
-                    "synced_file",
-                    "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx",
-                )
             else:
-                input_file = (
-                    "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
-                )
+                stage2_config = {}

-            success = process_derived_columns(input_file)
+            input_file = stage2_config.get("input", {}).get(
+                "synced_file",
+                "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx",
+            )
+            derived_file = stage2_config.get("output", {}).get(
+                "derived_file",
+                "data/processed/derived/HVDC_WAREHOUSE_HITACHI_HE_derived.xlsx",
+            )
+            preserve_colors = stage2_config.get("output", {}).get(
+                "preserve_colors",
+                False,
+            )
+
+            success = process_derived_columns(
+                input_file=input_file,
+                output_file=derived_file,
+                preserve_colors=preserve_colors,
+            )
             if not success:
                 return False

         elif stage_num == 3:
             print("[Stage 3] Report Generation...")
+            data_root_value = config.get("paths", {}).get("data_root", "data")
+            reports_root_value = config.get("paths", {}).get("reports_root")
+            pipeline_root = Path(__file__).parent
+            data_root_path = (
+                (pipeline_root / data_root_value)
+                if not Path(data_root_value).is_absolute()
+                else Path(data_root_value)
+            )
+            reports_dir = (
+                (pipeline_root / reports_root_value)
+                if reports_root_value and not Path(reports_root_value).is_absolute()
+                else (Path(reports_root_value) if reports_root_value else None)
+            )
+            if reports_dir is None:
+                reports_dir = data_root_path / "processed" / "reports"
+
             print("INFO: Stage 3 requires separate script execution.")
             print("      python scripts/stage3_report/report_generator.py")
+            print(f"INFO: 보고서 출력 경로: {reports_dir}")

         elif stage_num == 4:
             print("[Stage 4] Anomaly Detection...")
             print("INFO: Stage 4 requires separate script execution.")
             print("      python scripts/stage4_anomaly/anomaly_detector.py")

         else:
             print(f"ERROR: 알 수 없는 Stage 번호: {stage_num}")
             return False

         stage_duration = time.time() - stage_start_time
         print(f"[OK] Stage {stage_num} completed (Duration: {stage_duration:.2f}s)\n")
         return True

     except Exception as e:
         print(f"[ERROR] Stage {stage_num} failed: {e}")
         return False


 def run_all_stages(config: dict) -> bool:
     """모든 Stage를 순차적으로 실행합니다."""
     print_banner()

     stages = [1, 2, 3, 4]
     total_start_time = time.time()
diff --git a/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py b/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
index 30548b8c256a751f6e53acb16e4da17fdd614d0d..a058896286fc3b275abbb119154a4adb7b919414 100644
--- a/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
+++ b/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
@@ -1,72 +1,138 @@
 """
 파생 컬럼 처리기 (Derived Columns Processor)

 동기화된 데이터에서 13개의 파생 컬럼을 자동으로 계산하는 최적화된 스크립트입니다.
 Excel 공식을 Python pandas 벡터화 연산으로 변환하여 고성능 처리를 제공합니다.

 주요 기능:
 - 13개 파생 컬럼 자동 계산
 - 벡터화 연산으로 고성능 처리 (10배 속도 향상)
 - 원본 컬럼명 보존 (site  handling - 공백 2개)
 - 색상 보존 전략 지원

 작성자: AI Development Team
 버전: v2.0
 작성일: 2025-10-19
 """

 from __future__ import annotations

+from copy import copy
 from pathlib import Path
 from typing import Iterable, Tuple

 import pandas as pd  # type: ignore[import-untyped]
+import yaml
+
+try:  # pragma: no cover - optional dependency guard
+    from openpyxl import load_workbook
+except ImportError:  # pragma: no cover - import error handled at runtime
+    load_workbook = None  # type: ignore[assignment]
+
+PIPELINE_ROOT = Path(__file__).resolve().parents[2]
+STAGE2_CONFIG_PATH = PIPELINE_ROOT / "config" / "stage2_derived_config.yaml"
+
+DEFAULT_SYNCED_INPUT = "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
+DEFAULT_DERIVED_OUTPUT = "data/processed/derived/HVDC_WAREHOUSE_HITACHI_HE_derived.xlsx"

 from .column_definitions import (
     DERIVED_COLUMNS,
     FINAL_HANDLING_COLUMN,
     MINUS_COLUMN,
     SITE_COLUMNS,
     SITE_HANDLING_COLUMN,
     SQM_COLUMN,
     STACK_STATUS_COLUMN,
     STATUS_CURRENT_COLUMN,
     STATUS_LOCATION_COLUMN,
     STATUS_LOCATION_DATE_COLUMN,
     STATUS_SITE_COLUMN,
     STATUS_STORAGE_COLUMN,
     STATUS_WAREHOUSE_COLUMN,
     TOTAL_HANDLING_COLUMN,
     WAREHOUSE_COLUMNS,
     WH_HANDLING_COLUMN,
 )

 SITE_COLUMN_LOOKUP = {col.lower() for col in SITE_COLUMNS}
 WAREHOUSE_COLUMN_LOOKUP = {col.lower() for col in WAREHOUSE_COLUMNS}


+def _load_stage2_config() -> dict:
+    """Stage2 설정 파일을 로드합니다. / Load stage 2 configuration."""
+
+    if not STAGE2_CONFIG_PATH.exists():
+        return {}
+
+    with STAGE2_CONFIG_PATH.open("r", encoding="utf-8") as handle:
+        return yaml.safe_load(handle) or {}
+
+
+def _resolve_path(path_value: str | Path) -> Path:
+    """상대 경로를 절대 경로로 변환합니다. / Resolve relative path to absolute."""
+
+    path = Path(path_value)
+    if not path.is_absolute():
+        path = PIPELINE_ROOT / path
+    return path
+
+
+def _ensure_output_directory(target_path: Path) -> None:
+    """출력 디렉터리를 생성합니다. / Ensure output directory exists."""
+
+    target_path.parent.mkdir(parents=True, exist_ok=True)
+
+
+def _preserve_cell_colors(source_path: Path, target_path: Path) -> None:
+    """색상을 보존합니다. / Preserve cell colors from source to target."""
+
+    if load_workbook is None:
+        print("WARNING: openpyxl이 설치되지 않아 색상 보존을 수행하지 못했습니다.")
+        return
+
+    if not source_path.exists() or not target_path.exists():
+        return
+
+    source_wb = load_workbook(source_path)
+    target_wb = load_workbook(target_path)
+    source_ws = source_wb.active
+    target_ws = target_wb.active
+
+    max_row = min(source_ws.max_row, target_ws.max_row)
+    max_col = min(source_ws.max_column, target_ws.max_column)
+
+    for row_idx in range(1, max_row + 1):
+        for col_idx in range(1, max_col + 1):
+            source_cell = source_ws.cell(row=row_idx, column=col_idx)
+            target_cell = target_ws.cell(row=row_idx, column=col_idx)
+            if source_cell.fill is not None:
+                target_cell.fill = copy(source_cell.fill)
+
+    target_wb.save(target_path)
+
+
 def _latest_location_and_date(
     row: pd.Series,
 ) -> Tuple[str | None, pd.Timestamp | pd.NaT]:
     """최근 위치와 날짜를 계산합니다. / Compute the latest location and date."""
     non_null = row.dropna()
     if non_null.empty:
         return None, pd.NaT
     latest_date = non_null.max()
     latest_columns = non_null[non_null == latest_date].index
     return latest_columns[0], latest_date


 def _classify_storage(location: str | None) -> str:
     """위치 기반 보관 유형을 분류합니다. / Classify storage based on location."""
     if location is None or location == "":
         return ""
     if location == "Pre Arrival":
         return "Pre Arrival"
     lowered = location.lower()
     if lowered in SITE_COLUMN_LOOKUP:
         return "site"
     if lowered in WAREHOUSE_COLUMN_LOOKUP:
         return "warehouse"
     return ""

@@ -184,84 +250,111 @@ def calculate_derived_columns(df: pd.DataFrame) -> pd.DataFrame:
         working_df[SITE_HANDLING_COLUMN] = site_handling
     else:
         working_df[SITE_HANDLING_COLUMN] = 0
     working_df[TOTAL_HANDLING_COLUMN] = (
         working_df[WH_HANDLING_COLUMN] + working_df[SITE_HANDLING_COLUMN]
     )
     working_df[MINUS_COLUMN] = (
         working_df[SITE_HANDLING_COLUMN] - working_df[WH_HANDLING_COLUMN]
     )
     working_df[FINAL_HANDLING_COLUMN] = (
         working_df[TOTAL_HANDLING_COLUMN] + working_df[MINUS_COLUMN]
     )

     if "규격" in working_df.columns and "수량" in working_df.columns:
         working_df[SQM_COLUMN] = (working_df["규격"] * working_df["수량"]) / 10000
     else:
         working_df[SQM_COLUMN] = ""
         print("WARNING: '규격' 또는 '수량' 컬럼이 없어 SQM 계산을 건너뜁니다.")

     working_df[STACK_STATUS_COLUMN] = ""

     return working_df


 def process_derived_columns(
-    input_file: str = "HVDC WAREHOUSE_HITACHI(HE).synced.xlsx",
+    input_file: str | Path | None = None,
+    output_file: str | Path | None = None,
+    preserve_colors: bool | None = None,
 ) -> bool:
     """파생 컬럼을 계산합니다. / Process derived columns."""
+
+    config = _load_stage2_config()
+
+    input_path_value = input_file or config.get("input", {}).get(
+        "synced_file", DEFAULT_SYNCED_INPUT
+    )
+    output_path_value = output_file or config.get("output", {}).get(
+        "derived_file", DEFAULT_DERIVED_OUTPUT
+    )
+    preserve_colors_value = (
+        preserve_colors
+        if preserve_colors is not None
+        else bool(config.get("output", {}).get("preserve_colors", False))
+    )
+
+    input_path = _resolve_path(input_path_value)
+    output_path = _resolve_path(output_path_value)
+
     print("=== 파생 컬럼 처리 시작 ===")
-    print(f"입력 파일: {input_file}")
+    print(f"입력 파일: {input_path}")
+    print(f"출력 파일: {output_path}")
+    print(f"색상 보존: {'활성화' if preserve_colors_value else '비활성화'}")

-    # 파일 존재 확인
-    if not Path(input_file).exists():
-        raise FileNotFoundError(f"입력 파일을 찾을 수 없습니다: {input_file}")
+    if not input_path.exists():
+        raise FileNotFoundError(f"입력 파일을 찾을 수 없습니다: {input_path}")

-    # 데이터 로드
-    df = pd.read_excel(input_file)
+    _ensure_output_directory(output_path)
+
+    df = pd.read_excel(input_path)
     print(f"원본 데이터 로드 완료: {len(df)}행, {len(df.columns)}컬럼")

     df = calculate_derived_columns(df)

     wh_cols = [c for c in WAREHOUSE_COLUMNS if c in df.columns]
     st_cols = [c for c in SITE_COLUMNS if c in df.columns]

     print(f"Warehouse 컬럼: {len(wh_cols)}개 - {wh_cols}")
     print(f"Site 컬럼: {len(st_cols)}개 - {st_cols}")

     print(
         "SUCCESS: 파생 컬럼 %s개 계산 완료 (행: %s, 컬럼: %s)"
         % (len(DERIVED_COLUMNS), len(df), len(df.columns))
     )

-    # 결과 저장
-    output_file = "HVDC WAREHOUSE_HITACHI(HE).xlsx"
-    df.to_excel(output_file, index=False)
-    print(f"SUCCESS: 파일 저장 완료: {output_file}")
+    df.to_excel(output_path, index=False)
+    print(f"SUCCESS: 파일 저장 완료: {output_path}")
+
+    if preserve_colors_value:
+        _preserve_cell_colors(input_path, output_path)
+        print("INFO: 입력 파일의 색상을 보존했습니다.")

     return True


 def main() -> int:
     """메인 실행을 수행합니다. / Execute script entry point."""
     try:
+        config = _load_stage2_config()
+        default_output_path = _resolve_path(
+            config.get("output", {}).get("derived_file", DEFAULT_DERIVED_OUTPUT)
+        )
+
         success = process_derived_columns()
         if success:
             print("\n" + "=" * 60)
             print("SUCCESS: 파생 컬럼 처리 완료!")
-            print("FILE: 결과 파일: HVDC WAREHOUSE_HITACHI(HE).xlsx")
-            print("INFO: 색상은 Step 1에서 이미 적용되었습니다.")
+            print(f"FILE: 결과 파일: {default_output_path}")
+            print("INFO: 색상은 설정에 따라 자동으로 보존됩니다.")
             print("=" * 60)
         else:
             print("ERROR: 처리 실패")
             return 1
     except Exception as e:
         print(f"ERROR: 오류 발생: {e}")
         return 1

     return 0


 if __name__ == "__main__":
     exit(main())
-
-    exit(main())
diff --git a/hvdc_pipeline/scripts/stage3_report/report_generator.py b/hvdc_pipeline/scripts/stage3_report/report_generator.py
index 1b2fb44525c816ae144a5da2778e81cd8585fca3..b0555eb3c6437f6113c72f18db076ebb40596504 100644
--- a/hvdc_pipeline/scripts/stage3_report/report_generator.py
+++ b/hvdc_pipeline/scripts/stage3_report/report_generator.py
@@ -1,65 +1,94 @@
 """
  HVDC 입고 로직 구현 및 집계 시스템 종합 보고서 (v3.0-corrected)
 Samsung C&T · ADNOC · DSV Partnership

 ===== 수정 버전 (v3.0-corrected) =====
  주요 수정사항:
 1. 창고 vs 현장 입고 분리
 2. 출고 타이밍 정확성 개선
 3. 재고 검증 로직 강화
 4. 이중 계산 방지

 핵심 개선사항:
 1. 창고 컬럼만 입고로 계산 (현장 제외)
 2. 창고간 이동의 목적지는 제외 (이중 계산 방지)
 3. 다음 날 이동만 출고로 인정 (동일 날짜 제외)
 4. Status_Location과 물리적 위치 교차 검증
 5. 입고/출고/재고 일관성 검증 강화

 입고 로직 3단계: calculate_warehouse_inbound_corrected() → create_monthly_inbound_pivot() → calculate_final_location()
 Multi-Level Header: 창고 17열(누계 포함), 현장 9열
 """

-import pandas as pd
-import numpy as np
+import logging
+import os
+import re
 from datetime import datetime, timedelta
 from pathlib import Path
-import logging
 from typing import Dict, List, Optional, Tuple
+
+import numpy as np
+import pandas as pd
+import yaml
 import warnings

 warnings.filterwarnings("ignore")
-import os
-import re

 # 로깅 설정
 logging.basicConfig(
     level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)

+PIPELINE_ROOT = Path(__file__).resolve().parents[2]
+PIPELINE_CONFIG_PATH = PIPELINE_ROOT / "config" / "pipeline_config.yaml"
+STAGE2_CONFIG_PATH = PIPELINE_ROOT / "config" / "stage2_derived_config.yaml"
+DEFAULT_STAGE2_OUTPUT = "data/processed/derived/HVDC_WAREHOUSE_HITACHI_HE_derived.xlsx"
+DEFAULT_REPORTS_DIR = "data/processed/reports"
+
+
+def _load_yaml_config(config_path: Path) -> Dict:
+    """YAML 설정을 로드합니다. / Load a YAML configuration file."""
+
+    if not config_path.exists():
+        return {}
+
+    with config_path.open("r", encoding="utf-8") as handle:
+        loaded = yaml.safe_load(handle) or {}
+    return loaded
+
+
+def _resolve_path(path_value: str | Path) -> Path:
+    """상대 경로를 절대 경로로 변환합니다. / Resolve a relative path to absolute."""
+
+    candidate = Path(path_value)
+    if not candidate.is_absolute():
+        candidate = PIPELINE_ROOT / candidate
+    return candidate
+
+
 # 수정 버전 정보
 CORRECTED_VERSION = "v3.0-corrected"  #  버전 업데이트
 CORRECTED_DATE = "2025-01-09"
 VERIFICATION_RATE = 99.97  # 검증 정합률 (%)


 # Function Guard 매크로 - 중복 정의 방지
 def _check_duplicate_function(func_name: str):
     """중복 함수 정의 감지"""
     if func_name in globals():
         raise RuntimeError(f"Duplicate definition detected: {func_name}")


 # 공통 헬퍼 함수
 def _get_pkg(row):
     """Pkg 컬럼에서 수량을 안전하게 추출하는 헬퍼 함수"""
     pkg_value = row.get("Pkg", 1)
     if pd.isna(pkg_value) or pkg_value == "" or pkg_value == 0:
         return 1
     try:
         return int(pkg_value)
     except (ValueError, TypeError):
         return 1


@@ -185,55 +214,90 @@ def validate_kpi_thresholds(stats: Dict) -> Dict:
     if "inbound_result" in stats and "outbound_result" in stats:
         total_inbound = stats["inbound_result"]["total_inbound"]
         total_outbound = stats["outbound_result"]["total_outbound"]

         validation_results["Inbound_Outbound_Ratio"] = {
             "status": "PASS" if total_inbound >= total_outbound else "FAIL",
             "value": f"{total_inbound} ≥ {total_outbound}",
             "threshold": "입고 ≥ 출고",
         }

     all_pass = all(result["status"] == "PASS" for result in validation_results.values())

     logger.info(
         f" 수정 버전 KPI 검증 완료: {'ALL PASS' if all_pass else 'SOME FAILED'}"
     )
     return validation_results


 class CorrectedWarehouseIOCalculator:
     """수정된 창고 입출고 계산기"""

     def __init__(self):
         """초기화"""
         self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

-        # 실제 데이터 경로 설정 (현재 디렉토리 기준)
-        self.data_path = Path(".")  # 현재 hitachi 디렉토리
-        self.hitachi_file = self.data_path / "HVDC WAREHOUSE_HITACHI(HE).xlsx"
-        self.simense_file = self.data_path / "HVDC WAREHOUSE_SIMENSE(SIM).xlsx"
-        self.invoice_file = self.data_path / "HVDC WAREHOUSE_INVOICE.xlsx"
+        pipeline_config = _load_yaml_config(PIPELINE_CONFIG_PATH)
+        stage2_config = _load_yaml_config(STAGE2_CONFIG_PATH)
+
+        paths_config = pipeline_config.get("paths", {})
+        data_root = _resolve_path(paths_config.get("data_root", "data"))
+        reports_root_value = paths_config.get("reports_root")
+        reports_root = (
+            _resolve_path(reports_root_value)
+            if reports_root_value
+            else data_root / "processed" / "reports"
+        )
+
+        derived_output_value = stage2_config.get("output", {}).get(
+            "derived_file", DEFAULT_STAGE2_OUTPUT
+        )
+        derived_output_path = _resolve_path(derived_output_value)
+
+        self.stage2_output_dir = derived_output_path.parent
+        self.reports_output_dir = reports_root
+        self.data_path = self.stage2_output_dir
+
+        self.hitachi_file = derived_output_path
+
+        simense_candidates = [
+            self.stage2_output_dir / "HVDC_WAREHOUSE_SIMENSE_SIM_derived.xlsx",
+            self.stage2_output_dir / "HVDC WAREHOUSE_SIMENSE(SIM).xlsx",
+        ]
+        self.simense_file = next(
+            (candidate for candidate in simense_candidates if candidate.exists()),
+            simense_candidates[0],
+        )
+
+        invoice_candidates = [
+            data_root / "processed" / "invoices" / "HVDC_WAREHOUSE_INVOICE.xlsx",
+            self.stage2_output_dir / "HVDC WAREHOUSE_INVOICE.xlsx",
+        ]
+        self.invoice_file = next(
+            (candidate for candidate in invoice_candidates if candidate.exists()),
+            invoice_candidates[0],
+        )

         #  수정: 창고와 현장을 명확히 분리
         self.warehouse_columns = [
             "DHL Warehouse",
             "DSV Indoor",
             "DSV Al Markaz",
             "Hauler Indoor",
             "DSV Outdoor",
             "DSV MZP",
             "HAULER",
             "JDN MZD",
             "MOSB",
             "AAA Storage",
         ]

         self.site_columns = ["AGI", "DAS", "MIR", "SHU"]

         #  수정: 위치 우선순위 (타이브레이커용)
         self.location_priority = {
             "DSV Al Markaz": 1,
             "DSV Indoor": 2,
             "DSV Outdoor": 3,
             "AAA Storage": 4,
             "Hauler Indoor": 5,
             "HAULER": 6,
@@ -1503,50 +1567,52 @@ class CorrectedWarehouseIOCalculator:
             (estimated_sqm_count / total_records) * 100 if total_records > 0 else 0
         )

         quality_analysis = {
             "total_records": total_records,
             "actual_sqm_count": actual_sqm_count,
             "estimated_sqm_count": estimated_sqm_count,
             "actual_sqm_percentage": actual_percentage,
             "estimated_sqm_percentage": estimated_percentage,
             "data_quality_score": actual_percentage,
         }

         logger.info(
             f" SQM 데이터 품질 분석 완료: 실제 {actual_percentage:.1f}%, 추정 {estimated_percentage:.1f}%"
         )
         return quality_analysis


 class HVDCExcelReporterFinal:
     """HVDC Excel 리포트 생성기 (수정된 버전)"""

     def __init__(self):
         """초기화"""
         self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
         self.calculator = CorrectedWarehouseIOCalculator()
+        self.report_output_dir = self.calculator.reports_output_dir
+        self.report_output_dir.mkdir(parents=True, exist_ok=True)

         logger.info(" HVDC Excel Reporter Final 초기화 완료 (v3.0-corrected)")

     def calculate_warehouse_statistics(self) -> Dict:
         """위 4 결과 + 월별 Pivot + SQM 기반 누적 재고 → Excel 확장"""
         logger.info(" calculate_warehouse_statistics() - 종합 통계 계산 (SQM 확장)")

         # 데이터 로드 및 처리
         self.calculator.load_real_hvdc_data()
         df = self.calculator.process_real_data()
         df = self.calculator.calculate_final_location(df)

         # 4가지 핵심 계산 (기존)
         inbound_result = self.calculator.calculate_warehouse_inbound_corrected(df)
         outbound_result = self.calculator.calculate_warehouse_outbound_corrected(df)
         inventory_result = self.calculator.calculate_warehouse_inventory_corrected(df)
         direct_result = self.calculator.calculate_direct_delivery(df)

         # 월별 피벗 계산 (기존)
         inbound_pivot = self.calculator.create_monthly_inbound_pivot(df)

         #  NEW: SQM 기반 누적 재고 계산
         sqm_inbound = self.calculator.calculate_monthly_sqm_inbound(df)
         sqm_outbound = self.calculator.calculate_monthly_sqm_outbound(df)
         sqm_cumulative = self.calculator.calculate_cumulative_sqm_inventory(
@@ -2214,115 +2280,116 @@ class HVDCExcelReporterFinal:
                 aaa_count = data_df["AAA Storage"].notna().sum()
                 print(f"    {data_name} - AAA Storage: {aaa_count}건")
             else:
                 print(f"    {data_name} - AAA Storage: 컬럼 없음")

         #  검증: Status_Location_YearMonth 컬럼 확인
         if "Status_Location_YearMonth" in combined_original.columns:
             print(f"    Status_Location_YearMonth 컬럼 포함")
         else:
             print(f"    Status_Location_YearMonth 컬럼 없음")

         #  검증: handling 컬럼들 확인
         handling_cols = [
             "wh_handling_original",
             "site_handling_original",
             "total_handling_original",
             "total handling",
         ]
         for col in handling_cols:
             if col in combined_original.columns:
                 non_null = combined_original[col].notna().sum()
                 print(f"    {col}: {non_null}건")
             else:
                 print(f"    {col}: 컬럼 없음")

-        # output 폴더 자동 생성
-        output_dir = Path("output")
-        output_dir.mkdir(exist_ok=True)
-
-        #  FIX: 전체 데이터는 CSV로도 저장 (백업용)
         hitachi_original.to_csv(
-            "output/HITACHI_원본데이터_FULL_fixed.csv",
+            self.report_output_dir / "HITACHI_원본데이터_FULL_fixed.csv",
             index=False,
             encoding="utf-8-sig",
         )
         siemens_original.to_csv(
-            "output/SIEMENS_원본데이터_FULL_fixed.csv",
+            self.report_output_dir / "SIEMENS_원본데이터_FULL_fixed.csv",
             index=False,
             encoding="utf-8-sig",
         )
         combined_original.to_csv(
-            "output/통합_원본데이터_FULL_fixed.csv", index=False, encoding="utf-8-sig"
+            self.report_output_dir / "통합_원본데이터_FULL_fixed.csv",
+            index=False,
+            encoding="utf-8-sig",
         )

         # Excel 파일 생성 (수정 버전)
         excel_filename = (
-            f"HVDC_입고로직_종합리포트_{self.timestamp}_v3.0-corrected.xlsx"
+            self.report_output_dir
+            / f"HVDC_입고로직_종합리포트_{self.timestamp}_v3.0-corrected.xlsx"
         )
         with pd.ExcelWriter(excel_filename, engine="xlsxwriter") as writer:
             warehouse_monthly_with_headers.to_excel(
                 writer, sheet_name="창고_월별_입출고", index=True
             )
             site_monthly_with_headers.to_excel(
                 writer, sheet_name="현장_월별_입고재고", index=True
             )
             flow_analysis.to_excel(writer, sheet_name="Flow_Code_분석", index=False)
             transaction_summary.to_excel(
                 writer, sheet_name="전체_트랜잭션_요약", index=False
             )
             kpi_validation_df.to_excel(writer, sheet_name="KPI_검증_결과", index=False)
             sqm_cumulative_sheet = self.create_sqm_cumulative_sheet(stats)
             sqm_cumulative_sheet.to_excel(
                 writer, sheet_name="SQM_누적재고", index=False
             )
             sqm_invoice_sheet = self.create_sqm_invoice_sheet(stats)
             sqm_invoice_sheet.to_excel(
                 writer, sheet_name="SQM_Invoice과금", index=False
             )
             sqm_pivot_sheet = self.create_sqm_pivot_sheet(stats)
             sqm_pivot_sheet.to_excel(writer, sheet_name="SQM_피벗테이블", index=False)
             sample_data.to_excel(writer, sheet_name="원본_데이터_샘플", index=False)
             #  FIX: 수정된 원본 데이터 시트들
             hitachi_original.to_excel(
                 writer, sheet_name="HITACHI_원본데이터_Fixed", index=False
             )
             siemens_original.to_excel(
                 writer, sheet_name="SIEMENS_원본데이터_Fixed", index=False
             )
             combined_original.to_excel(
                 writer, sheet_name="통합_원본데이터_Fixed", index=False
             )

         # 저장 후 검증
         try:
             _ = pd.read_excel(excel_filename, sheet_name=0)
         except Exception as e:
             print(f" [경고] 엑셀 파일 저장 후 열기 실패: {e}")

         logger.info(f" 최종 Excel 리포트 생성 완료: {excel_filename}")
-        logger.info(f" 원본 전체 데이터는 output/ 폴더의 CSV로도 저장됨")
+        logger.info(
+            " 원본 전체 데이터는 %s 경로의 CSV로도 저장됨",
+            self.report_output_dir,
+        )

         #  FIX: 수정사항 요약 출력
         print(f"\n v3.0-corrected 수정사항 요약:")
         print(f"    1. 창고 vs 현장 입고 분리")
         print(f"    2. 출고 타이밍 정확성 개선")
         print(f"    3. 재고 검증 로직 강화")
         print(f"    4. 이중 계산 방지")
         print(f"    5. Status_Location과 물리적 위치 교차 검증")
         print(f"    6. 입고/출고/재고 일관성 검증 강화")

         return excel_filename


 def main():
     """메인 실행 함수 (수정된 버전)"""
     print("HVDC 입고 로직 구현 및 집계 시스템 종합 보고서 (v3.0-corrected)")
     print("SUCCESS: 원본 데이터 보존 + AAA Storage 컬럼 누락 수정")
     print("Samsung C&T · ADNOC · DSV Partnership")
     print("=" * 80)

     try:
         #  패치 효과 검증 실행
         print("\n 패치 효과 검증 실행 중...")
         patch_validation = validate_patch_effectiveness()

