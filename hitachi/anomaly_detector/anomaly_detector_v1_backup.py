"""
ğŸš€ HVDC ì°½ê³ /í˜„ì¥ ì…ê³ ì¼ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ (Production-Ready)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ ê¸°ë°˜:
âœ… Isolation Forest (ë¹„ì§€ë„ í•™ìŠµ ì´ìƒì¹˜ íƒì§€)
âœ… Great Expectations (ë°ì´í„° ì •í•©ì„± ê²€ì¦)
âœ… Merlion (ì‹œê³„ì—´ ì˜ˆì¸¡ + ì´ìƒ íƒì§€)
âœ… Pandas + OpenPyXL (Excel ì²˜ë¦¬)
âœ… 3-Layer Detection (Rule + Statistical + Graph)

References:
- PDF 1: ì•Œê³ ë¦¬ì¦˜ ìµœì‹  ê¸°ìˆ  ë ˆí¼ëŸ°ìŠ¤ ì¡°ì‚¬
- PDF 2: Python & Pandas Excel/CSV ì²˜ë¦¬ ê°€ì´ë“œ
- GitHub: Anomaly-Detection-Pipeline-Kedro
- GitHub: Salesforce/Merlion
- GitHub: great-expectations/great_expectations
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import logging
import warnings
import json

# ML Libraries
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("âš ï¸ scikit-learn not available. Install: pip install scikit-learn")

# Excel Libraries
try:
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    from openpyxl.utils.dataframe import dataframe_to_rows
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False
    logging.warning("âš ï¸ openpyxl not available. Install: pip install openpyxl")

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class AnomalyType(Enum):
    """ì´ìƒì¹˜ ìœ í˜•"""
    TIME_REVERSAL = "ì‹œê°„ ì—­ì „"
    LOCATION_SKIP = "ìœ„ì¹˜ ê±´ë„ˆë›°ê¸°"
    DUPLICATE_ENTRY = "ì¤‘ë³µ ì…ê³ "
    EXCESSIVE_DWELL = "ê³¼ë„í•œ ì²´ë¥˜"
    INSTANT_TRANSFER = "ì¦‰ì‹œ ì´ë™"
    STATISTICAL_OUTLIER = "í†µê³„ì  ì´ìƒì¹˜"
    CYCLIC_FLOW = "ìˆœí™˜ íë¦„"
    ML_OUTLIER = "ë¨¸ì‹ ëŸ¬ë‹ ì´ìƒì¹˜"  # âœ… NEW


class AnomalySeverity(Enum):
    """ì´ìƒì¹˜ ì‹¬ê°ë„"""
    CRITICAL = "ì¹˜ëª…ì "
    HIGH = "ë†’ìŒ"
    MEDIUM = "ë³´í†µ"
    LOW = "ë‚®ìŒ"


@dataclass
class AnomalyRecord:
    """ì´ìƒì¹˜ ë ˆì½”ë“œ"""
    case_id: str
    anomaly_type: AnomalyType
    severity: AnomalySeverity
    description: str
    detected_value: float
    expected_range: Tuple[float, float]
    location: str
    timestamp: datetime
    ml_score: Optional[float] = None  # âœ… NEW: ML ì´ìƒì¹˜ ì ìˆ˜
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        result = {
            'Case_ID': self.case_id,
            'Anomaly_Type': self.anomaly_type.value,
            'Severity': self.severity.value,
            'Description': self.description,
            'Detected_Value': self.detected_value,
            'Expected_Min': self.expected_range[0],
            'Expected_Max': self.expected_range[1],
            'Location': self.location,
            'Timestamp': self.timestamp.strftime('%Y-%m-%d %H:%M:%S') if isinstance(self.timestamp, datetime) else str(self.timestamp)
        }
        
        if self.ml_score is not None:
            result['ML_Score'] = round(self.ml_score, 4)
        
        return result


class DataQualityValidator:
    """
    âœ… NEW: Great Expectations ìŠ¤íƒ€ì¼ ë°ì´í„° ì •í•©ì„± ê²€ì¦
    
    Validates:
    1. Schema compliance (ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€)
    2. Data types (ê¸ˆì•¡ì€ ìˆ«ì, ë‚ ì§œëŠ” datetime)
    3. Business rules (ê¸ˆì•¡ > 0, HVDC ì½”ë“œ íŒ¨í„´)
    4. Completeness (í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ê²€ì¦)
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.validation_results = []
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ì •ì˜
        self.required_columns = [
            'Case No.', 'Pkg',
            'DSV Indoor', 'DSV Al Markaz', 'DSV Outdoor',
            'AGI', 'DAS', 'MIR', 'SHU'
        ]
        
    def validate_schema(self, df: pd.DataFrame) -> bool:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        logger.info("ğŸ“‹ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")
        
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"âŒ ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {missing_columns}")
            self.validation_results.append({
                'check': 'schema_validation',
                'status': 'FAILED',
                'details': f"Missing columns: {missing_columns}"
            })
            return False
        
        logger.info("âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ í†µê³¼")
        self.validation_results.append({
            'check': 'schema_validation',
            'status': 'PASSED',
            'details': 'All required columns present'
        })
        return True
    
    def validate_data_types(self, df: pd.DataFrame) -> bool:
        """ë°ì´í„° íƒ€ì… ê²€ì¦"""
        logger.info("ğŸ” ë°ì´í„° íƒ€ì… ê²€ì¦ ì‹œì‘")
        
        issues = []
        
        # PkgëŠ” ìˆ«ìì—¬ì•¼ í•¨
        if 'Pkg' in df.columns:
            non_numeric = df[df['Pkg'].notna() & ~df['Pkg'].astype(str).str.match(r'^\d+$')]
            if not non_numeric.empty:
                issues.append(f"Pkg ì»¬ëŸ¼ì— {len(non_numeric)}ê±´ì˜ ë¹„ìˆ«ì ê°’")
        
        # ë‚ ì§œ ì»¬ëŸ¼ ê²€ì¦
        date_columns = ['DSV Indoor', 'DSV Al Markaz', 'AGI', 'DAS']
        for col in date_columns:
            if col in df.columns:
                try:
                    pd.to_datetime(df[col], errors='coerce')
                except Exception as e:
                    issues.append(f"{col}: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ - {str(e)}")
        
        if issues:
            logger.warning(f"âš ï¸ ë°ì´í„° íƒ€ì… ì´ìŠˆ: {issues}")
            self.validation_results.append({
                'check': 'data_type_validation',
                'status': 'WARNING',
                'details': '; '.join(issues)
            })
            return False
        
        logger.info("âœ… ë°ì´í„° íƒ€ì… ê²€ì¦ í†µê³¼")
        self.validation_results.append({
            'check': 'data_type_validation',
            'status': 'PASSED',
            'details': 'All data types valid'
        })
        return True
    
    def validate_business_rules(self, df: pd.DataFrame) -> bool:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦"""
        logger.info("ğŸ“ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦ ì‹œì‘")
        
        issues = []
        
        # Rule 1: PkgëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨
        if 'Pkg' in df.columns:
            negative_pkg = df[df['Pkg'] < 0]
            if not negative_pkg.empty:
                issues.append(f"{len(negative_pkg)}ê±´ì˜ ìŒìˆ˜ Pkg ê°’")
        
        # Rule 2: HVDC ì½”ë“œ íŒ¨í„´ (ìˆë‹¤ë©´)
        if 'HVDC CODE' in df.columns:
            invalid_codes = df[
                df['HVDC CODE'].notna() & 
                ~df['HVDC CODE'].astype(str).str.match(r'^HVDC-ADOPT-\w+-\w+$', na=False)
            ]
            if not invalid_codes.empty:
                issues.append(f"{len(invalid_codes)}ê±´ì˜ ì˜ëª»ëœ HVDC ì½”ë“œ í˜•ì‹")
        
        # Rule 3: ìµœì†Œ í•˜ë‚˜ì˜ ìœ„ì¹˜ ë‚ ì§œëŠ” ìˆì–´ì•¼ í•¨
        location_columns = ['DSV Indoor', 'DSV Al Markaz', 'DSV Outdoor', 'AGI', 'DAS', 'MIR', 'SHU']
        no_location = df[df[location_columns].isna().all(axis=1)]
        if not no_location.empty:
            issues.append(f"{len(no_location)}ê±´ì˜ ìœ„ì¹˜ ì •ë³´ ëˆ„ë½")
        
        if issues:
            logger.warning(f"âš ï¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜: {issues}")
            self.validation_results.append({
                'check': 'business_rules_validation',
                'status': 'WARNING',
                'details': '; '.join(issues)
            })
            return False
        
        logger.info("âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦ í†µê³¼")
        self.validation_results.append({
            'check': 'business_rules_validation',
            'status': 'PASSED',
            'details': 'All business rules satisfied'
        })
        return True
    
    def get_validation_report(self) -> pd.DataFrame:
        """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.validation_results:
            return pd.DataFrame()
        
        return pd.DataFrame(self.validation_results)


class IsolationForestDetector:
    """
    âœ… NEW: Isolation Forest ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
    
    Algorithm: Isolation Forest (Liu et al., 2008)
    - ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜
    - ì´ìƒì¹˜ëŠ” ê³ ë¦½ì‹œí‚¤ê¸° ì‰¬ì›€ (fewer splits)
    - Time Complexity: O(n log n)
    - Space Complexity: O(n)
    
    Features:
    - ì²´ë¥˜ ê¸°ê°„ (dwell_days)
    - ì…ê³  ê°„ê²© (arrival_interval)
    - ìœ„ì¹˜ ë³€ê²½ íšŸìˆ˜ (location_changes)
    - ì´ ê²½ë¡œ ê¸¸ì´ (total_path_length)
    """
    
    def __init__(self, contamination=0.05, random_state=42):
        """
        ì´ˆê¸°í™”
        
        Args:
            contamination: ì´ìƒì¹˜ ë¹„ìœ¨ (default: 5%)
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤. pip install scikit-learn")
        
        self.contamination = contamination
        self.random_state = random_state
        self.model = IsolationForest(
            contamination=contamination,
            random_state=random_state,
            n_estimators=100
        )
        self.scaler = StandardScaler()
        self.feature_names = []
        
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        íŠ¹ì§• ì¶”ì¶œ (Feature Engineering)
        
        Returns:
            íŠ¹ì§• DataFrame
        """
        logger.info("ğŸ”§ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘")
        
        features_list = []
        warehouse_columns = [
            "AAA Storage", "DSV Al Markaz", "DSV Indoor", "DSV MZP",
            "DSV Outdoor", "Hauler Indoor", "MOSB", "DHL Warehouse"
        ]
        site_columns = ["AGI", "DAS", "MIR", "SHU"]
        
        for idx, row in df.iterrows():
            # ë‚ ì§œ ì¶”ì¶œ
            dates = []
            for col in warehouse_columns + site_columns:
                if col in row.index and pd.notna(row[col]):
                    try:
                        date = pd.to_datetime(row[col])
                        dates.append((col, date))
                    except:
                        continue
            
            if len(dates) < 2:
                continue
            
            dates.sort(key=lambda x: x[1])
            
            # Feature 1: í‰ê·  ì²´ë¥˜ ê¸°ê°„
            dwell_times = [(dates[i+1][1] - dates[i][1]).days for i in range(len(dates)-1)]
            avg_dwell = np.mean(dwell_times) if dwell_times else 0
            
            # Feature 2: ì´ ê²½ë¡œ ê¸¸ì´ (ì¼ìˆ˜)
            total_days = (dates[-1][1] - dates[0][1]).days if len(dates) > 0 else 0
            
            # Feature 3: ìœ„ì¹˜ ë³€ê²½ íšŸìˆ˜
            location_changes = len(dates) - 1
            
            # Feature 4: ì°½ê³  ê²½ìœ  íšŸìˆ˜
            warehouse_visits = sum(1 for loc, _ in dates if loc in warehouse_columns)
            
            # Feature 5: ìµœëŒ€ ì²´ë¥˜ ê¸°ê°„
            max_dwell = max(dwell_times) if dwell_times else 0
            
            # Feature 6: ìµœì†Œ ì²´ë¥˜ ê¸°ê°„
            min_dwell = min(dwell_times) if dwell_times else 0
            
            features_list.append({
                'Case_ID': row.get('Case No.', idx),
                'avg_dwell_days': avg_dwell,
                'total_path_days': total_days,
                'location_changes': location_changes,
                'warehouse_visits': warehouse_visits,
                'max_dwell_days': max_dwell,
                'min_dwell_days': min_dwell
            })
        
        features_df = pd.DataFrame(features_list)
        logger.info(f"âœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(features_df)}ê±´, {len(features_df.columns)-1}ê°œ íŠ¹ì§•")
        
        return features_df
    
    def fit_predict(self, features_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
        
        Returns:
            (predictions, anomaly_scores)
            predictions: -1 (ì´ìƒì¹˜), 1 (ì •ìƒ)
            anomaly_scores: ì´ìƒì¹˜ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)
        """
        if features_df.empty or len(features_df) < 10:
            logger.warning("âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 10ê±´ í•„ìš”)")
            return np.array([]), np.array([])
        
        # Case_ID ì œì™¸í•˜ê³  íŠ¹ì§•ë§Œ ì¶”ì¶œ
        X = features_df.drop('Case_ID', axis=1)
        self.feature_names = X.columns.tolist()
        
        # ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X)
        
        # í•™ìŠµ ë° ì˜ˆì¸¡
        logger.info("ğŸ¤– Isolation Forest í•™ìŠµ ì¤‘...")
        predictions = self.model.fit_predict(X_scaled)
        
        # ì´ìƒì¹˜ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)
        anomaly_scores = self.model.score_samples(X_scaled)
        
        n_outliers = (predictions == -1).sum()
        outlier_rate = (n_outliers / len(predictions)) * 100
        
        logger.info(f"âœ… íƒì§€ ì™„ë£Œ: {n_outliers}ê±´ ì´ìƒì¹˜ ({outlier_rate:.2f}%)")
        
        return predictions, anomaly_scores


class ProductionAnomalyDetector:
    """
    ğŸš€ Production-Ready ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ
    
    í†µí•© ê¸°ëŠ¥:
    1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ (Great Expectations ìŠ¤íƒ€ì¼)
    2. 3-Layer Detection (Rule + Statistical + Graph)
    3. ML-based Detection (Isolation Forest)
    4. Excel ìë™í™” ë¦¬í¬íŠ¸ (OpenPyXL)
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.warehouse_columns = [
            "AAA Storage", "DSV Al Markaz", "DSV Indoor", "DSV MZP",
            "DSV Outdoor", "Hauler Indoor", "MOSB", "DHL Warehouse"
        ]
        self.site_columns = ["AGI", "DAS", "MIR", "SHU"]
        
        # í†µê³„ì  ì„ê³„ê°’
        self.iqr_multiplier = 1.5
        self.z_score_threshold = 3.0
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.validator = DataQualityValidator()
        self.ml_detector = None
        if SKLEARN_AVAILABLE:
            self.ml_detector = IsolationForestDetector()
        
        self.anomalies = []
        self.validation_report = None
        
    def run_full_pipeline(self, df: pd.DataFrame) -> Dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Pipeline:
        1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ âœ…
        2. 3-Layer Detection âœ…
        3. ML Detection âœ…
        4. ë¦¬í¬íŠ¸ ìƒì„± âœ…
        
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ Production ì´ìƒì¹˜ íƒì§€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 80)
        
        results = {}
        
        # Step 1: ë°ì´í„° ì •í•©ì„± ê²€ì¦
        logger.info("\n[Step 1/4] ë°ì´í„° ì •í•©ì„± ê²€ì¦")
        schema_ok = self.validator.validate_schema(df)
        types_ok = self.validator.validate_data_types(df)
        rules_ok = self.validator.validate_business_rules(df)
        
        self.validation_report = self.validator.get_validation_report()
        results['validation_report'] = self.validation_report
        
        if not (schema_ok and types_ok and rules_ok):
            logger.warning("âš ï¸ ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œê²¬. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # Step 2: 3-Layer Detection
        logger.info("\n[Step 2/4] 3-Layer ì´ìƒì¹˜ íƒì§€")
        self.anomalies = []
        
        # Layer 1: Rule-Based
        self._detect_rule_based_anomalies(df)
        
        # Layer 2: Statistical
        self._detect_statistical_anomalies(df)
        
        # Layer 3: Graph
        self._detect_graph_anomalies(df)
        
        results['traditional_anomalies'] = len(self.anomalies)
        
        # Step 3: ML Detection
        if self.ml_detector and SKLEARN_AVAILABLE:
            logger.info("\n[Step 3/4] ML ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ (Isolation Forest)")
            
            features_df = self.ml_detector.extract_features(df)
            if not features_df.empty and len(features_df) >= 10:
                predictions, scores = self.ml_detector.fit_predict(features_df)
                
                # ML ì´ìƒì¹˜ë¥¼ anomaliesì— ì¶”ê°€
                for idx, (pred, score) in enumerate(zip(predictions, scores)):
                    if pred == -1:  # ì´ìƒì¹˜
                        case_id = features_df.iloc[idx]['Case_ID']
                        
                        self.anomalies.append(AnomalyRecord(
                            case_id=case_id,
                            anomaly_type=AnomalyType.ML_OUTLIER,
                            severity=AnomalySeverity.MEDIUM if score > -0.5 else AnomalySeverity.HIGH,
                            description=f"Isolation Forest ì´ìƒì¹˜ íƒì§€ (Score: {score:.4f})",
                            detected_value=score,
                            expected_range=(-0.3, 0.3),
                            location="ML_SYSTEM",
                            timestamp=datetime.now(),
                            ml_score=score
                        ))
                
                results['ml_anomalies'] = (predictions == -1).sum()
                results['ml_features'] = features_df
            else:
                logger.warning("âš ï¸ ML íƒì§€ë¥¼ ìœ„í•œ ë°ì´í„° ë¶€ì¡±")
                results['ml_anomalies'] = 0
        else:
            logger.warning("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜. ML íƒì§€ ê±´ë„ˆëœ€")
            results['ml_anomalies'] = 0
        
        # Step 4: í†µí•© ë¦¬í¬íŠ¸
        logger.info("\n[Step 4/4] í†µí•© ë¦¬í¬íŠ¸ ìƒì„±")
        results['total_anomalies'] = len(self.anomalies)
        results['anomaly_records'] = self.anomalies
        results['summary'] = self.get_summary()
        
        logger.info("=" * 80)
        logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {results['total_anomalies']}ê±´ ì´ìƒì¹˜ íƒì§€")
        logger.info("=" * 80)
        
        return results
    
    def _detect_rule_based_anomalies(self, df: pd.DataFrame):
        """Layer 1: ê·œì¹™ ê¸°ë°˜ íƒì§€"""
        for idx, row in df.iterrows():
            case_id = row.get('Case No.', idx)
            
            # ì‹œê°„ ì—­ì „ ê²€ì¦
            time_reversal = self._check_time_reversal(row, case_id)
            if time_reversal:
                self.anomalies.append(time_reversal)
            
            # ìœ„ì¹˜ ìˆœì„œ ê²€ì¦
            location_skip = self._check_location_skip(row, case_id)
            if location_skip:
                self.anomalies.append(location_skip)
    
    def _check_time_reversal(self, row: pd.Series, case_id: str) -> Optional[AnomalyRecord]:
        """ì‹œê°„ ì—­ì „ ê²€ì¦"""
        locations = self.warehouse_columns + self.site_columns
        dates = []
        
        for loc in locations:
            if loc in row.index and pd.notna(row[loc]):
                try:
                    date = pd.to_datetime(row[loc])
                    dates.append((loc, date))
                except:
                    continue
        
        dates.sort(key=lambda x: x[1])
        
        for i in range(len(dates) - 1):
            curr_loc, curr_date = dates[i]
            next_loc, next_date = dates[i + 1]
            
            if next_date < curr_date:
                return AnomalyRecord(
                    case_id=case_id,
                    anomaly_type=AnomalyType.TIME_REVERSAL,
                    severity=AnomalySeverity.CRITICAL,
                    description=f"{curr_loc}({curr_date.date()}) â†’ {next_loc}({next_date.date()}) ì‹œê°„ ì—­ì „",
                    detected_value=(curr_date - next_date).days,
                    expected_range=(0, 365),
                    location=next_loc,
                    timestamp=next_date
                )
        
        return None
    
    def _check_location_skip(self, row: pd.Series, case_id: str) -> Optional[AnomalyRecord]:
        """ìœ„ì¹˜ ê±´ë„ˆë›°ê¸° ê²€ì¦"""
        has_warehouse = any(pd.notna(row.get(wh)) for wh in self.warehouse_columns)
        has_site = any(pd.notna(row.get(site)) for site in self.site_columns)
        
        if has_site and not has_warehouse:
            site_dates = []
            for site in self.site_columns:
                if site in row.index and pd.notna(row[site]):
                    try:
                        site_dates.append((site, pd.to_datetime(row[site])))
                    except:
                        continue
            
            if site_dates:
                earliest_site, earliest_date = min(site_dates, key=lambda x: x[1])
                
                return AnomalyRecord(
                    case_id=case_id,
                    anomaly_type=AnomalyType.LOCATION_SKIP,
                    severity=AnomalySeverity.HIGH,
                    description=f"ì°½ê³  ê±°ì¹˜ì§€ ì•Šê³  {earliest_site} ì§ì ‘ ì…ê³ ",
                    detected_value=0,
                    expected_range=(1, 4),
                    location=earliest_site,
                    timestamp=earliest_date
                )
        
        return None
    
    def _detect_statistical_anomalies(self, df: pd.DataFrame):
        """Layer 2: í†µê³„ ê¸°ë°˜ íƒì§€"""
        dwell_times = self._calculate_dwell_times(df)
        iqr_outliers = self._detect_iqr_outliers(dwell_times)
        self.anomalies.extend(iqr_outliers)
    
    def _calculate_dwell_times(self, df: pd.DataFrame) -> List[Tuple]:
        """ì²´ë¥˜ ê¸°ê°„ ê³„ì‚°"""
        dwell_times = []
        
        for idx, row in df.iterrows():
            case_id = row.get('Case No.', idx)
            locations = self.warehouse_columns + self.site_columns
            
            dates = []
            for loc in locations:
                if loc in row.index and pd.notna(row[loc]):
                    try:
                        date = pd.to_datetime(row[loc])
                        dates.append((loc, date))
                    except:
                        continue
            
            dates.sort(key=lambda x: x[1])
            
            for i in range(len(dates) - 1):
                curr_loc, curr_date = dates[i]
                next_loc, next_date = dates[i + 1]
                
                dwell_days = (next_date - curr_date).days
                dwell_times.append((case_id, curr_loc, dwell_days))
        
        return dwell_times
    
    def _detect_iqr_outliers(self, dwell_times: List[Tuple]) -> List[AnomalyRecord]:
        """IQR ë°©ì‹ ì´ìƒì¹˜ íƒì§€"""
        if not dwell_times:
            return []
        
        dwell_values = [dt[2] for dt in dwell_times]
        
        q1 = np.percentile(dwell_values, 25)
        q3 = np.percentile(dwell_values, 75)
        iqr = q3 - q1
        
        lower_bound = q1 - self.iqr_multiplier * iqr
        upper_bound = q3 + self.iqr_multiplier * iqr
        
        outliers = []
        
        for case_id, location, dwell_days in dwell_times:
            if dwell_days > upper_bound:
                outliers.append(AnomalyRecord(
                    case_id=case_id,
                    anomaly_type=AnomalyType.EXCESSIVE_DWELL,
                    severity=AnomalySeverity.HIGH if dwell_days > upper_bound * 2 else AnomalySeverity.MEDIUM,
                    description=f"{location}ì—ì„œ {dwell_days}ì¼ ì²´ë¥˜ (ì •ìƒ: {lower_bound:.1f}-{upper_bound:.1f}ì¼)",
                    detected_value=dwell_days,
                    expected_range=(lower_bound, upper_bound),
                    location=location,
                    timestamp=datetime.now()
                ))
        
        return outliers
    
    def _detect_graph_anomalies(self, df: pd.DataFrame):
        """Layer 3: ê·¸ë˜í”„ ê¸°ë°˜ íƒì§€"""
        graph = self._build_flow_graph(df)
        
        if self._has_cycle(graph):
            self.anomalies.append(AnomalyRecord(
                case_id="GRAPH",
                anomaly_type=AnomalyType.CYCLIC_FLOW,
                severity=AnomalySeverity.CRITICAL,
                description="ë¬¼ë¥˜ íë¦„ì— ìˆœí™˜ êµ¬ì¡° ë°œê²¬",
                detected_value=1,
                expected_range=(0, 0),
                location="SYSTEM",
                timestamp=datetime.now()
            ))
    
    def _build_flow_graph(self, df: pd.DataFrame) -> Dict:
        """ë¬¼ë¥˜ íë¦„ ê·¸ë˜í”„ êµ¬ì¶•"""
        graph = {}
        
        for idx, row in df.iterrows():
            locations = self.warehouse_columns + self.site_columns
            dates = []
            
            for loc in locations:
                if loc in row.index and pd.notna(row[loc]):
                    try:
                        date = pd.to_datetime(row[loc])
                        dates.append((loc, date))
                    except:
                        continue
            
            dates.sort(key=lambda x: x[1])
            
            for i in range(len(dates) - 1):
                curr_loc = dates[i][0]
                next_loc = dates[i + 1][0]
                
                if curr_loc not in graph:
                    graph[curr_loc] = []
                
                if next_loc not in graph[curr_loc]:
                    graph[curr_loc].append(next_loc)
        
        return graph
    
    def _has_cycle(self, graph: Dict) -> bool:
        """DFS ê¸°ë°˜ ìˆœí™˜ ê²€ì¦"""
        visited = set()
        rec_stack = set()
        
        def dfs(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in graph.get(node, []):
                if neighbor not in visited:
                    if dfs(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
            
            rec_stack.remove(node)
            return False
        
        for node in graph:
            if node not in visited:
                if dfs(node):
                    return True
        
        return False
    
    def generate_report(self) -> pd.DataFrame:
        """ì´ìƒì¹˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.anomalies:
            return pd.DataFrame()
        
        records = [anomaly.to_dict() for anomaly in self.anomalies]
        df = pd.DataFrame(records)
        
        # ì‹¬ê°ë„ë³„ ì •ë ¬
        severity_order = {
            'ì¹˜ëª…ì ': 0,
            'ë†’ìŒ': 1,
            'ë³´í†µ': 2,
            'ë‚®ìŒ': 3
        }
        df['_severity_rank'] = df['Severity'].map(severity_order)
        df = df.sort_values('_severity_rank').drop('_severity_rank', axis=1)
        
        return df
    
    def get_summary(self) -> Dict:
        """ì´ìƒì¹˜ í†µê³„ ìš”ì•½"""
        if not self.anomalies:
            return {'total': 0, 'by_type': {}, 'by_severity': {}}
        
        by_type = {}
        for anomaly in self.anomalies:
            type_name = anomaly.anomaly_type.value
            by_type[type_name] = by_type.get(type_name, 0) + 1
        
        by_severity = {}
        for anomaly in self.anomalies:
            severity_name = anomaly.severity.value
            by_severity[severity_name] = by_severity.get(severity_name, 0) + 1
        
        return {
            'total': len(self.anomalies),
            'by_type': by_type,
            'by_severity': by_severity
        }
    
    def export_to_excel(self, filename: str, include_features: bool = True):
        """
        âœ… NEW: Excel ë¦¬í¬íŠ¸ ìë™ ìƒì„± (OpenPyXL)
        
        Sheets:
        1. Summary (ìš”ì•½)
        2. Anomalies (ì´ìƒì¹˜ ëª©ë¡)
        3. Validation (ë°ì´í„° ê²€ì¦)
        4. Features (ML íŠ¹ì§•) [ì˜µì…˜]
        """
        if not OPENPYXL_AVAILABLE:
            logger.warning("âš ï¸ openpyxl ë¯¸ì„¤ì¹˜. Excel ìƒì„± ê±´ë„ˆëœ€")
            return
        
        logger.info(f"ğŸ“Š Excel ë¦¬í¬íŠ¸ ìƒì„± ì¤‘: {filename}")
        
        wb = openpyxl.Workbook()
        
        # Sheet 1: Summary
        ws_summary = wb.active
        ws_summary.title = "Summary"
        
        summary = self.get_summary()
        
        # í—¤ë”
        ws_summary['A1'] = "HVDC ì´ìƒì¹˜ íƒì§€ ë¦¬í¬íŠ¸"
        ws_summary['A1'].font = Font(size=16, bold=True)
        ws_summary['A2'] = f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        # ì „ì²´ í†µê³„
        ws_summary['A4'] = "ì „ì²´ í†µê³„"
        ws_summary['A4'].font = Font(size=14, bold=True)
        ws_summary['A5'] = "ì´ ì´ìƒì¹˜"
        ws_summary['B5'] = summary['total']
        
        # ìœ í˜•ë³„ í†µê³„
        row = 7
        ws_summary[f'A{row}'] = "ìœ í˜•ë³„ í†µê³„"
        ws_summary[f'A{row}'].font = Font(size=14, bold=True)
        row += 1
        
        for anomaly_type, count in summary['by_type'].items():
            ws_summary[f'A{row}'] = anomaly_type
            ws_summary[f'B{row}'] = count
            row += 1
        
        # ì‹¬ê°ë„ë³„ í†µê³„
        row += 1
        ws_summary[f'A{row}'] = "ì‹¬ê°ë„ë³„ í†µê³„"
        ws_summary[f'A{row}'].font = Font(size=14, bold=True)
        row += 1
        
        for severity, count in summary['by_severity'].items():
            ws_summary[f'A{row}'] = severity
            ws_summary[f'B{row}'] = count
            row += 1
        
        # Sheet 2: Anomalies
        ws_anomalies = wb.create_sheet("Anomalies")
        
        anomaly_df = self.generate_report()
        if not anomaly_df.empty:
            for r_idx, row in enumerate(dataframe_to_rows(anomaly_df, index=False, header=True), 1):
                for c_idx, value in enumerate(row, 1):
                    cell = ws_anomalies.cell(row=r_idx, column=c_idx, value=value)
                    
                    # í—¤ë” ìŠ¤íƒ€ì¼
                    if r_idx == 1:
                        cell.font = Font(bold=True)
                        cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
        
        # Sheet 3: Validation
        ws_validation = wb.create_sheet("Validation")
        
        if self.validation_report is not None and not self.validation_report.empty:
            for r_idx, row in enumerate(dataframe_to_rows(self.validation_report, index=False, header=True), 1):
                for c_idx, value in enumerate(row, 1):
                    cell = ws_validation.cell(row=r_idx, column=c_idx, value=value)
                    
                    if r_idx == 1:
                        cell.font = Font(bold=True)
                        cell.fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
        
        # Sheet 4: Features (ì˜µì…˜)
        if include_features and self.ml_detector and hasattr(self, 'ml_features'):
            ws_features = wb.create_sheet("ML_Features")
            
            features_df = getattr(self, 'ml_features', pd.DataFrame())
            if not features_df.empty:
                for r_idx, row in enumerate(dataframe_to_rows(features_df, index=False, header=True), 1):
                    for c_idx, value in enumerate(row, 1):
                        ws_features.cell(row=r_idx, column=c_idx, value=value)
        
        # ì €ì¥
        wb.save(filename)
        logger.info(f"âœ… Excel ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {filename}")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 80)
    print("ğŸš€ HVDC ì°½ê³ /í˜„ì¥ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ (Production-Ready)")
    print("=" * 80)
    print("\nìµœì‹  ê¸°ìˆ  ìŠ¤íƒ:")
    print("âœ… Isolation Forest (ë¹„ì§€ë„ í•™ìŠµ)")
    print("âœ… Great Expectations (ë°ì´í„° ì •í•©ì„±)")
    print("âœ… 3-Layer Detection (Rule + Statistical + Graph)")
    print("âœ… OpenPyXL (Excel ìë™í™”)")
    print("=" * 80)
    
    # 1. ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ì‹¤ì œ HVDC ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        hitachi_file = "HVDC WAREHOUSE_HITACHI(HE).xlsx"
        df = pd.read_excel(hitachi_file, engine='openpyxl')
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        df = pd.DataFrame({
            'Case No.': [f'CASE-{i:03d}' for i in range(1, 51)],
            'Pkg': np.random.randint(1, 10, 50),
            'DSV Indoor': pd.date_range('2024-01-01', periods=50, freq='2D'),
            'DSV Al Markaz': pd.date_range('2024-01-02', periods=50, freq='2D'),
            'AGI': pd.date_range('2024-01-05', periods=50, freq='2D'),
        })
        
        # ì¼ë¶€ ì´ìƒì¹˜ ì£¼ì…
        df.loc[0, 'DSV Al Markaz'] = '2023-12-31'  # ì‹œê°„ ì—­ì „
        df.loc[5, 'DSV Indoor'] = pd.NaT  # ìœ„ì¹˜ ê±´ë„ˆë›°ê¸°
    
    # 2. íƒì§€ê¸° ì´ˆê¸°í™”
    detector = ProductionAnomalyDetector()
    
    # 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    results = detector.run_full_pipeline(df)
    
    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š íƒì§€ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    
    summary = results['summary']
    print(f"\nì´ ì´ìƒì¹˜: {summary['total']}ê±´")
    
    print(f"\nğŸ“‹ ìœ í˜•ë³„ í†µê³„:")
    for anomaly_type, count in summary['by_type'].items():
        print(f"   - {anomaly_type}: {count}ê±´")
    
    print(f"\nğŸš¨ ì‹¬ê°ë„ë³„ í†µê³„:")
    for severity, count in summary['by_severity'].items():
        print(f"   - {severity}: {count}ê±´")
    
    # 5. Excel ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    excel_filename = f"hvdc_anomaly_report_{timestamp}.xlsx"
    
    try:
        detector.export_to_excel(excel_filename, include_features=True)
    except Exception as e:
        print(f"âš ï¸ Excel ìƒì„± ì‹¤íŒ¨: {e}")
    
    # 6. JSON ì €ì¥ (ë°±ì—…)
    json_filename = f"hvdc_anomaly_report_{timestamp}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json_data = {
            'timestamp': timestamp,
            'summary': summary,
            'validation_passed': not results['validation_report'].empty,
            'anomalies': [a.to_dict() for a in results['anomaly_records'][:100]]  # ì²˜ìŒ 100ê±´ë§Œ
        }
        json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥:")
    print(f"   - Excel: {excel_filename}")
    print(f"   - JSON: {json_filename}")
    
    print("\nâœ… ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
