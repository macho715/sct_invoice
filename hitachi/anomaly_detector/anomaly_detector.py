# -*- coding: utf-8 -*-
"""
HVDC ì…ì¶œê³  ì´ìƒì¹˜ íƒì§€ v2 (Hybrid / Plugin-based)
- Rule + Statistical + ML(PyOD/Sklearn) 3-Layer
- í—¤ë” ì •ê·œí™”, ì ìˆ˜ ìº˜ë¦¬ë¸Œë ˆì´ì…˜(ECDF), 30ì´ˆ ì•Œë¦¼ ì„ê³„
- Excel/JSON ë¦¬í¬íŠ¸(ì„ íƒ), ë°°ì¹˜/ì›Œì»¤ íŒŒë¼ë¯¸í„°
"""
from __future__ import annotations

import json
import logging
import math
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd

# ----- Optional deps (graceful fallback) --------------------------------------
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except Exception:
    SKLEARN_AVAILABLE = False
    IsolationForest = object  # type: ignore
    StandardScaler = object   # type: ignore

try:
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    from openpyxl.utils.dataframe import dataframe_to_rows
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

try:
    # PyODëŠ” ë‹¤ì–‘í•œ ë¹„ì§€ë„ ì´ìƒì¹˜ ì•Œê³ ë¦¬ì¦˜ ì œê³µ(ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©)
    from pyod.models.iforest import IForest as PyODIForest  # type: ignore
    PYOD_AVAILABLE = True
except Exception:
    PYOD_AVAILABLE = False

# ----- Logging ----------------------------------------------------------------
logger = logging.getLogger("hvdc.anomaly")
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
    )

# ----- Domain enums / schema ---------------------------------------------------
class AnomalyType(Enum):
    TIME_REVERSAL = "ì‹œê°„ ì—­ì „"
    LOCATION_SKIP = "ìœ„ì¹˜ ìŠ¤í‚µ"
    EXCESSIVE_DWELL = "ê³¼ë„ ì²´ë¥˜"
    ML_OUTLIER = "ë¨¸ì‹ ëŸ¬ë‹ ì´ìƒì¹˜"
    DATA_QUALITY = "ë°ì´í„° í’ˆì§ˆ"

class AnomalySeverity(Enum):
    CRITICAL = "ì¹˜ëª…ì "
    HIGH = "ë†’ìŒ"
    MEDIUM = "ë³´í†µ"
    LOW = "ë‚®ìŒ"

@dataclass(frozen=True)
class AnomalyRecord:
    case_id: str
    anomaly_type: AnomalyType
    severity: AnomalySeverity
    description: str
    detected_value: Optional[float]
    expected_range: Optional[Tuple[float, float]]
    location: Optional[str]
    timestamp: datetime
    risk_score: Optional[float] = None  # [0..1] calibrated
    
    def to_dict(self) -> Dict:
        return {
            "Case_ID": self.case_id,
            "Anomaly_Type": self.anomaly_type.value,
            "Severity": self.severity.value,
            "Description": self.description,
            "Detected_Value": self.detected_value,
            "Expected_Range": self.expected_range,
            "Location": self.location,
            "Timestamp": self.timestamp.isoformat(),
            "Risk_Score": None if self.risk_score is None else round(float(self.risk_score), 4),
        }

# ----- Config -----------------------------------------------------------------
@dataclass
class DetectorConfig:
    # í—¤ë” ì •ê·œí™”(ë™ì˜ì–´ ë§¤í•‘): Master > Slave
    column_map: Dict[str, str] = None
    # ì°½ê³ /í˜„ì¥ ì—´(ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©)
    warehouse_columns: List[str] = None
    site_columns: List[str] = None

    # í†µê³„ íƒì§€ íŒŒë¼ë¯¸í„°
    iqr_k: float = 1.5
    mad_k: float = 3.5

    # ML íƒì§€ íŒŒë¼ë¯¸í„°
    use_pyod_first: bool = True
    contamination: float = 0.02  # 2% ê°€ì •(ë°ì´í„°ì— ë”°ë¼ ì¡°ì ˆ)
    random_state: int = 42

    # ë°°ì¹˜/ì›Œì»¤
    batch_size: int = 1000
    max_workers: int = 8  # (ìš”êµ¬ì‚¬í•­: 32 ì´í•˜)

    # ì•Œë¦¼
    alert_window_sec: int = 30
    min_risk_to_alert: float = 0.8  # 0.0~1.0

    def __post_init__(self):
        if self.column_map is None:
            # Master í—¤ë” ì´ë¦„ìœ¼ë¡œ ì •ê·œí™”
            self.column_map = {
                # í‚¤ í•„ë“œ
                "Case No.": "CASE_NO",
                "CASE NO": "CASE_NO",
                "CASE_NO": "CASE_NO",
                # HVDC CODE
                "HVDC CODE": "HVDC_CODE",
                "HVDC Code": "HVDC_CODE",
                # ì°½ê³ /í˜„ì¥(í‘œê¸° ë³€í˜• í¡ìˆ˜)
                "DSV Indoor": "DSV_INDOOR",
                "DSV Al Markaz": "DSV_AL_MARKAZ",
                "AAA Storage": "AAA_STORAGE",
                "DSV Outdoor": "DSV_OUTDOOR",
                "MOSB": "MOSB",
                "Hauler Indoor": "HAULER_INDOOR",
                "DHL Warehouse": "DHL_WAREHOUSE",
                "DSV MZP": "DSV_MZP",
                "AGI": "AGI",
                "DAS": "DAS",
                "MIR": "MIR",
                "SHU": "SHU",
                # ê¸ˆì•¡/ìˆ˜ëŸ‰ ë“±
                "ê¸ˆì•¡": "AMOUNT",
                "ìˆ˜ëŸ‰": "QTY",
                "Pkg": "PKG",
            }
        if self.warehouse_columns is None:
            self.warehouse_columns = [
                "AAA_STORAGE", "DSV_AL_MARKAZ", "DSV_INDOOR", "DSV_MZP",
                "DSV_OUTDOOR", "HAULER_INDOOR", "MOSB", "DHL_WAREHOUSE"
            ]
        if self.site_columns is None:
            self.site_columns = ["AGI", "DAS", "MIR", "SHU"]

# ----- Utilities ---------------------------------------------------------------
class HeaderNormalizer:
    def __init__(self, column_map: Dict[str, str]):
        self.map = {k.lower(): v for k, v in column_map.items()}

    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        new_cols = []
        for c in df.columns:
            key = str(c).strip().lower()
            new_cols.append(self.map.get(key, str(c).strip().upper().replace(" ", "_")))
        df = df.copy()
        df.columns = new_cols
        return df

class DataQualityValidator:
    """ê°„ë‹¨/ë¹ ë¥¸ ì •í•©ì„± ê²€ì¦(í•„ìš” ì‹œ Great Expectations/Panderaë¡œ í™•ì¥)"""
    HVDC_PATTERN = r"^HVDC-ADOPT-\d{3}-\d{4}$"

    def validate(self, df: pd.DataFrame) -> List[str]:
        issues: List[str] = []
        if "CASE_NO" not in df.columns:
            issues.append("í•„ìˆ˜ í•„ë“œ ëˆ„ë½: CASE_NO")
        else:
            dup = df["CASE_NO"].astype(str).duplicated().sum()
            if dup:
                issues.append(f"CASE_NO ì¤‘ë³µ {dup}ê±´")

        if "HVDC_CODE" in df.columns:
            bad = ~df["HVDC_CODE"].astype(str).str.match(self.HVDC_PATTERN, na=False)
            n_bad = int(bad.sum())
            if n_bad:
                issues.append(f"HVDC_CODE í˜•ì‹ ì˜¤ë¥˜ {n_bad}ê±´")

        # ìˆ˜ì¹˜í˜• ê¸°ë³¸ ì²´í¬
        for num_col in ("AMOUNT", "QTY", "PKG"):
            if num_col in df.columns:
                nonnum = pd.to_numeric(df[num_col], errors="coerce").isna() & df[num_col].notna()
                if int(nonnum.sum()):
                    issues.append(f"{num_col} ë¹„ìˆ«ì ê°’ {int(nonnum.sum())}ê±´")

        # ë‚ ì§œ ë³€í™˜ ê°€ëŠ¥ì„±(ì°½ê³ /í˜„ì¥)
        for col in df.columns:
            if col in set(cfg.warehouse_columns + cfg.site_columns):
                # í—ˆìš©: ê²°ì¸¡/ë¬¸ìì—´ â†’ to_datetime ë³€í™˜ ì‹¤íŒ¨ìœ¨ë§Œ ì§‘ê³„
                s = pd.to_datetime(df[col], errors="coerce")
                fail_mask = (df[col].notna()) & (s.isna())
                fail = int(fail_mask.sum())
                if fail:
                    issues.append(f"{col}: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ {fail}ê±´")

        return issues

# ----- Feature engineering -----------------------------------------------------
class FeatureBuilder:
    def __init__(self, cfg: DetectorConfig):
        self.cfg = cfg

    def build(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[Tuple[str, str, int]]]:
        """
        ë°˜í™˜:
          - í–‰ ë‹¨ìœ„ í”¼ì²˜(ì •ê·œí™”ëœ CASE_NO index)
          - dwell ëª©ë¡[(case_id, location, dwell_days)]
        """
        rows = []
        dwell_list: List[Tuple[str, str, int]] = []

        for _, row in df.iterrows():
            case_id = str(row.get("CASE_NO", "NA"))
            points: List[Tuple[str, pd.Timestamp]] = []
            for col in (self.cfg.warehouse_columns + self.cfg.site_columns):
                if col in row.index and pd.notna(row[col]):
                    dt = pd.to_datetime(row[col], errors="coerce")
                    if pd.notna(dt):
                        points.append((col, dt))

            points.sort(key=lambda x: x[1])
            if len(points) >= 2:
                # dwell(ë‹¤ìŒ ì§€ì ê¹Œì§€ ì²´ë¥˜ì¼)
                for (loc_a, t_a), (loc_b, t_b) in zip(points[:-1], points[1:]):
                    dwell = max(0, (t_b - t_a).days)
                    dwell_list.append((case_id, loc_a, dwell))

            # scalar features
            n_touch = len(points)
            first_ts = points[0][1].value if n_touch else np.nan
            last_ts = points[-1][1].value if n_touch else np.nan
            total_days = np.nan
            if n_touch >= 2:
                total_days = (points[-1][1] - points[0][1]).days

            rows.append(
                dict(
                    CASE_NO=case_id,
                    TOUCH_COUNT=n_touch,
                    TOTAL_DAYS=total_days,
                    FIRST_TS=first_ts,
                    LAST_TS=last_ts,
                    AMOUNT=pd.to_numeric(row.get("AMOUNT", np.nan), errors="coerce"),
                    QTY=pd.to_numeric(row.get("QTY", np.nan), errors="coerce"),
                    PKG=pd.to_numeric(row.get("PKG", np.nan), errors="coerce"),
                )
            )

        feat = pd.DataFrame(rows).set_index("CASE_NO", drop=True)
        return feat, dwell_list

# ----- Statistical detectors ---------------------------------------------------
class StatDetector:
    def __init__(self, iqr_k: float = 1.5, mad_k: float = 3.5):
        self.iqr_k = iqr_k
        self.mad_k = mad_k

    def iqr_outliers(self, dwell_list: List[Tuple[str, str, int]]) -> List[AnomalyRecord]:
        if not dwell_list:
            return []
        vals = np.array([d for _, _, d in dwell_list], dtype=float)
        q1, q3 = np.percentile(vals, 25), np.percentile(vals, 75)
        iqr = q3 - q1
        lo, hi = q1 - self.iqr_k * iqr, q3 + self.iqr_k * iqr

        out = []
        for case_id, loc, d in dwell_list:
            if d > hi:
                sev = AnomalySeverity.HIGH if d > 2 * hi else AnomalySeverity.MEDIUM
                out.append(
                    AnomalyRecord(
                    case_id=case_id,
                    anomaly_type=AnomalyType.EXCESSIVE_DWELL,
                        severity=sev,
                        description=f"{loc}ì—ì„œ {d}ì¼ ì²´ë¥˜ (ì •ìƒâ‰ˆ{lo:.1f}~{hi:.1f}ì¼)",
                        detected_value=float(d),
                        expected_range=(float(lo), float(hi)),
                        location=loc,
                        timestamp=datetime.now(),
                    )
                )
        return out

# ----- Rule-based detectors ----------------------------------------------------
class RuleDetector:
    def __init__(self, cfg: DetectorConfig):
        self.cfg = cfg

    def time_reversal(self, row: pd.Series) -> Optional[AnomalyRecord]:
        pts: List[Tuple[str, pd.Timestamp]] = []
        for col in (self.cfg.warehouse_columns + self.cfg.site_columns):
            if col in row.index and pd.notna(row[col]):
                ts = pd.to_datetime(row[col], errors="coerce")
                if pd.notna(ts):
                    pts.append((col, ts))
        if len(pts) < 2:
            return None
        
        # ì‹œê°„ ì—­ì „ì´ ìˆëŠ”ì§€ í™•ì¸ (ì •ë ¬ ì „í›„ ë¹„êµ)
        pts_sorted = sorted(pts, key=lambda x: x[1])
        original_order = [p[0] for p in pts]
        sorted_order = [p[0] for p in pts_sorted]
        
        # ì›ë˜ ìˆœì„œì™€ ì‹œê°„ìˆœ ì •ë ¬ëœ ìˆœì„œê°€ ë‹¤ë¥´ë©´ ì‹œê°„ ì—­ì „
        if original_order != sorted_order:
            return AnomalyRecord(
                case_id=str(row.get("CASE_NO", "NA")),
                anomaly_type=AnomalyType.TIME_REVERSAL,
                severity=AnomalySeverity.HIGH,
                description="ì‹œê°„ ì—­ì „(ìˆœì„œ ë¶ˆì¼ì¹˜) ë°œìƒ",
                detected_value=None,
                expected_range=None,
                location=None,
                timestamp=datetime.now(),
            )
        return None

    def location_skip(self, row: pd.Series) -> Optional[AnomalyRecord]:
        """ì—…ë¬´ìƒ ë¶ˆê°€ëŠ¥í•œ ìˆœë²ˆ ìŠ¤í‚µ(ê°„ë‹¨ ê·œì¹™ ì˜ˆì‹œ)"""
        # í•„ìš” ì‹œ í”„ë¡œì íŠ¸ ë£° ì¹´í…Œê³ ë¦¬(E)ë¡œ ê°•í™”
        return None

# ----- ML detector (with calibration) -----------------------------------------
class ECDFCalibrator:
    """ì ìˆ˜ ë¶„í¬ ê¸°ë°˜ ìœ„í—˜ë„ ë³´ì •: ë‚®ì„ìˆ˜ë¡ ì •ìƒì¸ decision_function/scoreë¥¼ [0..1] ìœ„í—˜ë„ë¡œ ë³€í™˜"""
    def __init__(self):
        self.ref: Optional[np.ndarray] = None

    def fit(self, raw_scores: np.ndarray) -> "ECDFCalibrator":
        self.ref = np.sort(raw_scores.astype(float))
        return self

    def transform(self, raw_scores: np.ndarray) -> np.ndarray:
        if self.ref is None or len(self.ref) == 0:
            return np.clip((raw_scores - raw_scores.min()) / (raw_scores.ptp() + 1e-9), 0, 1)
        # ì›ì ìˆ˜ê°€ "ì‘ì„ìˆ˜ë¡ ì´ìƒ"ì´ë¼ê³  ê°€ì • â†’ ë¶„ìœ„ìˆ˜ë¡œ ìœ„í—˜ë„ ì‚°ì¶œ
        # ìœ„í—˜ë„ = 1 - ECDF(x)
        idx = np.searchsorted(self.ref, raw_scores, side="right")
        ecdf = idx / float(len(self.ref))
        risk = 1.0 - ecdf
        return np.clip(risk, 0, 1)

class MLDetector:
    def __init__(self, contamination: float = 0.02, random_state: int = 42, use_pyod_first: bool = True):
        self.contamination = contamination
        self.random_state = random_state
        self.use_pyod_first = use_pyod_first and PYOD_AVAILABLE
        self.model = None
        self.scaler = None
        self.calib = ECDFCalibrator()

    def fit_predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """return: (y_pred[0/1], risk[0..1])"""
        if X.empty or (not SKLEARN_AVAILABLE and not PYOD_AVAILABLE):
            return np.zeros(len(X), dtype=int), np.zeros(len(X), dtype=float)

        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        Xs = self.scaler.fit_transform(X.values) if self.scaler else X.values

        if self.use_pyod_first:
            # PyOD IForest
            self.model = PyODIForest(contamination=self.contamination, random_state=self.random_state)
            self.model.fit(Xs)
            # PyODì˜ decision_scores_: ê°’ì´ í´ìˆ˜ë¡ ì´ìƒì¹˜
            raw = np.asarray(self.model.decision_scores_, dtype=float)
            risk = ECDFCalibrator().fit(raw).transform(raw)
            y = (risk >= (1 - self.contamination)).astype(int)
            return y, risk

        # Sklearn IsolationForest (decision_function: í´ìˆ˜ë¡ ì •ìƒ)
        self.model = IsolationForest(
            contamination=self.contamination,
            random_state=self.random_state,
            n_estimators=256,
        )
        self.model.fit(Xs)
        dec = self.model.decision_function(Xs)  # +: ì •ìƒ, -: ì´ìƒ
        # ìœ„í—˜ë„ = 1 - ECDF(dec)
        risk = ECDFCalibrator().fit(dec).transform(dec)
        y = (risk >= (1 - self.contamination)).astype(int)
        return y, risk

# ----- Alert manager -----------------------------------------------------------
class AlertManager:
    def __init__(self, window_sec: int = 30, min_risk: float = 0.8):
        self.window = window_sec
        self.min_risk = min_risk
        self._last: Optional[float] = None

    def on_anomaly(self, risk: float) -> bool:
        """
        ì´ìƒì¹˜ê°€ ì—°ë‹¬ì•„ ë°œìƒí•˜ê³  window ë‚´ í•´ì†Œë˜ì§€ ì•Šìœ¼ë©´ True ë°˜í™˜(ì•Œë¦¼)
        """
        now = time.time()
        if risk < self.min_risk:
            self._last = None
            return False
        if self._last is None:
            self._last = now
            return False
        if (now - self._last) >= self.window:
            self._last = now
            return True
        return False
    
# ----- Orchestrator ------------------------------------------------------------
class HybridAnomalyDetector:
    def __init__(self, cfg: DetectorConfig):
        self.cfg = cfg
        self.normalizer = HeaderNormalizer(cfg.column_map)
        self.validator = DataQualityValidator()
        self.rule = RuleDetector(cfg)
        self.stat = StatDetector(cfg.iqr_k, cfg.mad_k)
        self.ml = MLDetector(cfg.contamination, cfg.random_state, cfg.use_pyod_first)
        self.alert = AlertManager(cfg.alert_window_sec, cfg.min_risk_to_alert)
        self._summary: Dict = {}

    def run(self, df_raw: pd.DataFrame, export_excel: Optional[str] = None, export_json: Optional[str] = None) -> Dict:
        df = self.normalizer.normalize(df_raw)
        issues = self.validator.validate(df)
        anomalies: List[AnomalyRecord] = []
        if issues:
            logger.warning(f"ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ: {issues}")
            anomalies.extend([
                AnomalyRecord(
                    case_id=str(df.iloc[0].get("CASE_NO", "NA")) if len(df) else "NA",
                    anomaly_type=AnomalyType.DATA_QUALITY,
                    severity=AnomalySeverity.MEDIUM,
                    description="; ".join(issues),
                    detected_value=None,
                    expected_range=None,
                    location=None,
                    timestamp=datetime.now(),
                )
            ])

        # Feature build
        fb = FeatureBuilder(self.cfg)
        feat, dwell_list = fb.build(df)

        # 1) Rule
        for _, row in df.iterrows():
            r1 = self.rule.time_reversal(row)
            if r1:
                anomalies.append(r1)

        # 2) Stat
        anomalies.extend(self.stat.iqr_outliers(dwell_list))

        # 3) ML
        use_cols = [c for c in ["TOUCH_COUNT", "TOTAL_DAYS", "AMOUNT", "QTY", "PKG"] if c in feat.columns]
        X = feat[use_cols].fillna(0.0)
        y, risk = self.ml.fit_predict(X)
        if len(X):
            for i, (case_id, yi, ri) in enumerate(zip(X.index, y, risk)):
                if yi == 1:
                    sev = (
                        AnomalySeverity.CRITICAL if ri >= 0.98 else
                        AnomalySeverity.HIGH if ri >= 0.9 else
                        AnomalySeverity.MEDIUM
                    )
                    rec = AnomalyRecord(
                        case_id=str(case_id),
                        anomaly_type=AnomalyType.ML_OUTLIER,
                        severity=sev,
                        description=f"ML ì´ìƒì¹˜(ìœ„í—˜ë„ {ri:.3f})",
                        detected_value=float(ri),
                        expected_range=None,
                        location=None,
                        timestamp=datetime.now(),
                        risk_score=float(ri),
                    )
                    anomalies.append(rec)
                    # 30ì´ˆ ì•Œë¦¼ ìœˆë„ìš°
                    if self.alert.on_anomaly(ri):
                        logger.error("ğŸš¨ 30ì´ˆ ë‚´ ë³µêµ¬ ì—†ìŒ: ì•Œë¦¼ íŠ¸ë¦¬ê±°")

        # Summary
        self._summary = self._build_summary(anomalies)

        # Export
        if export_excel:
            self._export_excel(Path(export_excel), anomalies, feat)
        if export_json:
            self._export_json(Path(export_json), anomalies)
        
        return {
            "summary": self._summary,
            "anomalies": anomalies,
            "features": feat,
        }

    def _build_summary(self, anomalies: List[AnomalyRecord]) -> Dict:
        by_type: Dict[str, int] = {}
        by_sev: Dict[str, int] = {}
        for a in anomalies:
            by_type[a.anomaly_type.value] = by_type.get(a.anomaly_type.value, 0) + 1
            by_sev[a.severity.value] = by_sev.get(a.severity.value, 0) + 1
        return {
            "total": len(anomalies),
            "by_type": by_type,
            "by_severity": by_sev,
        }

    # -------- Exporters --------
    def _export_json(self, path: Path, anomalies: List[AnomalyRecord]) -> None:
        data = [a.to_dict() for a in anomalies]
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        logger.info(f"JSON ì €ì¥: {path}")

    def _export_excel(self, path: Path, anomalies: List[AnomalyRecord], feat: pd.DataFrame) -> None:
        if not OPENPYXL_AVAILABLE:
            logger.warning("openpyxl ë¯¸ì„¤ì¹˜ë¡œ Excel ìƒëµ")
            return
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = "Summary"
        ws["A1"] = "HVDC ì´ìƒì¹˜ íƒì§€ ë¦¬í¬íŠ¸"
        ws["A1"].font = Font(size=16, bold=True)
        ws["A2"] = f"ìƒì„±ì¼ì‹œ: {datetime.now():%Y-%m-%d %H:%M:%S}"

        # Summary block
        s = self._summary
        ws["A4"] = "ì´ ì´ìƒì¹˜"; ws["B4"] = s["total"]
        ws["A6"] = "ìœ í˜•ë³„"
        r = 7
        for k, v in s["by_type"].items():
            ws.cell(r, 1).value = k; ws.cell(r, 2).value = v; r += 1
        r += 1; ws.cell(r, 1).value = "ì‹¬ê°ë„ë³„"; r += 1
        for k, v in s["by_severity"].items():
            ws.cell(r, 1).value = k; ws.cell(r, 2).value = v; r += 1

        # Anomalies
        ws2 = wb.create_sheet("Anomalies")
        cols = list(AnomalyRecord.__annotations__.keys())
        for i, c in enumerate(cols, start=1):
            ws2.cell(1, i).value = c
            ws2.cell(1, i).font = Font(bold=True)
        for ridx, a in enumerate(anomalies, start=2):
            row = a.to_dict()
            for cidx, c in enumerate(cols, start=1):
                ws2.cell(ridx, cidx).value = row.get(
                    c if c != "risk_score" else "Risk_Score"
                )

        # Features
        ws3 = wb.create_sheet("Features")
        for r in dataframe_to_rows(feat.reset_index(), index=False, header=True):
            ws3.append(r)

        wb.save(path)
        logger.info(f"Excel ì €ì¥: {path}")


# -------- convenience runner ---------------------------------------------------
cfg = DetectorConfig()
fb = FeatureBuilder(cfg)

def main():
    import argparse

    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True, help="Excel/CSV íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--sheet", default=None, help="Excel ì‹œíŠ¸ëª…(ì˜µì…˜)")
    ap.add_argument("--excel-out", default=None, help="ê²°ê³¼ Excel ê²½ë¡œ")
    ap.add_argument("--json-out", default=None, help="ê²°ê³¼ JSON ê²½ë¡œ")
    args = ap.parse_args()

    p = Path(args.input)
    if p.suffix.lower() in (".xlsx", ".xlsm", ".xls"):
        df = pd.read_excel(p, sheet_name=args.sheet)
    else:
        df = pd.read_csv(p, encoding="utf-8")

    # DataFrameì¸ì§€ í™•ì¸
    if not isinstance(df, pd.DataFrame):
        logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {type(df)}")
        return

    det = HybridAnomalyDetector(cfg)
    result = det.run(df, export_excel=args.excel_out, export_json=args.json_out)

    s = result["summary"]
    logger.info(f"ì´ ì´ìƒì¹˜: {s['total']}")
    logger.info(f"ìœ í˜•ë³„: {s['by_type']}")
    logger.info(f"ì‹¬ê°ë„ë³„: {s['by_severity']}")

if __name__ == "__main__":
    main()
