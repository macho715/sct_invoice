"""
Masterfile â†’ Warehouse ìë™ ë™ê¸°í™” ì—”ì§„
CASE NO ë§¤ì¹­ ê¸°ë°˜ ë°ì´í„° ì—…ë°ì´íŠ¸ (A~AQì—´ ë²”ìœ„ ì œí•œ)
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import os
import shutil
from pathlib import Path

try:
    from .case_matcher import CaseMatcher
    from ..formatters.header_detector import HeaderDetector
    from ..validators.hvdc_validator import HVDCValidator
    from ..validators.update_tracker import UpdateTracker
    from ..formatters.header_matcher import HeaderMatcher
    from .parallel_processor import ParallelProcessor
    from ..validators.change_tracker import ChangeTracker
    from ..formatters.excel_formatter import ExcelFormatter
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ fallback
    from case_matcher import CaseMatcher
    from formatters.header_detector import HeaderDetector
    from validators.hvdc_validator import HVDCValidator
    from validators.update_tracker import UpdateTracker
    from formatters.header_matcher import HeaderMatcher
    from parallel_processor import ParallelProcessor
    from validators.change_tracker import ChangeTracker
    from formatters.excel_formatter import ExcelFormatter


class DataSynchronizer:
    """ë°ì´í„° ë™ê¸°í™” ì—”ì§„ í´ë˜ìŠ¤"""

    def __init__(
        self,
        column_limit: str = "AG",  # Master íŒŒì¼ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì»¬ëŸ¼
        backup_enabled: bool = True,
        validation_enabled: bool = True,
        prioritize_dates: bool = True,
        max_workers: int = None,
    ):
        """
        ì´ˆê¸°í™”

        Args:
            column_limit: ì—…ë°ì´íŠ¸í•  ìµœëŒ€ ì»¬ëŸ¼ (ê¸°ë³¸ê°’: 'AQ')
            backup_enabled: ë°±ì—… ìƒì„± ì—¬ë¶€
            validation_enabled: ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì—¬ë¶€
            prioritize_dates: ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ìš°ì„ ìˆœìœ„ ì„¤ì •
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì›Œì»¤ ìˆ˜
        """
        self.column_limit = column_limit
        self.backup_enabled = backup_enabled
        self.validation_enabled = validation_enabled
        self.prioritize_dates = prioritize_dates

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.case_matcher = CaseMatcher(max_workers=max_workers)
        self.header_detector = HeaderDetector()
        self.hvdc_validator = HVDCValidator()
        self.update_tracker = UpdateTracker()
        self.header_matcher = HeaderMatcher()
        self.parallel_processor = ParallelProcessor(max_workers)
        self.change_tracker = ChangeTracker()

        # ì»¬ëŸ¼ ì œí•œ ì¸ë±ìŠ¤ ê³„ì‚° (AQ = 43ë²ˆì§¸ ì»¬ëŸ¼, 0-based index = 42)
        self.max_column_index = self._column_letter_to_index(column_limit)

        # ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        # ìµœìš°ì„ : ì°½ê³ ë³„ ë‚ ì§œ ì»¬ëŸ¼ë“¤
        self.high_priority_warehouse_keywords = [
            "dhl warehouse",
            "dsv indoor",
            "dsv al markaz",
            "dsv outdoor",
            "hauler indoor",
            "dsv mzp",
            "mosb",
            "shifting",
            "mir",
            "shu",
            "das",
            "agi",
        ]

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ë“¤
        self.medium_priority_date_keywords = [
            "etd/atd",
            "eta/ata",
            "status_location_date",
            "etd",
            "eta",
            "atd",
            "ata",
            "arrival",
            "departure",
            "delivery",
            "shipment",
            "date",
        ]

        # í†µí•© í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.warehouse_date_keywords = (
            self.high_priority_warehouse_keywords
            + self.medium_priority_date_keywords
            + ["warehouse", "site", "location", "actual", "estimated", "time"]
        )

        # ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜
        self.date_column_patterns = [
            r".*date.*",
            r".*eta.*",
            r".*etd.*",
            r".*ata.*",
            r".*atd.*",
            r".*arrival.*",
            r".*departure.*",
            r".*delivery.*",
            r".*shipment.*",
            r".*warehouse.*",
            r".*site.*",
            r".*location.*",
        ]

        # ë™ê¸°í™” ì´ë ¥
        self.sync_history = []

    def _column_letter_to_index(self, column_letter: str) -> int:
        """
        ì—‘ì…€ ì»¬ëŸ¼ ë¬¸ìë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (A=0, B=1, ..., AQ=42)

        Args:
            column_letter: ì»¬ëŸ¼ ë¬¸ì (ì˜ˆ: 'AQ')

        Returns:
            0-based ì»¬ëŸ¼ ì¸ë±ìŠ¤
        """
        result = 0
        for char in column_letter.upper():
            result = result * 26 + (ord(char) - ord("A") + 1)
        return result - 1

    def _index_to_column_letter(self, index: int) -> str:
        """
        ì¸ë±ìŠ¤ë¥¼ ì—‘ì…€ ì»¬ëŸ¼ ë¬¸ìë¡œ ë³€í™˜

        Args:
            index: 0-based ì»¬ëŸ¼ ì¸ë±ìŠ¤

        Returns:
            ì»¬ëŸ¼ ë¬¸ì (ì˜ˆ: 'AQ')
        """
        result = ""
        index += 1  # 1-basedë¡œ ë³€ê²½
        while index > 0:
            index -= 1
            result = chr(index % 26 + ord("A")) + result
            index //= 26
        return result

    def _identify_date_columns(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        DataFrameì—ì„œ ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ë“¤ì„ ì‹ë³„í•˜ê³  ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¥˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)

        Args:
            df: ëŒ€ìƒ DataFrame

        Returns:
            ìš°ì„ ìˆœìœ„ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë”•ì…”ë„ˆë¦¬
        """
        date_columns = {
            "high_priority": [],  # ì°½ê³ ë³„/í˜„ì¥ë³„ ì¤‘ìš” ë‚ ì§œ (ìµœìš°ì„ )
            "medium_priority": [],  # ì¼ë°˜ ë‚ ì§œ í•„ë“œ
            "low_priority": [],  # ê¸°íƒ€ ì‹œê°„ ê´€ë ¨ í•„ë“œ
        }

        for col in df.columns:
            col_lower = str(col).lower().strip()

            # 1. ìµœìš°ì„ : ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ì»¬ëŸ¼ë“¤ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            if any(
                keyword == col_lower or keyword in col_lower
                for keyword in self.high_priority_warehouse_keywords
            ):
                date_columns["high_priority"].append(col)
                print(f"[ìµœìš°ì„ ] ì°½ê³ ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue

            # 2. ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ë“¤
            if any(
                keyword == col_lower or keyword in col_lower
                for keyword in self.medium_priority_date_keywords
            ):
                date_columns["medium_priority"].append(col)
                print(f"[ì¤‘ê°„] ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue

            # 3. ì¼ë°˜ ë‚ ì§œ íŒ¨í„´ ë§¤ì¹­ (ì¶”ê°€ íƒì§€)
            if any(
                re.match(pattern, col_lower, re.IGNORECASE)
                for pattern in self.date_column_patterns
            ):
                if (
                    col not in date_columns["high_priority"]
                    and col not in date_columns["medium_priority"]
                ):
                    date_columns["medium_priority"].append(col)
                    print(f"[íŒ¨í„´ë§¤ì¹­] ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                    continue

            # 4. ìƒíƒœ ê´€ë ¨ ë‚ ì§œ í•„ë“œ (Status_Location_Date ë“±)
            if "status" in col_lower and any(
                date_word in col_lower
                for date_word in ["date", "time", "year", "month"]
            ):
                date_columns["medium_priority"].append(col)
                print(f"[ìƒíƒœ] ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue

            # 5. ê¸°íƒ€ ì‹œê°„ ê´€ë ¨ í•„ë“œ
            if any(
                time_keyword in col_lower
                for time_keyword in ["time", "schedule", "plan", "handling"]
            ):
                if (
                    col not in date_columns["high_priority"]
                    and col not in date_columns["medium_priority"]
                ):
                    date_columns["low_priority"].append(col)

        # ë°œê²¬ëœ ë‚ ì§œ ì»¬ëŸ¼ ìš”ì•½ ì¶œë ¥
        print(f"\\n=== ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼ ===")
        print(
            f"ìµœìš°ì„  ({len(date_columns['high_priority'])}ê°œ): {date_columns['high_priority']}"
        )
        print(
            f"ì¤‘ê°„ ({len(date_columns['medium_priority'])}ê°œ): {date_columns['medium_priority']}"
        )
        print(
            f"ë‚®ìŒ ({len(date_columns['low_priority'])}ê°œ): {date_columns['low_priority']}"
        )

        return date_columns

    def _validate_date_value(
        self, value: Any, column_name: str
    ) -> Dict[str, Any]:  # returns (summary, df) [patched]
        """
        ë‚ ì§œ ê°’ì˜ ìœ íš¨ì„± ê²€ì¦

        Args:
            value: ê²€ì¦í•  ë‚ ì§œ ê°’
            column_name: ì»¬ëŸ¼ëª…

        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        validation_result = {
            "valid": True,
            "error": None,
            "warnings": [],
            "parsed_date": None,
            "original_value": value,
        }

        if pd.isna(value) or value is None or str(value).strip() == "":
            return validation_result  # ë¹ˆ ê°’ì€ ìœ íš¨í•¨

        value_str = str(value).strip()

        # ë‚ ì§œ íŒŒì‹± ì‹œë„
        date_formats = [
            "%Y-%m-%d",
            "%d/%m/%Y",
            "%m/%d/%Y",
            "%Y/%m/%d",
            "%Y-%m-%d %H:%M:%S",
            "%d/%m/%Y %H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%d/%m/%Y %H:%M",
            "%Y%m%d",
            "%d-%m-%Y",
            "%m-%d-%Y",
        ]

        parsed_date = None
        for date_format in date_formats:
            try:
                parsed_date = pd.to_datetime(value_str, format=date_format)
                validation_result["parsed_date"] = parsed_date
                break
            except (ValueError, TypeError):
                continue

        # pandasì˜ ì¼ë°˜ì ì¸ ë‚ ì§œ íŒŒì‹± ì‹œë„
        if parsed_date is None:
            try:
                parsed_date = pd.to_datetime(value_str, infer_datetime_format=True)
                validation_result["parsed_date"] = parsed_date
            except (ValueError, TypeError):
                validation_result["valid"] = False
                validation_result["error"] = (
                    f"ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŒ: '{value_str}'"
                )
                return validation_result

        # ë‚ ì§œ ë²”ìœ„ ê²€ì¦
        if parsed_date is not None:
            current_year = datetime.now().year

            # ê³¼ê±° ë„ˆë¬´ ë¨¼ ë‚ ì§œ (1900ë…„ ì´ì „) ë˜ëŠ” ë¯¸ë˜ ë„ˆë¬´ ë¨¼ ë‚ ì§œ (10ë…„ í›„) ê²½ê³ 
            if parsed_date.year < 1900:
                validation_result["warnings"].append(
                    f"ë§¤ìš° ì˜¤ë˜ëœ ë‚ ì§œ: {parsed_date.year}ë…„"
                )
            elif parsed_date.year > current_year + 10:
                validation_result["warnings"].append(
                    f"ë„ˆë¬´ ë¨¼ ë¯¸ë˜ ë‚ ì§œ: {parsed_date.year}ë…„"
                )

            # ETA/ETD íŠ¹ë³„ ê²€ì¦
            if any(keyword in column_name.lower() for keyword in ["eta", "etd"]):
                if parsed_date < pd.Timestamp.now() - pd.Timedelta(days=365):
                    validation_result["warnings"].append(
                        "ETA/ETDê°€ 1ë…„ ì´ìƒ ê³¼ê±° ë‚ ì§œì„"
                    )

        return validation_result

    def _prioritize_column_mapping(
        self, column_mapping: Dict[str, str], warehouse_df: pd.DataFrame
    ) -> Dict[str, str]:
        """
        ì»¬ëŸ¼ ë§¤í•‘ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬ (ë‚ ì§œ ì»¬ëŸ¼ ìš°ì„ )

        Args:
            column_mapping: ì›ë³¸ ì»¬ëŸ¼ ë§¤í•‘
            warehouse_df: Warehouse DataFrame

        Returns:
            ìš°ì„ ìˆœìœ„ê°€ ì ìš©ëœ ì»¬ëŸ¼ ë§¤í•‘
        """
        if not self.prioritize_dates:
            return column_mapping

        # ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜
        date_columns = self._identify_date_columns(warehouse_df)

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì»¬ëŸ¼ ê·¸ë£¹í™”
        prioritized_mapping = {}

        # 1. ìµœìš°ì„ : ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ
        for master_col, warehouse_col in column_mapping.items():
            if warehouse_col in date_columns["high_priority"]:
                prioritized_mapping[master_col] = warehouse_col

        # 2. ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ
        for master_col, warehouse_col in column_mapping.items():
            if (
                warehouse_col in date_columns["medium_priority"]
                and master_col not in prioritized_mapping
            ):
                prioritized_mapping[master_col] = warehouse_col

        # 3. ë‚®ì€ ìš°ì„ ìˆœìœ„: ê¸°íƒ€ ì‹œê°„ ê´€ë ¨
        for master_col, warehouse_col in column_mapping.items():
            if (
                warehouse_col in date_columns["low_priority"]
                and master_col not in prioritized_mapping
            ):
                prioritized_mapping[master_col] = warehouse_col

        # 4. ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        for master_col, warehouse_col in column_mapping.items():
            if master_col not in prioritized_mapping:
                prioritized_mapping[master_col] = warehouse_col

        return prioritized_mapping

    def _get_date_priority(
        self, column_name: str, date_columns: Dict[str, List[str]]
    ) -> str:
        """
        ì»¬ëŸ¼ì˜ ë‚ ì§œ ìš°ì„ ìˆœìœ„ í™•ì¸

        Args:
            column_name: í™•ì¸í•  ì»¬ëŸ¼ëª…
            date_columns: ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼

        Returns:
            ìš°ì„ ìˆœìœ„ ('high_priority', 'medium_priority', 'low_priority', 'non_date')
        """
        if column_name in date_columns["high_priority"]:
            return "high_priority"
        elif column_name in date_columns["medium_priority"]:
            return "medium_priority"
        elif column_name in date_columns["low_priority"]:
            return "low_priority"
        else:
            return "non_date"

    def create_backup(self, warehouse_file_path: str) -> str:
        """
        Warehouse íŒŒì¼ì˜ ë°±ì—… ìƒì„±

        Args:
            warehouse_file_path: ë°±ì—…í•  íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        if not self.backup_enabled:
            return None

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = Path(warehouse_file_path)
        backup_dir = file_path.parent / "backups"
        backup_dir.mkdir(exist_ok=True)

        # ë°±ì—… í´ë” ì“°ê¸° ê¶Œí•œ í™•ì¸
        if not os.access(backup_dir, os.W_OK):
            raise PermissionError(f"ë°±ì—… í´ë”ì— ì“°ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {backup_dir}")

        backup_filename = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
        backup_path = backup_dir / backup_filename

        shutil.copy2(warehouse_file_path, backup_path)

        return str(backup_path)

    def load_and_analyze_files(
        self, masterfile_path: str, warehouse_path: str
    ) -> Dict[str, Any]:
        """
        Masterfileê³¼ Warehouse íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„ì„

        Args:
            masterfile_path: Masterfile ê²½ë¡œ
            warehouse_path: Warehouse íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        analysis_result = {
            "masterfile": {},
            "warehouse": {},
            "compatibility": {},
            "sync_feasibility": True,
            "issues": [],
        }

        try:
            # Masterfile ë¶„ì„
            master_sheets = pd.ExcelFile(masterfile_path).sheet_names
            master_df = None
            master_sheet = None

            # CASE Listê°€ í¬í•¨ëœ ì‹œíŠ¸ ì°¾ê¸°
            for sheet in master_sheets:
                if "case" in sheet.lower() and "list" in sheet.lower():
                    master_sheet = sheet
                    break

            if not master_sheet and master_sheets:
                master_sheet = master_sheets[0]  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©

            if master_sheet:
                # í—¤ë” íƒì§€í•˜ì—¬ ë¡œë“œ
                df_preview = pd.read_excel(
                    masterfile_path, sheet_name=master_sheet, nrows=20, header=None
                )
                header_row = self.header_detector.detect_header_row(df_preview)

                if header_row is not None:
                    master_df = pd.read_excel(
                        masterfile_path, sheet_name=master_sheet, header=header_row
                    )
                else:
                    master_df = pd.read_excel(masterfile_path, sheet_name=master_sheet)

                analysis_result["masterfile"] = {
                    "sheet_name": master_sheet,
                    "shape": master_df.shape,
                    "columns": list(master_df.columns),
                    "header_row": header_row,
                    "case_column": self._find_case_column(master_df),
                }

            # Warehouse íŒŒì¼ ë¶„ì„
            warehouse_sheets = pd.ExcelFile(warehouse_path).sheet_names
            warehouse_df = None
            warehouse_sheet = None

            # Case List ì‹œíŠ¸ ì°¾ê¸°
            for sheet in warehouse_sheets:
                if "case" in sheet.lower():
                    warehouse_sheet = sheet
                    break

            if not warehouse_sheet and warehouse_sheets:
                warehouse_sheet = warehouse_sheets[0]

            if warehouse_sheet:
                # í—¤ë” íƒì§€í•˜ì—¬ ë¡œë“œ
                df_preview = pd.read_excel(
                    warehouse_path, sheet_name=warehouse_sheet, nrows=20, header=None
                )
                header_row = self.header_detector.detect_header_row(df_preview)

                if header_row is not None:
                    warehouse_df = pd.read_excel(
                        warehouse_path, sheet_name=warehouse_sheet, header=header_row
                    )
                else:
                    warehouse_df = pd.read_excel(
                        warehouse_path, sheet_name=warehouse_sheet
                    )

                analysis_result["warehouse"] = {
                    "sheet_name": warehouse_sheet,
                    "shape": warehouse_df.shape,
                    "columns": list(warehouse_df.columns),
                    "header_row": header_row,
                    "case_column": self._find_case_column(warehouse_df),
                    "column_limit_index": min(
                        self.max_column_index, warehouse_df.shape[1] - 1
                    ),
                    "updatable_columns": list(
                        warehouse_df.columns[: self.max_column_index + 1]
                    ),
                }

            # í˜¸í™˜ì„± ê²€ì‚¬
            if master_df is not None and warehouse_df is not None:
                analysis_result["compatibility"] = self._check_compatibility(
                    master_df,
                    warehouse_df,
                    analysis_result["masterfile"]["case_column"],
                    analysis_result["warehouse"]["case_column"],
                )

                # ì €ì¥ (ë™ê¸°í™”ì—ì„œ ì‚¬ìš©)
                analysis_result["masterfile"]["dataframe"] = master_df
                analysis_result["warehouse"]["dataframe"] = warehouse_df

        except Exception as e:
            analysis_result["sync_feasibility"] = False
            analysis_result["issues"].append(f"íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        return analysis_result

    def _find_case_column(self, df: pd.DataFrame) -> Optional[str]:
        """
        DataFrameì—ì„œ CASE NO ì»¬ëŸ¼ ì°¾ê¸°

        Args:
            df: ëŒ€ìƒ DataFrame

        Returns:
            CASE NO ì»¬ëŸ¼ëª… ë˜ëŠ” None
        """
        case_keywords = ["case no", "case_no", "caseno", "case", "case number"]

        for col in df.columns:
            col_lower = str(col).lower().strip()
            for keyword in case_keywords:
                if keyword in col_lower:
                    return col

        return None

    def _check_compatibility(
        self,
        master_df: pd.DataFrame,
        warehouse_df: pd.DataFrame,
        master_case_col: str,
        warehouse_case_col: str,
    ) -> Dict[str, Any]:
        """
        ë‘ íŒŒì¼ ê°„ì˜ í˜¸í™˜ì„± ê²€ì‚¬

        Args:
            master_df: Masterfile DataFrame
            warehouse_df: Warehouse DataFrame
            master_case_col: Masterfile CASE ì»¬ëŸ¼ëª…
            warehouse_case_col: Warehouse CASE ì»¬ëŸ¼ëª…

        Returns:
            í˜¸í™˜ì„± ê²€ì‚¬ ê²°ê³¼
        """
        compatibility = {
            "case_columns_found": bool(master_case_col and warehouse_case_col),
            "common_columns": [],
            "master_only_columns": [],
            "warehouse_only_columns": [],
            "column_mapping": {},
            "issues": [],
        }

        if not compatibility["case_columns_found"]:
            compatibility["issues"].append("CASE NO ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return compatibility

        # ì»¬ëŸ¼ ë¹„êµ (ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ)
        master_columns = set(master_df.columns)
        warehouse_columns = set(warehouse_df.columns[: self.max_column_index + 1])

        compatibility["common_columns"] = list(master_columns & warehouse_columns)
        compatibility["master_only_columns"] = list(master_columns - warehouse_columns)
        compatibility["warehouse_only_columns"] = list(
            warehouse_columns - master_columns
        )

        # ì»¬ëŸ¼ ë§¤í•‘ ìƒì„± (í‘œì¤€í™”ëœ ì´ë¦„ ê¸°ë°˜)
        for master_col in master_df.columns:
            master_norm = self.header_detector.standardize_column_name(master_col)

            # Warehouseì—ì„œ ë™ì¼í•œ í‘œì¤€í™”ëœ ì´ë¦„ì„ ê°€ì§„ ì»¬ëŸ¼ ì°¾ê¸°
            for warehouse_col in warehouse_df.columns[: self.max_column_index + 1]:
                warehouse_norm = self.header_detector.standardize_column_name(
                    warehouse_col
                )

                if master_norm == warehouse_norm:
                    compatibility["column_mapping"][master_col] = warehouse_col
                    break

        return compatibility

    def synchronize_data(
        self, masterfile_path: str, warehouse_path: str, dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰

        Args:
            masterfile_path: Masterfile ê²½ë¡œ
            warehouse_path: Warehouse íŒŒì¼ ê²½ë¡œ
            dry_run: ì‹¤ì œ ì €ì¥ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰

        Returns:
            ë™ê¸°í™” ê²°ê³¼
        """
        sync_result = {
            "success": False,
            "timestamp": datetime.now().isoformat(),
            "backup_path": None,
            "analysis": {},
            "matching_results": {},
            "update_summary": {},
            "issues": [],
            "dry_run": dry_run,
        }

        try:
            # 1. íŒŒì¼ ë¶„ì„
            analysis = self.load_and_analyze_files(masterfile_path, warehouse_path)
            sync_result["analysis"] = analysis

            if not analysis["sync_feasibility"]:
                sync_result["issues"].extend(analysis["issues"])
                return sync_result

            # 2. ë°±ì—… ìƒì„±
            if not dry_run:
                backup_path = self.create_backup(warehouse_path)
                sync_result["backup_path"] = backup_path

            # 3. UpdateTracker ì´ˆê¸°í™” ë° Before ìƒíƒœ ìº¡ì²˜
            warehouse_df = analysis["warehouse"]["dataframe"]
            self.update_tracker.capture_before_state(
                warehouse_df, analysis["warehouse"]["sheet_name"]
            )

            # 3. CASE NO ë§¤ì¹­
            master_df = analysis["masterfile"]["dataframe"]
            warehouse_df = analysis["warehouse"]["dataframe"]

            master_case_col = analysis["masterfile"]["case_column"]
            warehouse_case_col = analysis["warehouse"]["case_column"]

            if not (master_case_col and warehouse_case_col):
                sync_result["issues"].append("CASE NO ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë™ê¸°í™” ë¶ˆê°€")
                return sync_result

            matching_results = self.case_matcher.find_matching_cases(
                master_df, warehouse_df
            )
            sync_result["matching_results"] = (
                self.case_matcher.generate_matching_report(matching_results)
            )

            # 4. ì»¬ëŸ¼ ë§¤í•‘ ìš°ì„ ìˆœìœ„ ì ìš© (ë‚ ì§œ ì»¬ëŸ¼ ìš°ì„ )
            prioritized_mapping = self._prioritize_column_mapping(
                analysis["compatibility"]["column_mapping"], warehouse_df
            )

            # 5. ë°ì´í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰ (CASE NO ë§¤ì¹­ â†’ ë‚ ì§œ ìš°ì„  ì—…ë°ì´íŠ¸)
            update_summary, warehouse_df = self._perform_updates(
                master_df, warehouse_df, matching_results, prioritized_mapping, dry_run
            )
            sync_result["update_summary"] = update_summary

            # 6. UpdateTracker After ìƒíƒœ ìº¡ì²˜ ë° ì¶”ì  ì¢…ë£Œ
            self.update_tracker.capture_after_state(
                warehouse_df, analysis["warehouse"]["sheet_name"]
            )
            self.update_tracker.end_update_tracking()

            # 7. íŒŒì¼ ì €ì¥ (dry_runì´ ì•„ë‹Œ ê²½ìš°)
            if not dry_run and update_summary["total_changes"] > 0:
                # ì»¬ëŸ¼ ë²”ìœ„ ì œí•œ ì ìš©í•˜ì—¬ ì €ì¥
                updated_warehouse = warehouse_df.iloc[:, : self.max_column_index + 1]

                with pd.ExcelWriter(warehouse_path, engine="openpyxl") as writer:
                    updated_warehouse.to_excel(
                        writer,
                        sheet_name=analysis["warehouse"]["sheet_name"],
                        index=False,
                    )

                sync_result["success"] = True
                print(f"\nâœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {warehouse_path}")

                # 8. ìƒ‰ìƒ ì ìš© (ExcelFormatter)
                print(f"ğŸ¨ ë³€ê²½ì‚¬í•­ ìƒ‰ìƒ í‘œì‹œ ì ìš© ì¤‘...")
                try:
                    formatter = ExcelFormatter(self.change_tracker)
                    success = formatter.apply_formatting_inplace(
                        excel_file_path=warehouse_path,
                        sheet_name=analysis["warehouse"]["sheet_name"],
                    )
                    if success:
                        print(f"âœ… ìƒ‰ìƒ í‘œì‹œ ì™„ë£Œ")
                    else:
                        print(f"âš ï¸ ìƒ‰ìƒ í‘œì‹œ ì‹¤íŒ¨ (ë°ì´í„°ëŠ” ì •ìƒ ì—…ë°ì´íŠ¸ë¨)")
                except Exception as e:
                    print(f"âš ï¸ ìƒ‰ìƒ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)} (ë°ì´í„°ëŠ” ì •ìƒ ì—…ë°ì´íŠ¸ë¨)")

            elif dry_run:
                sync_result["success"] = True  # ì‹œë®¬ë ˆì´ì…˜ ì„±ê³µ
                print(f"\nğŸ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì™„ë£Œ (ì‹¤ì œ íŒŒì¼ ë³€ê²½ ì—†ìŒ)")

            # 9. ìƒì„¸ ì¶”ì  ë¦¬í¬íŠ¸ ìƒì„±
            if update_summary["total_changes"] > 0:
                print(f"\nğŸ“Š ìƒì„¸ ì¶”ì  ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

                # íˆíŠ¸ë§µ ìƒì„±
                heatmap_path = self.update_tracker.create_change_heatmap()
                sync_result["heatmap_path"] = heatmap_path

                # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
                detailed_report_path = self.update_tracker.generate_detailed_report()
                sync_result["detailed_report_path"] = detailed_report_path

                # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
                comparison_report = (
                    self.update_tracker.generate_change_comparison_report()
                )
                sync_result["comparison_report"] = comparison_report

            # 6. ë™ê¸°í™” ì´ë ¥ ì €ì¥
            self.sync_history.append(
                {
                    "timestamp": sync_result["timestamp"],
                    "masterfile": masterfile_path,
                    "warehouse": warehouse_path,
                    "dry_run": dry_run,
                    "changes": update_summary["total_changes"],
                    "new_cases": len(matching_results.get("new_cases", [])),
                    "updated_cases": len(matching_results.get("exact_matches", {}))
                    + len(matching_results.get("fuzzy_matches", {})),
                }
            )

        except Exception as e:
            sync_result["issues"].append(f"ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")

        return sync_result

    def _perform_updates(
        self,
        master_df: pd.DataFrame,
        warehouse_df: pd.DataFrame,
        matching_results: Dict,
        column_mapping: Dict[str, str],
        dry_run: bool,
    ) -> Tuple[Dict[str, Any], pd.DataFrame]:
        """
        ì‹¤ì œ ë°ì´í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰ (ë³‘ë ¬ ì²˜ë¦¬)

        Args:
            master_df: Masterfile DataFrame
            warehouse_df: Warehouse DataFrame
            matching_results: ë§¤ì¹­ ê²°ê³¼
            column_mapping: ì»¬ëŸ¼ ë§¤í•‘ (ì´ë¯¸ ìš°ì„ ìˆœìœ„ ì ìš©ë¨)
            dry_run: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ

        Returns:
            (ì—…ë°ì´íŠ¸ ìš”ì•½, ì—…ë°ì´íŠ¸ëœ warehouse_df)
        """
        update_summary = {
            "updated_records": 0,
            "new_records": 0,
            "skipped_records": 0,
            "total_changes": 0,
            "updated_fields": {},
            "validation_errors": [],
            "changes_detail": [],
            "date_updates": {
                "high_priority_dates": 0,  # ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ
                "medium_priority_dates": 0,  # ì¼ë°˜ ë‚ ì§œ
                "low_priority_dates": 0,  # ê¸°íƒ€ ì‹œê°„ ê´€ë ¨
                "non_date_fields": 0,  # ë‚ ì§œê°€ ì•„ë‹Œ í•„ë“œ
            },
        }

        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ì‹ë³„ (ë™ì  í—¤ë” ë§¤ì¹­)
        date_columns = self.header_matcher.get_all_date_columns(warehouse_df.columns)

        # 1. ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ (ë³‘ë ¬)
        all_matches = {
            **matching_results.get("exact_matches", {}),
            **matching_results.get("fuzzy_matches", {}),
        }

        if all_matches:
            update_summary, warehouse_df = self._update_existing_records_parallel(
                master_df,
                warehouse_df,
                all_matches,
                date_columns,
                dry_run,
                update_summary,
            )

        # 2. ì‹ ê·œ ë ˆì½”ë“œ ì¶”ê°€
        new_cases = matching_results.get("new_cases", [])
        print(f"[DEBUG] ì‹ ê·œ ì¼€ì´ìŠ¤ ê°ì§€: {len(new_cases)}ê°œ")
        if new_cases:
            print(
                f"[DEBUG] ì‹ ê·œ ì¼€ì´ìŠ¤ ìƒì„¸: {[case.get('case_no', 'N/A') for case in new_cases[:5]]}..."
            )
            update_summary, warehouse_df = self._add_new_records_parallel(
                master_df, warehouse_df, new_cases, dry_run, update_summary
            )
        else:
            print("[DEBUG] ì‹ ê·œ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return update_summary, warehouse_df

    def _update_existing_records_parallel(
        self,
        master_df,
        warehouse_df,
        all_matches,
        date_columns,
        dry_run,
        update_summary,
    ):
        """ê¸°ì¡´ ë ˆì½”ë“œ ë³‘ë ¬ ì—…ë°ì´íŠ¸"""

        def update_batch(match_items):
            """ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
            batch_changes = []
            batch_errors = []

            for master_idx, match_info in match_items:
                warehouse_idx = match_info["target_index"]
                master_row = master_df.iloc[master_idx]

                for col_name in warehouse_df.columns:
                    if col_name in master_row.index:
                        master_value = master_row[col_name]
                        warehouse_value = warehouse_df.at[warehouse_idx, col_name]

                        is_date = self.header_matcher.is_date_column(col_name)

                        # Master ê°’ì´ ìˆê³ , ê°’ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                        if pd.notna(master_value) and not self._values_equal_safe(
                            master_value, warehouse_value
                        ):
                            # ë‚ ì§œ ì—…ë°ì´íŠ¸ ë””ë²„ê·¸ ë¡œê·¸
                            if is_date:
                                print(
                                    f"[DEBUG] ë‚ ì§œ ì—…ë°ì´íŠ¸: {col_name} - {warehouse_value} -> {master_value}"
                                )

                            # ë³€ê²½ ê¸°ë¡
                            batch_changes.append(
                                {
                                    "wh_idx": warehouse_idx,
                                    "col_name": col_name,
                                    "new_value": master_value,
                                    "old_value": warehouse_value,
                                    "is_date": is_date,
                                    "master_idx": master_idx,
                                    "case_no": match_info.get("target_case", ""),
                                }
                            )

            return batch_changes

        # ë§¤ì¹˜ëœ ì¼€ì´ìŠ¤ë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
        match_items = list(all_matches.items())
        batch_size = max(
            100, len(match_items) // (self.parallel_processor.max_workers * 2)
        )

        # ë³‘ë ¬ ì—…ë°ì´íŠ¸
        change_batches = self.parallel_processor.process_batches(
            match_items, batch_size, update_batch, use_threads=True
        )

        # ë³€ê²½ì‚¬í•­ ì ìš©
        for batch in change_batches:
            for change in batch:
                if not dry_run:
                    warehouse_df.at[change["wh_idx"], change["col_name"]] = change[
                        "new_value"
                    ]

                # ë³€ê²½ì‚¬í•­ ì¶”ì 
                self.change_tracker.add_change(
                    case_no=change["case_no"],
                    column_name=change["col_name"],
                    old_value=change["old_value"],
                    new_value=change["new_value"],
                    change_type="date_update" if change["is_date"] else "field_update",
                    priority="master_priority",
                    row_index=change["wh_idx"],
                )

                # í†µê³„ ì—…ë°ì´íŠ¸
                update_summary["total_changes"] += 1
                if change["is_date"]:
                    update_summary["date_updates"]["high_priority_dates"] += 1
                else:
                    update_summary["date_updates"]["non_date_fields"] += 1

        update_summary["updated_records"] = len(all_matches)
        return update_summary, warehouse_df

    def _add_new_records_parallel(
        self, master_df, warehouse_df, new_cases, dry_run, update_summary
    ):
        """ì‹ ê·œ ë ˆì½”ë“œ ë³‘ë ¬ ì¶”ê°€"""

        def process_new_cases_batch(cases_batch):
            """ì‹ ê·œ ì¼€ì´ìŠ¤ ë°°ì¹˜ ì²˜ë¦¬"""
            new_rows = []

            for case_info in cases_batch:
                master_idx = case_info["source_index"]
                master_row = master_df.iloc[master_idx]

                # ìƒˆ í–‰ ìƒì„± (warehouse_dfì™€ ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì¡°)
                new_row = {}
                for col in warehouse_df.columns:
                    if col in master_row.index:
                        new_row[col] = master_row[col]
                    else:
                        new_row[col] = None

                new_rows.append(new_row)

                # ì‹ ê·œ ì¼€ì´ìŠ¤ ë¡œê¹…
                self.change_tracker.log_new_case(case_no=case_info["case_no"])

            return new_rows

        # ì‹ ê·œ ì¼€ì´ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
        batch_size = max(
            50, len(new_cases) // (self.parallel_processor.max_workers * 2)
        )

        new_rows_batches = self.parallel_processor.process_batches(
            new_cases, batch_size, process_new_cases_batch, use_threads=True
        )

        # ì‹ ê·œ í–‰ë“¤ì„ DataFrameì— ì¶”ê°€ (ëì— ì¼ê´„ ì¶”ê°€)
        if not dry_run and new_rows_batches:
            # ëª¨ë“  ì‹ ê·œ í–‰ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
            all_new_rows = []
            for batch in new_rows_batches:
                all_new_rows.extend(batch)

            if all_new_rows:
                new_rows_df = pd.DataFrame(all_new_rows)
                # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ê³  ëì— ì¶”ê°€
                warehouse_df = pd.concat(
                    [warehouse_df, new_rows_df], ignore_index=True, sort=False
                )
                print(f"[DEBUG] ì‹ ê·œ {len(all_new_rows)}ê°œ í–‰ì„ íŒŒì¼ ëì— ì¶”ê°€ ì™„ë£Œ")

        update_summary["new_records"] = len(new_cases)
        return update_summary, warehouse_df

    def _values_equal_safe(self, val1, val2) -> bool:
        """
        ì•ˆì „í•œ ê°’ ë¹„êµ (pandas ë°°ì—´ ì˜¤ë¥˜ ë°©ì§€)

        Args:
            val1: ì²« ë²ˆì§¸ ê°’
            val2: ë‘ ë²ˆì§¸ ê°’

        Returns:
            ë™ì¼ ì—¬ë¶€
        """
        try:
            # pandas Seriesë‚˜ numpy arrayì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ë§Œ ì¶”ì¶œ
            if hasattr(val1, "__len__") and not isinstance(val1, str):
                val1 = val1.iloc[0] if hasattr(val1, "iloc") else val1[0]
            if hasattr(val2, "__len__") and not isinstance(val2, str):
                val2 = val2.iloc[0] if hasattr(val2, "iloc") else val2[0]

            # NaN ì²´í¬
            val1_is_na = (
                pd.isna(val1)
                if pd.__version__ >= "1.0.0"
                else (val1 is None or str(val1).strip() == "")
            )
            val2_is_na = (
                pd.isna(val2)
                if pd.__version__ >= "1.0.0"
                else (val2 is None or str(val2).strip() == "")
            )

            if val1_is_na and val2_is_na:
                return True
            if val1_is_na or val2_is_na:
                return False

            # ë¬¸ìì—´ ë¹„êµ
            return str(val1).strip() == str(val2).strip()

        except Exception:
            # ì˜ˆì™¸ ë°œìƒì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            return str(val1) == str(val2)

    def _values_equal(self, val1, val2) -> bool:
        """
        ë‘ ê°’ì´ ë™ì¼í•œì§€ í™•ì¸ (NaN ì²˜ë¦¬ í¬í•¨)

        Args:
            val1: ì²« ë²ˆì§¸ ê°’
            val2: ë‘ ë²ˆì§¸ ê°’

        Returns:
            ë™ì¼ ì—¬ë¶€
        """
        try:
            # pandas Seriesë‚˜ numpy arrayì¸ ê²½ìš° ì²˜ë¦¬
            if hasattr(val1, "__len__") and not isinstance(val1, str):
                val1 = val1.iloc[0] if hasattr(val1, "iloc") else val1[0]
            if hasattr(val2, "__len__") and not isinstance(val2, str):
                val2 = val2.iloc[0] if hasattr(val2, "iloc") else val2[0]

            # NaN ì²´í¬
            val1_is_na = (
                pd.isna(val1)
                if pd.__version__ >= "1.0.0"
                else (val1 is None or str(val1).strip() == "")
            )
            val2_is_na = (
                pd.isna(val2)
                if pd.__version__ >= "1.0.0"
                else (val2 is None or str(val2).strip() == "")
            )

            if val1_is_na and val2_is_na:
                return True
            if val1_is_na or val2_is_na:
                return False

            # ë¬¸ìì—´ ë¹„êµ
            return str(val1).strip() == str(val2).strip()

        except Exception:
            # ì˜ˆì™¸ ë°œìƒì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            return str(val1) == str(val2)

    def _validate_field_update(
        self, column_name: str, new_value: Any, old_value: Any
    ) -> Dict[str, Any]:
        """
        í•„ë“œ ì—…ë°ì´íŠ¸ ìœ íš¨ì„± ê²€ì¦

        Args:
            column_name: ì»¬ëŸ¼ëª…
            new_value: ìƒˆ ê°’
            old_value: ê¸°ì¡´ ê°’

        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        validation_result = {"valid": True, "error": None, "warnings": []}

        # HVDC ì½”ë“œ ê²€ì¦
        if "hvdc" in column_name.lower() and "code" in column_name.lower():
            if pd.notna(new_value):
                hvdc_validation = self.hvdc_validator.validate_hvdc_code(str(new_value))
                if not hvdc_validation["valid"]:
                    validation_result["valid"] = False
                    validation_result["error"] = (
                        f"HVDC ì½”ë“œ í˜•ì‹ ì˜¤ë¥˜: {hvdc_validation['error']}"
                    )

        # ìˆ«ì í•„ë“œ ê²€ì¦
        numeric_keywords = ["cbm", "weight", "pkg", "price", "qty", "quantity"]
        if any(keyword in column_name.lower() for keyword in numeric_keywords):
            if pd.notna(new_value):
                try:
                    float_val = float(new_value)
                    if float_val < 0:
                        validation_result["warnings"].append("ìŒìˆ˜ ê°’ì´ ì…ë ¥ë¨")
                except (ValueError, TypeError):
                    validation_result["valid"] = False
                    validation_result["error"] = f"ìˆ«ì í˜•ì‹ì´ ì•„ë‹˜: '{new_value}'"

        return validation_result

    def get_sync_history(self) -> List[Dict]:
        """ë™ê¸°í™” ì´ë ¥ ë°˜í™˜"""
        return self.sync_history.copy()

    def generate_sync_report(self, sync_result: Dict[str, Any]) -> str:
        """
        ë™ê¸°í™” ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            sync_result: ë™ê¸°í™” ê²°ê³¼

        Returns:
            ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
        """
        report_lines = [
            "=" * 60,
            "HVDC ë°ì´í„° ë™ê¸°í™” ë¦¬í¬íŠ¸",
            "=" * 60,
            f"ì‹¤í–‰ ì‹œê°„: {sync_result['timestamp']}",
            f"ëª¨ë“œ: {'ì‹œë®¬ë ˆì´ì…˜' if sync_result['dry_run'] else 'ì‹¤ì œ ì—…ë°ì´íŠ¸'}",
            f"ìƒíƒœ: {'ì„±ê³µ' if sync_result['success'] else 'ì‹¤íŒ¨'}",
            "",
        ]

        if sync_result.get("backup_path"):
            report_lines.append(f"ë°±ì—… íŒŒì¼: {sync_result['backup_path']}")
            report_lines.append("")

        # ë§¤ì¹­ ê²°ê³¼
        if "matching_results" in sync_result:
            matching = sync_result["matching_results"]["summary"]
            report_lines.extend(
                [
                    "ë§¤ì¹­ ê²°ê³¼:",
                    f"  - ì „ì²´ ì†ŒìŠ¤ ì¼€ì´ìŠ¤: {matching['total_source_cases']}",
                    f"  - ì •í™•í•œ ë§¤ì¹˜: {matching['exact_matches']}",
                    f"  - ìœ ì‚¬ ë§¤ì¹˜: {matching['fuzzy_matches']}",
                    f"  - ì‹ ê·œ ì¼€ì´ìŠ¤: {matching['new_cases']}",
                    f"  - ëª¨í˜¸í•œ ë§¤ì¹˜: {matching['ambiguous_matches']}",
                    f"  - ë§¤ì¹˜ìœ¨: {matching['match_rate']:.1f}%",
                    "",
                ]
            )

        # ì—…ë°ì´íŠ¸ ìš”ì•½
        if "update_summary" in sync_result:
            update = sync_result["update_summary"]
            report_lines.extend(
                [
                    "ì—…ë°ì´íŠ¸ ìš”ì•½:",
                    f"  - ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ: {update['updated_records']}",
                    f"  - ì‹ ê·œ ë ˆì½”ë“œ: {update['new_records']}",
                    f"  - ê±´ë„ˆë›´ ë ˆì½”ë“œ: {update['skipped_records']}",
                    f"  - ì´ ë³€ê²½ì‚¬í•­: {update['total_changes']}",
                    "",
                ]
            )

            # ë‚ ì§œ ì—…ë°ì´íŠ¸ ì„¸ë¶€ì‚¬í•­
            if "date_updates" in update:
                date_stats = update["date_updates"]
                total_date_updates = (
                    date_stats["high_priority_dates"]
                    + date_stats["medium_priority_dates"]
                    + date_stats["low_priority_dates"]
                )

                if total_date_updates > 0:
                    report_lines.extend(
                        [
                            "ë‚ ì§œ í•„ë“œ ì—…ë°ì´íŠ¸ (ìµœìš°ì„  ì²˜ë¦¬):",
                            f"  - ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ: {date_stats['high_priority_dates']}ê°œ",
                            f"  - ì¼ë°˜ ë‚ ì§œ í•„ë“œ: {date_stats['medium_priority_dates']}ê°œ",
                            f"  - ê¸°íƒ€ ì‹œê°„ ê´€ë ¨: {date_stats['low_priority_dates']}ê°œ",
                            f"  - ë‚ ì§œ ì™¸ í•„ë“œ: {date_stats['non_date_fields']}ê°œ",
                            f"  - ë‚ ì§œ ì—…ë°ì´íŠ¸ ë¹„ìœ¨: {(total_date_updates/max(update['total_changes'], 1)*100):.1f}%",
                            "",
                        ]
                    )

            if update["validation_errors"]:
                report_lines.extend(
                    [
                        f"ê²€ì¦ ì˜¤ë¥˜ ({len(update['validation_errors'])}ê±´):",
                        *[
                            f"  - {error['error']}"
                            for error in update["validation_errors"][:5]
                        ],
                        "",
                    ]
                )

        # ì´ìŠˆ
        if sync_result.get("issues"):
            report_lines.extend(
                [
                    "ë°œìƒí•œ ì´ìŠˆ:",
                    *[f"  - {issue}" for issue in sync_result["issues"]],
                    "",
                ]
            )

        report_lines.append("=" * 60)

        return "\n".join(report_lines)
