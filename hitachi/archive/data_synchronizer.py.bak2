"""
Masterfile â†’ Warehouse ìë™ ë™ê¸°í™” ì—”ì§„
CASE NO ë§¤ì¹­ ê¸°ë°˜ ë°ì´í„° ì—…ë°ì´íŠ¸ (A~AQì—´ ë²”ìœ„ ì œí•œ)
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import os
import shutil
from pathlib import Path

from .case_matcher import CaseMatcher
from .header_detector import HeaderDetector
from .hvdc_validator import HVDCValidator
from .update_tracker import UpdateTracker


class DataSynchronizer:
    """ë°ì´í„° ë™ê¸°í™” ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 column_limit: str = 'AQ',
                 backup_enabled: bool = True,
                 validation_enabled: bool = True,
                 prioritize_dates: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            column_limit: ì—…ë°ì´íŠ¸í•  ìµœëŒ€ ì»¬ëŸ¼ (ê¸°ë³¸ê°’: 'AQ')
            backup_enabled: ë°±ì—… ìƒì„± ì—¬ë¶€
            validation_enabled: ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì—¬ë¶€
            prioritize_dates: ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ìš°ì„ ìˆœìœ„ ì„¤ì •
        """
        self.column_limit = column_limit
        self.backup_enabled = backup_enabled
        self.validation_enabled = validation_enabled
        self.prioritize_dates = prioritize_dates
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.case_matcher = CaseMatcher()
        self.header_detector = HeaderDetector()
        self.hvdc_validator = HVDCValidator()
        self.update_tracker = UpdateTracker()
        
        # ì»¬ëŸ¼ ì œí•œ ì¸ë±ìŠ¤ ê³„ì‚° (AQ = 43ë²ˆì§¸ ì»¬ëŸ¼, 0-based index = 42)
        self.max_column_index = self._column_letter_to_index(column_limit)
        
        # ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        # ìµœìš°ì„ : ì°½ê³ ë³„ ë‚ ì§œ ì»¬ëŸ¼ë“¤
        self.high_priority_warehouse_keywords = [
            'dhl warehouse', 'dsv indoor', 'dsv al markaz', 'dsv outdoor', 
            'hauler indoor', 'dsv mzp', 'mosb', 'shifting', 'mir', 'shu', 'das', 'agi'
        ]
        
        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ë“¤  
        self.medium_priority_date_keywords = [
            'etd/atd', 'eta/ata', 'status_location_date', 'etd', 'eta', 'atd', 'ata',
            'arrival', 'departure', 'delivery', 'shipment', 'date'
        ]
        
        # í†µí•© í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.warehouse_date_keywords = (
            self.high_priority_warehouse_keywords + 
            self.medium_priority_date_keywords +
            ['warehouse', 'site', 'location', 'actual', 'estimated', 'time']
        )
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜
        self.date_column_patterns = [
            r'.*date.*', r'.*eta.*', r'.*etd.*', r'.*ata.*', r'.*atd.*',
            r'.*arrival.*', r'.*departure.*', r'.*delivery.*', r'.*shipment.*',
            r'.*warehouse.*', r'.*site.*', r'.*location.*'
        ]
        
        # ë™ê¸°í™” ì´ë ¥
        self.sync_history = []
    
    def _column_letter_to_index(self, column_letter: str) -> int:
        """
        ì—‘ì…€ ì»¬ëŸ¼ ë¬¸ìë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (A=0, B=1, ..., AQ=42)
        
        Args:
            column_letter: ì»¬ëŸ¼ ë¬¸ì (ì˜ˆ: 'AQ')
            
        Returns:
            0-based ì»¬ëŸ¼ ì¸ë±ìŠ¤
        """
        result = 0
        for char in column_letter.upper():
            result = result * 26 + (ord(char) - ord('A') + 1)
        return result - 1
    
    def _index_to_column_letter(self, index: int) -> str:
        """
        ì¸ë±ìŠ¤ë¥¼ ì—‘ì…€ ì»¬ëŸ¼ ë¬¸ìë¡œ ë³€í™˜
        
        Args:
            index: 0-based ì»¬ëŸ¼ ì¸ë±ìŠ¤
            
        Returns:
            ì»¬ëŸ¼ ë¬¸ì (ì˜ˆ: 'AQ')
        """
        result = ''
        index += 1  # 1-basedë¡œ ë³€ê²½
        while index > 0:
            index -= 1
            result = chr(index % 26 + ord('A')) + result
            index //= 26
        return result
    
    def _identify_date_columns(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        DataFrameì—ì„œ ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ë“¤ì„ ì‹ë³„í•˜ê³  ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¥˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        
        Args:
            df: ëŒ€ìƒ DataFrame
            
        Returns:
            ìš°ì„ ìˆœìœ„ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë”•ì…”ë„ˆë¦¬
        """
        date_columns = {
            'high_priority': [],    # ì°½ê³ ë³„/í˜„ì¥ë³„ ì¤‘ìš” ë‚ ì§œ (ìµœìš°ì„ )
            'medium_priority': [],  # ì¼ë°˜ ë‚ ì§œ í•„ë“œ
            'low_priority': []      # ê¸°íƒ€ ì‹œê°„ ê´€ë ¨ í•„ë“œ
        }
        
        for col in df.columns:
            col_lower = str(col).lower().strip()
            
            # 1. ìµœìš°ì„ : ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ ì»¬ëŸ¼ë“¤ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            if any(keyword == col_lower or keyword in col_lower for keyword in self.high_priority_warehouse_keywords):
                date_columns['high_priority'].append(col)
                print(f"[ìµœìš°ì„ ] ì°½ê³ ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue
            
            # 2. ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ë“¤
            if any(keyword == col_lower or keyword in col_lower for keyword in self.medium_priority_date_keywords):
                date_columns['medium_priority'].append(col)
                print(f"[ì¤‘ê°„] ì¼ë°˜ ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue
            
            # 3. ì¼ë°˜ ë‚ ì§œ íŒ¨í„´ ë§¤ì¹­ (ì¶”ê°€ íƒì§€)
            if any(re.match(pattern, col_lower, re.IGNORECASE) for pattern in self.date_column_patterns):
                if col not in date_columns['high_priority'] and col not in date_columns['medium_priority']:
                    date_columns['medium_priority'].append(col)
                    print(f"[íŒ¨í„´ë§¤ì¹­] ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                    continue
            
            # 4. ìƒíƒœ ê´€ë ¨ ë‚ ì§œ í•„ë“œ (Status_Location_Date ë“±)
            if 'status' in col_lower and any(date_word in col_lower for date_word in ['date', 'time', 'year', 'month']):
                date_columns['medium_priority'].append(col)
                print(f"[ìƒíƒœ] ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {col}")
                continue
            
            # 5. ê¸°íƒ€ ì‹œê°„ ê´€ë ¨ í•„ë“œ
            if any(time_keyword in col_lower for time_keyword in ['time', 'schedule', 'plan', 'handling']):
                if col not in date_columns['high_priority'] and col not in date_columns['medium_priority']:
                    date_columns['low_priority'].append(col)
        
        # ë°œê²¬ëœ ë‚ ì§œ ì»¬ëŸ¼ ìš”ì•½ ì¶œë ¥
        print(f"\\n=== ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼ ===")
        print(f"ìµœìš°ì„  ({len(date_columns['high_priority'])}ê°œ): {date_columns['high_priority']}")
        print(f"ì¤‘ê°„ ({len(date_columns['medium_priority'])}ê°œ): {date_columns['medium_priority']}")
        print(f"ë‚®ìŒ ({len(date_columns['low_priority'])}ê°œ): {date_columns['low_priority']}")
        
        return date_columns
    
    def _validate_date_value(self, value: Any, column_name: str) -> Dict[str, Any]:
        """
        ë‚ ì§œ ê°’ì˜ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            value: ê²€ì¦í•  ë‚ ì§œ ê°’
            column_name: ì»¬ëŸ¼ëª…
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        validation_result = {
            'valid': True,
            'error': None,
            'warnings': [],
            'parsed_date': None,
            'original_value': value
        }
        
        if pd.isna(value) or value is None or str(value).strip() == '':
            return validation_result  # ë¹ˆ ê°’ì€ ìœ íš¨í•¨
        
        value_str = str(value).strip()
        
        # ë‚ ì§œ íŒŒì‹± ì‹œë„
        date_formats = [
            '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%m/%d',
            '%Y-%m-%d %H:%M:%S', '%d/%m/%Y %H:%M:%S',
            '%Y-%m-%d %H:%M', '%d/%m/%Y %H:%M',
            '%Y%m%d', '%d-%m-%Y', '%m-%d-%Y'
        ]
        
        parsed_date = None
        for date_format in date_formats:
            try:
                parsed_date = pd.to_datetime(value_str, format=date_format)
                validation_result['parsed_date'] = parsed_date
                break
            except (ValueError, TypeError):
                continue
        
        # pandasì˜ ì¼ë°˜ì ì¸ ë‚ ì§œ íŒŒì‹± ì‹œë„
        if parsed_date is None:
            try:
                parsed_date = pd.to_datetime(value_str, infer_datetime_format=True)
                validation_result['parsed_date'] = parsed_date
            except (ValueError, TypeError):
                validation_result['valid'] = False
                validation_result['error'] = f"ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŒ: '{value_str}'"
                return validation_result
        
        # ë‚ ì§œ ë²”ìœ„ ê²€ì¦
        if parsed_date is not None:
            current_year = datetime.now().year
            
            # ê³¼ê±° ë„ˆë¬´ ë¨¼ ë‚ ì§œ (1900ë…„ ì´ì „) ë˜ëŠ” ë¯¸ë˜ ë„ˆë¬´ ë¨¼ ë‚ ì§œ (10ë…„ í›„) ê²½ê³ 
            if parsed_date.year < 1900:
                validation_result['warnings'].append(f'ë§¤ìš° ì˜¤ë˜ëœ ë‚ ì§œ: {parsed_date.year}ë…„')
            elif parsed_date.year > current_year + 10:
                validation_result['warnings'].append(f'ë„ˆë¬´ ë¨¼ ë¯¸ë˜ ë‚ ì§œ: {parsed_date.year}ë…„')
            
            # ETA/ETD íŠ¹ë³„ ê²€ì¦
            if any(keyword in column_name.lower() for keyword in ['eta', 'etd']):
                if parsed_date < pd.Timestamp.now() - pd.Timedelta(days=365):
                    validation_result['warnings'].append('ETA/ETDê°€ 1ë…„ ì´ìƒ ê³¼ê±° ë‚ ì§œì„')
        
        return validation_result
    
    def _prioritize_column_mapping(self, column_mapping: Dict[str, str], warehouse_df: pd.DataFrame) -> Dict[str, str]:
        """
        ì»¬ëŸ¼ ë§¤í•‘ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬ (ë‚ ì§œ ì»¬ëŸ¼ ìš°ì„ )
        
        Args:
            column_mapping: ì›ë³¸ ì»¬ëŸ¼ ë§¤í•‘
            warehouse_df: Warehouse DataFrame
            
        Returns:
            ìš°ì„ ìˆœìœ„ê°€ ì ìš©ëœ ì»¬ëŸ¼ ë§¤í•‘
        """
        if not self.prioritize_dates:
            return column_mapping
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜
        date_columns = self._identify_date_columns(warehouse_df)
        
        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì»¬ëŸ¼ ê·¸ë£¹í™”
        prioritized_mapping = {}
        
        # 1. ìµœìš°ì„ : ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ
        for master_col, warehouse_col in column_mapping.items():
            if warehouse_col in date_columns['high_priority']:
                prioritized_mapping[master_col] = warehouse_col
        
        # 2. ì¤‘ê°„ ìš°ì„ ìˆœìœ„: ì¼ë°˜ ë‚ ì§œ
        for master_col, warehouse_col in column_mapping.items():
            if warehouse_col in date_columns['medium_priority'] and master_col not in prioritized_mapping:
                prioritized_mapping[master_col] = warehouse_col
        
        # 3. ë‚®ì€ ìš°ì„ ìˆœìœ„: ê¸°íƒ€ ì‹œê°„ ê´€ë ¨
        for master_col, warehouse_col in column_mapping.items():
            if warehouse_col in date_columns['low_priority'] and master_col not in prioritized_mapping:
                prioritized_mapping[master_col] = warehouse_col
        
        # 4. ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        for master_col, warehouse_col in column_mapping.items():
            if master_col not in prioritized_mapping:
                prioritized_mapping[master_col] = warehouse_col
        
        return prioritized_mapping
    
    def _get_date_priority(self, column_name: str, date_columns: Dict[str, List[str]]) -> str:
        """
        ì»¬ëŸ¼ì˜ ë‚ ì§œ ìš°ì„ ìˆœìœ„ í™•ì¸
        
        Args:
            column_name: í™•ì¸í•  ì»¬ëŸ¼ëª…
            date_columns: ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼
            
        Returns:
            ìš°ì„ ìˆœìœ„ ('high_priority', 'medium_priority', 'low_priority', 'non_date')
        """
        if column_name in date_columns['high_priority']:
            return 'high_priority'
        elif column_name in date_columns['medium_priority']:
            return 'medium_priority'
        elif column_name in date_columns['low_priority']:
            return 'low_priority'
        else:
            return 'non_date'
    
    def create_backup(self, warehouse_file_path: str) -> str:
        """
        Warehouse íŒŒì¼ì˜ ë°±ì—… ìƒì„±
        
        Args:
            warehouse_file_path: ë°±ì—…í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        if not self.backup_enabled:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = Path(warehouse_file_path)
        backup_dir = file_path.parent / "backups"
        backup_dir.mkdir(exist_ok=True)
        
        backup_filename = f"{file_path.stem}_backup_{timestamp}{file_path.suffix}"
        backup_path = backup_dir / backup_filename
        
        shutil.copy2(warehouse_file_path, backup_path)
        
        return str(backup_path)
    
    def load_and_analyze_files(self, 
                             masterfile_path: str, 
                             warehouse_path: str) -> Dict[str, Any]:
        """
        Masterfileê³¼ Warehouse íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„ì„
        
        Args:
            masterfile_path: Masterfile ê²½ë¡œ
            warehouse_path: Warehouse íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        analysis_result = {
            'masterfile': {},
            'warehouse': {},
            'compatibility': {},
            'sync_feasibility': True,
            'issues': []
        }
        
        try:
            # Masterfile ë¶„ì„
            master_sheets = pd.ExcelFile(masterfile_path).sheet_names
            master_df = None
            master_sheet = None
            
            # CASE Listê°€ í¬í•¨ëœ ì‹œíŠ¸ ì°¾ê¸°
            for sheet in master_sheets:
                if 'case' in sheet.lower() and 'list' in sheet.lower():
                    master_sheet = sheet
                    break
            
            if not master_sheet and master_sheets:
                master_sheet = master_sheets[0]  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
            
            if master_sheet:
                # í—¤ë” íƒì§€í•˜ì—¬ ë¡œë“œ
                df_preview = pd.read_excel(masterfile_path, sheet_name=master_sheet, nrows=20, header=None)
                header_row = self.header_detector.detect_header_row(df_preview)
                
                if header_row is not None:
                    master_df = pd.read_excel(masterfile_path, sheet_name=master_sheet, header=header_row)
                else:
                    master_df = pd.read_excel(masterfile_path, sheet_name=master_sheet)
                
                analysis_result['masterfile'] = {
                    'sheet_name': master_sheet,
                    'shape': master_df.shape,
                    'columns': list(master_df.columns),
                    'header_row': header_row,
                    'case_column': self._find_case_column(master_df)
                }
            
            # Warehouse íŒŒì¼ ë¶„ì„
            warehouse_sheets = pd.ExcelFile(warehouse_path).sheet_names
            warehouse_df = None
            warehouse_sheet = None
            
            # Case List ì‹œíŠ¸ ì°¾ê¸°
            for sheet in warehouse_sheets:
                if 'case' in sheet.lower():
                    warehouse_sheet = sheet
                    break
            
            if not warehouse_sheet and warehouse_sheets:
                warehouse_sheet = warehouse_sheets[0]
            
            if warehouse_sheet:
                # í—¤ë” íƒì§€í•˜ì—¬ ë¡œë“œ
                df_preview = pd.read_excel(warehouse_path, sheet_name=warehouse_sheet, nrows=20, header=None)
                header_row = self.header_detector.detect_header_row(df_preview)
                
                if header_row is not None:
                    warehouse_df = pd.read_excel(warehouse_path, sheet_name=warehouse_sheet, header=header_row)
                else:
                    warehouse_df = pd.read_excel(warehouse_path, sheet_name=warehouse_sheet)
                
                analysis_result['warehouse'] = {
                    'sheet_name': warehouse_sheet,
                    'shape': warehouse_df.shape,
                    'columns': list(warehouse_df.columns),
                    'header_row': header_row,
                    'case_column': self._find_case_column(warehouse_df),
                    'column_limit_index': min(self.max_column_index, warehouse_df.shape[1] - 1),
                    'updatable_columns': list(warehouse_df.columns[:self.max_column_index + 1])
                }
            
            # í˜¸í™˜ì„± ê²€ì‚¬
            if master_df is not None and warehouse_df is not None:
                analysis_result['compatibility'] = self._check_compatibility(
                    master_df, warehouse_df,
                    analysis_result['masterfile']['case_column'],
                    analysis_result['warehouse']['case_column']
                )
                
                # ì €ì¥ (ë™ê¸°í™”ì—ì„œ ì‚¬ìš©)
                analysis_result['masterfile']['dataframe'] = master_df
                analysis_result['warehouse']['dataframe'] = warehouse_df
        
        except Exception as e:
            analysis_result['sync_feasibility'] = False
            analysis_result['issues'].append(f"íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return analysis_result
    
    def _find_case_column(self, df: pd.DataFrame) -> Optional[str]:
        """
        DataFrameì—ì„œ CASE NO ì»¬ëŸ¼ ì°¾ê¸°
        
        Args:
            df: ëŒ€ìƒ DataFrame
            
        Returns:
            CASE NO ì»¬ëŸ¼ëª… ë˜ëŠ” None
        """
        case_keywords = ['case no', 'case_no', 'caseno', 'case', 'case number']
        
        for col in df.columns:
            col_lower = str(col).lower().strip()
            for keyword in case_keywords:
                if keyword in col_lower:
                    return col
        
        return None
    
    def _check_compatibility(self, 
                           master_df: pd.DataFrame, 
                           warehouse_df: pd.DataFrame,
                           master_case_col: str,
                           warehouse_case_col: str) -> Dict[str, Any]:
        """
        ë‘ íŒŒì¼ ê°„ì˜ í˜¸í™˜ì„± ê²€ì‚¬
        
        Args:
            master_df: Masterfile DataFrame
            warehouse_df: Warehouse DataFrame
            master_case_col: Masterfile CASE ì»¬ëŸ¼ëª…
            warehouse_case_col: Warehouse CASE ì»¬ëŸ¼ëª…
            
        Returns:
            í˜¸í™˜ì„± ê²€ì‚¬ ê²°ê³¼
        """
        compatibility = {
            'case_columns_found': bool(master_case_col and warehouse_case_col),
            'common_columns': [],
            'master_only_columns': [],
            'warehouse_only_columns': [],
            'column_mapping': {},
            'issues': []
        }
        
        if not compatibility['case_columns_found']:
            compatibility['issues'].append('CASE NO ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ')
            return compatibility
        
        # ì»¬ëŸ¼ ë¹„êµ (ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ)
        master_columns = set(master_df.columns)
        warehouse_columns = set(warehouse_df.columns[:self.max_column_index + 1])
        
        compatibility['common_columns'] = list(master_columns & warehouse_columns)
        compatibility['master_only_columns'] = list(master_columns - warehouse_columns)
        compatibility['warehouse_only_columns'] = list(warehouse_columns - master_columns)
        
        # ì»¬ëŸ¼ ë§¤í•‘ ìƒì„± (í‘œì¤€í™”ëœ ì´ë¦„ ê¸°ë°˜)
        for master_col in master_df.columns:
            master_norm = self.header_detector.standardize_column_name(master_col)
            
            # Warehouseì—ì„œ ë™ì¼í•œ í‘œì¤€í™”ëœ ì´ë¦„ì„ ê°€ì§„ ì»¬ëŸ¼ ì°¾ê¸°
            for warehouse_col in warehouse_df.columns[:self.max_column_index + 1]:
                warehouse_norm = self.header_detector.standardize_column_name(warehouse_col)
                
                if master_norm == warehouse_norm:
                    compatibility['column_mapping'][master_col] = warehouse_col
                    break
        
        return compatibility
    
    def synchronize_data(self, 
                        masterfile_path: str, 
                        warehouse_path: str,
                        dry_run: bool = False) -> Dict[str, Any]:
        """
        ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰
        
        Args:
            masterfile_path: Masterfile ê²½ë¡œ
            warehouse_path: Warehouse íŒŒì¼ ê²½ë¡œ
            dry_run: ì‹¤ì œ ì €ì¥ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰
            
        Returns:
            ë™ê¸°í™” ê²°ê³¼
        """
        sync_result = {
            'success': False,
            'timestamp': datetime.now().isoformat(),
            'backup_path': None,
            'analysis': {},
            'matching_results': {},
            'update_summary': {},
            'issues': [],
            'dry_run': dry_run
        }
        
        try:
            # 1. íŒŒì¼ ë¶„ì„
            analysis = self.load_and_analyze_files(masterfile_path, warehouse_path)
            sync_result['analysis'] = analysis
            
            if not analysis['sync_feasibility']:
                sync_result['issues'].extend(analysis['issues'])
                return sync_result
            
            # 2. ë°±ì—… ìƒì„±
            if not dry_run:
                backup_path = self.create_backup(warehouse_path)
                sync_result['backup_path'] = backup_path
            
            # 3. UpdateTracker ì´ˆê¸°í™” ë° Before ìƒíƒœ ìº¡ì²˜
            warehouse_df = analysis['warehouse']['dataframe']
            self.update_tracker.capture_before_state(warehouse_df, analysis['warehouse']['sheet_name'])
            
            # 3. CASE NO ë§¤ì¹­
            master_df = analysis['masterfile']['dataframe']
            warehouse_df = analysis['warehouse']['dataframe']
            
            master_case_col = analysis['masterfile']['case_column']
            warehouse_case_col = analysis['warehouse']['case_column']
            
            if not (master_case_col and warehouse_case_col):
                sync_result['issues'].append('CASE NO ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë™ê¸°í™” ë¶ˆê°€')
                return sync_result
            
            matching_results = self.case_matcher.find_matching_cases(
                master_df[master_case_col], 
                warehouse_df[warehouse_case_col]
            )
            sync_result['matching_results'] = self.case_matcher.generate_matching_report(matching_results)
            
            # 4. ì»¬ëŸ¼ ë§¤í•‘ ìš°ì„ ìˆœìœ„ ì ìš© (ë‚ ì§œ ì»¬ëŸ¼ ìš°ì„ )
            prioritized_mapping = self._prioritize_column_mapping(
                analysis['compatibility']['column_mapping'], 
                warehouse_df
            )
            
            # 5. ë°ì´í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰ (CASE NO ë§¤ì¹­ â†’ ë‚ ì§œ ìš°ì„  ì—…ë°ì´íŠ¸)
            update_summary = self._perform_updates(
                master_df, warehouse_df,
                matching_results, prioritized_mapping,
                dry_run
            )
            sync_result['update_summary'] = update_summary
            
            # 6. UpdateTracker After ìƒíƒœ ìº¡ì²˜ ë° ì¶”ì  ì¢…ë£Œ
            self.update_tracker.capture_after_state(warehouse_df, analysis['warehouse']['sheet_name'])
            self.update_tracker.end_update_tracking()
            
            # 7. íŒŒì¼ ì €ì¥ (dry_runì´ ì•„ë‹Œ ê²½ìš°)
            if not dry_run and update_summary['total_changes'] > 0:
                # ì»¬ëŸ¼ ë²”ìœ„ ì œí•œ ì ìš©í•˜ì—¬ ì €ì¥
                updated_warehouse = warehouse_df.iloc[:, :self.max_column_index + 1]
                
                with pd.ExcelWriter(warehouse_path, engine='openpyxl') as writer:
                    updated_warehouse.to_excel(
                        writer, 
                        sheet_name=analysis['warehouse']['sheet_name'],
                        index=False
                    )
                
                sync_result['success'] = True
                print(f"\nâœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {warehouse_path}")
            elif dry_run:
                sync_result['success'] = True  # ì‹œë®¬ë ˆì´ì…˜ ì„±ê³µ
                print(f"\nğŸ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì™„ë£Œ (ì‹¤ì œ íŒŒì¼ ë³€ê²½ ì—†ìŒ)")
            
            # 8. ìƒì„¸ ì¶”ì  ë¦¬í¬íŠ¸ ìƒì„±
            if update_summary['total_changes'] > 0:
                print(f"\nğŸ“Š ìƒì„¸ ì¶”ì  ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
                
                # íˆíŠ¸ë§µ ìƒì„±
                heatmap_path = self.update_tracker.create_change_heatmap()
                sync_result['heatmap_path'] = heatmap_path
                
                # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
                detailed_report_path = self.update_tracker.generate_detailed_report()
                sync_result['detailed_report_path'] = detailed_report_path
                
                # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
                comparison_report = self.update_tracker.generate_change_comparison_report()
                sync_result['comparison_report'] = comparison_report
            
            # 6. ë™ê¸°í™” ì´ë ¥ ì €ì¥
            self.sync_history.append({
                'timestamp': sync_result['timestamp'],
                'masterfile': masterfile_path,
                'warehouse': warehouse_path,
                'dry_run': dry_run,
                'changes': update_summary['total_changes'],
                'new_cases': len(matching_results.get('new_cases', [])),
                'updated_cases': len(matching_results.get('exact_matches', {})) + len(matching_results.get('fuzzy_matches', {}))
            })
        
        except Exception as e:
            sync_result['issues'].append(f"ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return sync_result
    
    def _perform_updates(self, 
                        master_df: pd.DataFrame,
                        warehouse_df: pd.DataFrame,
                        matching_results: Dict,
                        column_mapping: Dict[str, str],
                        dry_run: bool) -> Dict[str, Any]:
        """
        ì‹¤ì œ ë°ì´í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰ (CASE NO ë§¤ì¹­ â†’ ë‚ ì§œ ìš°ì„  ì—…ë°ì´íŠ¸)
        
        Args:
            master_df: Masterfile DataFrame
            warehouse_df: Warehouse DataFrame
            matching_results: ë§¤ì¹­ ê²°ê³¼
            column_mapping: ì»¬ëŸ¼ ë§¤í•‘ (ì´ë¯¸ ìš°ì„ ìˆœìœ„ ì ìš©ë¨)
            dry_run: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            
        Returns:
            ì—…ë°ì´íŠ¸ ìš”ì•½
        """
        update_summary = {
            'updated_records': 0,
            'new_records': 0,
            'skipped_records': 0,
            'total_changes': 0,
            'updated_fields': {},
            'validation_errors': [],
            'changes_detail': [],
            'date_updates': {
                'high_priority_dates': 0,    # ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ
                'medium_priority_dates': 0,  # ì¼ë°˜ ë‚ ì§œ
                'low_priority_dates': 0,     # ê¸°íƒ€ ì‹œê°„ ê´€ë ¨
                'non_date_fields': 0         # ë‚ ì§œê°€ ì•„ë‹Œ í•„ë“œ
            }
        }
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë¶„ë¥˜ (ì—…ë°ì´íŠ¸ ìš°ì„ ìˆœìœ„ ê²°ì •ìš©)
        date_columns = self._identify_date_columns(warehouse_df)
        
        # UpdateTracker ì—…ë°ì´íŠ¸ ì¶”ì  ì‹œì‘
        total_cases = len(matching_results.get('exact_matches', {})) + len(matching_results.get('fuzzy_matches', {})) + len(matching_results.get('new_cases', []))
        warehouse_columns = date_columns['high_priority']  # ìµœìš°ì„  ì°½ê³  ì»¬ëŸ¼ë“¤
        self.update_tracker.start_update_tracking(total_cases, warehouse_columns)
        
        # 1. ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ (ì •í™•í•œ ë§¤ì¹˜ + ìœ ì‚¬ ë§¤ì¹˜)
        all_matches = {**matching_results.get('exact_matches', {}), 
                      **matching_results.get('fuzzy_matches', {})}
        
        for master_idx, match_info in all_matches.items():
            warehouse_idx = match_info['target_index']
            changes_made = []
            
            print(f"[INFO] ì—…ë°ì´íŠ¸ ì¤‘: CASE {match_info['source_case']} (ë§¤ì¹­ íƒ€ì…: {match_info['match_type']})")
            
            # ë§¤í•‘ëœ ì»¬ëŸ¼ë“¤ì„ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì—…ë°ì´íŠ¸ (column_mappingì€ ì´ë¯¸ ìš°ì„ ìˆœìœ„ ì ìš©ë¨)
            for master_col, warehouse_col in column_mapping.items():
                if warehouse_col not in warehouse_df.columns:
                    continue
                
                # ì»¬ëŸ¼ ë²”ìœ„ ì œí•œ í™•ì¸
                warehouse_col_idx = warehouse_df.columns.get_loc(warehouse_col)
                if warehouse_col_idx > self.max_column_index:
                    continue
                
                master_value = master_df.loc[master_idx, master_col]
                warehouse_value = warehouse_df.loc[warehouse_idx, warehouse_col]
                
                # ê°’ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if not self._values_equal(master_value, warehouse_value):
                    # ë‚ ì§œ ì»¬ëŸ¼ì¸ì§€ í™•ì¸í•˜ê³  ìš°ì„ ìˆœìœ„ ê²°ì •
                    date_priority = self._get_date_priority(warehouse_col, date_columns)
                    
                    # ìœ íš¨ì„± ê²€ì¦ (ë‚ ì§œ ì»¬ëŸ¼ì— ëŒ€í•´ì„œëŠ” ë‚ ì§œ ê²€ì¦ ìˆ˜í–‰)
                    validation_result = None
                    if self.validation_enabled:
                        if date_priority != 'non_date':
                            # ë‚ ì§œ ê²€ì¦
                            validation_result = self._validate_date_value(master_value, warehouse_col)
                        else:
                            # ì¼ë°˜ í•„ë“œ ê²€ì¦
                            validation_result = self._validate_field_update(
                                master_col, master_value, warehouse_value
                            )
                        
                        if not validation_result['valid']:
                            update_summary['validation_errors'].append({
                                'row': warehouse_idx,
                                'column': warehouse_col,
                                'error': validation_result['error'],
                                'master_value': master_value,
                                'warehouse_value': warehouse_value,
                                'field_type': date_priority
                            })
                            continue
                    
                    # ì—…ë°ì´íŠ¸ ìˆ˜í–‰
                    if not dry_run:
                        warehouse_df.loc[warehouse_idx, warehouse_col] = master_value
                    
                    # ë‚ ì§œ ì»¬ëŸ¼ì¸ ê²½ìš° íŠ¹ë³„í•œ ë¡œê¹…
                    if date_priority in ['high_priority', 'medium_priority', 'low_priority']:
                        print(f"  [ë‚ ì§œ ì—…ë°ì´íŠ¸] {warehouse_col}: {warehouse_value} â†’ {master_value} (ìš°ì„ ìˆœìœ„: {date_priority})")
                        update_summary['date_updates'][f'{date_priority}_dates'] += 1
                    else:
                        update_summary['date_updates']['non_date_fields'] += 1
                    
                    changes_made.append({
                        'column': warehouse_col,
                        'old_value': warehouse_value,
                        'new_value': master_value,
                        'field_type': date_priority,
                        'validation_warnings': validation_result.get('warnings', []) if validation_result else []
                    })
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    if warehouse_col not in update_summary['updated_fields']:
                        update_summary['updated_fields'][warehouse_col] = 0
                    update_summary['updated_fields'][warehouse_col] += 1
            
            if changes_made:
                update_summary['updated_records'] += 1
                update_summary['total_changes'] += len(changes_made)
                update_summary['changes_detail'].append({
                    'row_index': warehouse_idx,
                    'case_no': match_info['source_case'],
                    'match_type': match_info['match_type'],
                    'changes': changes_made
                })
                
                # UpdateTrackerì— ì¼€ì´ìŠ¤ ì—…ë°ì´íŠ¸ ë¡œê·¸
                self.update_tracker.log_case_update(match_info['source_case'], changes_made)
            else:
                # ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ë„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                self.update_tracker.log_case_update(match_info['source_case'], [])
        
        # 2. ì‹ ê·œ ë ˆì½”ë“œ ì¶”ê°€
        new_cases = matching_results.get('new_cases', [])
        for new_case_info in new_cases:
            master_idx = new_case_info['source_index']
            
            # ìƒˆ í–‰ ë°ì´í„° ì¤€ë¹„
            new_row_data = {}
            for master_col, warehouse_col in column_mapping.items():
                if warehouse_col in warehouse_df.columns:
                    warehouse_col_idx = warehouse_df.columns.get_loc(warehouse_col)
                    if warehouse_col_idx <= self.max_column_index:
                        new_row_data[warehouse_col] = master_df.loc[master_idx, master_col]
            
            # ìœ íš¨ì„± ê²€ì¦
            if self.validation_enabled:
                validation_errors = []
                for col, value in new_row_data.items():
                    validation_result = self._validate_field_update(col, value, None)
                    if not validation_result['valid']:
                        validation_errors.append(validation_result['error'])
                
                if validation_errors:
                    update_summary['validation_errors'].extend(validation_errors)
                    update_summary['skipped_records'] += 1
                    continue
            
            # ìƒˆ í–‰ ì¶”ê°€
            if not dry_run:
                new_row_df = pd.DataFrame([new_row_data])
                warehouse_df = pd.concat([warehouse_df, new_row_df], ignore_index=True)
            
            update_summary['new_records'] += 1
            update_summary['total_changes'] += len(new_row_data)
            update_summary['changes_detail'].append({
                'row_index': 'NEW',
                'case_no': new_case_info['case_no'],
                'match_type': 'new_record',
                'changes': [{'column': col, 'old_value': None, 'new_value': val} 
                          for col, val in new_row_data.items()]
            })
            
            # UpdateTrackerì— ì‹ ê·œ ì¼€ì´ìŠ¤ ë¡œê·¸
            self.update_tracker.log_new_case(new_case_info['case_no'], new_row_data)
        
        return update_summary
    
    def _values_equal(self, val1, val2) -> bool:
        """
        ë‘ ê°’ì´ ë™ì¼í•œì§€ í™•ì¸ (NaN ì²˜ë¦¬ í¬í•¨)
        
        Args:
            val1: ì²« ë²ˆì§¸ ê°’
            val2: ë‘ ë²ˆì§¸ ê°’
            
        Returns:
            ë™ì¼ ì—¬ë¶€
        """
        try:
            # pandas Seriesë‚˜ numpy arrayì¸ ê²½ìš° ì²˜ë¦¬
            if hasattr(val1, '__len__') and not isinstance(val1, str):
                val1 = val1.iloc[0] if hasattr(val1, 'iloc') else val1[0]
            if hasattr(val2, '__len__') and not isinstance(val2, str):
                val2 = val2.iloc[0] if hasattr(val2, 'iloc') else val2[0]
            
            # NaN ì²´í¬
            val1_is_na = pd.isna(val1) if pd.__version__ >= '1.0.0' else (val1 is None or (hasattr(val1, '__len__') and len(str(val1).strip()) == 0))
            val2_is_na = pd.isna(val2) if pd.__version__ >= '1.0.0' else (val2 is None or (hasattr(val2, '__len__') and len(str(val2).strip()) == 0))
            
            if val1_is_na and val2_is_na:
                return True
            if val1_is_na or val2_is_na:
                return False
            
            # ë¬¸ìì—´ ë¹„êµ
            return str(val1).strip() == str(val2).strip()
        
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            return str(val1) == str(val2)
    
    def _validate_field_update(self, 
                             column_name: str, 
                             new_value: Any, 
                             old_value: Any) -> Dict[str, Any]:
        """
        í•„ë“œ ì—…ë°ì´íŠ¸ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            column_name: ì»¬ëŸ¼ëª…
            new_value: ìƒˆ ê°’
            old_value: ê¸°ì¡´ ê°’
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        validation_result = {
            'valid': True,
            'error': None,
            'warnings': []
        }
        
        # HVDC ì½”ë“œ ê²€ì¦
        if 'hvdc' in column_name.lower() and 'code' in column_name.lower():
            if pd.notna(new_value):
                hvdc_validation = self.hvdc_validator.validate_hvdc_code(str(new_value))
                if not hvdc_validation['valid']:
                    validation_result['valid'] = False
                    validation_result['error'] = f"HVDC ì½”ë“œ í˜•ì‹ ì˜¤ë¥˜: {hvdc_validation['error']}"
        
        # ìˆ«ì í•„ë“œ ê²€ì¦
        numeric_keywords = ['cbm', 'weight', 'pkg', 'price', 'qty', 'quantity']
        if any(keyword in column_name.lower() for keyword in numeric_keywords):
            if pd.notna(new_value):
                try:
                    float_val = float(new_value)
                    if float_val < 0:
                        validation_result['warnings'].append('ìŒìˆ˜ ê°’ì´ ì…ë ¥ë¨')
                except (ValueError, TypeError):
                    validation_result['valid'] = False
                    validation_result['error'] = f"ìˆ«ì í˜•ì‹ì´ ì•„ë‹˜: '{new_value}'"
        
        return validation_result
    
    def get_sync_history(self) -> List[Dict]:
        """ë™ê¸°í™” ì´ë ¥ ë°˜í™˜"""
        return self.sync_history.copy()
    
    def generate_sync_report(self, sync_result: Dict[str, Any]) -> str:
        """
        ë™ê¸°í™” ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            sync_result: ë™ê¸°í™” ê²°ê³¼
            
        Returns:
            ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
        """
        report_lines = [
            "=" * 60,
            "HVDC ë°ì´í„° ë™ê¸°í™” ë¦¬í¬íŠ¸",
            "=" * 60,
            f"ì‹¤í–‰ ì‹œê°„: {sync_result['timestamp']}",
            f"ëª¨ë“œ: {'ì‹œë®¬ë ˆì´ì…˜' if sync_result['dry_run'] else 'ì‹¤ì œ ì—…ë°ì´íŠ¸'}",
            f"ìƒíƒœ: {'ì„±ê³µ' if sync_result['success'] else 'ì‹¤íŒ¨'}",
            ""
        ]
        
        if sync_result.get('backup_path'):
            report_lines.append(f"ë°±ì—… íŒŒì¼: {sync_result['backup_path']}")
            report_lines.append("")
        
        # ë§¤ì¹­ ê²°ê³¼
        if 'matching_results' in sync_result:
            matching = sync_result['matching_results']['summary']
            report_lines.extend([
                "ë§¤ì¹­ ê²°ê³¼:",
                f"  - ì „ì²´ ì†ŒìŠ¤ ì¼€ì´ìŠ¤: {matching['total_source_cases']}",
                f"  - ì •í™•í•œ ë§¤ì¹˜: {matching['exact_matches']}",
                f"  - ìœ ì‚¬ ë§¤ì¹˜: {matching['fuzzy_matches']}",
                f"  - ì‹ ê·œ ì¼€ì´ìŠ¤: {matching['new_cases']}",
                f"  - ëª¨í˜¸í•œ ë§¤ì¹˜: {matching['ambiguous_matches']}",
                f"  - ë§¤ì¹˜ìœ¨: {matching['match_rate']:.1f}%",
                ""
            ])
        
        # ì—…ë°ì´íŠ¸ ìš”ì•½
        if 'update_summary' in sync_result:
            update = sync_result['update_summary']
            report_lines.extend([
                "ì—…ë°ì´íŠ¸ ìš”ì•½:",
                f"  - ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ: {update['updated_records']}",
                f"  - ì‹ ê·œ ë ˆì½”ë“œ: {update['new_records']}",
                f"  - ê±´ë„ˆë›´ ë ˆì½”ë“œ: {update['skipped_records']}",
                f"  - ì´ ë³€ê²½ì‚¬í•­: {update['total_changes']}",
                ""
            ])
            
            # ë‚ ì§œ ì—…ë°ì´íŠ¸ ì„¸ë¶€ì‚¬í•­
            if 'date_updates' in update:
                date_stats = update['date_updates']
                total_date_updates = (date_stats['high_priority_dates'] + 
                                    date_stats['medium_priority_dates'] + 
                                    date_stats['low_priority_dates'])
                
                if total_date_updates > 0:
                    report_lines.extend([
                        "ë‚ ì§œ í•„ë“œ ì—…ë°ì´íŠ¸ (ìµœìš°ì„  ì²˜ë¦¬):",
                        f"  - ì°½ê³ ë³„/í˜„ì¥ë³„ ë‚ ì§œ: {date_stats['high_priority_dates']}ê°œ",
                        f"  - ì¼ë°˜ ë‚ ì§œ í•„ë“œ: {date_stats['medium_priority_dates']}ê°œ", 
                        f"  - ê¸°íƒ€ ì‹œê°„ ê´€ë ¨: {date_stats['low_priority_dates']}ê°œ",
                        f"  - ë‚ ì§œ ì™¸ í•„ë“œ: {date_stats['non_date_fields']}ê°œ",
                        f"  - ë‚ ì§œ ì—…ë°ì´íŠ¸ ë¹„ìœ¨: {(total_date_updates/max(update['total_changes'], 1)*100):.1f}%",
                        ""
                    ])
            
            if update['validation_errors']:
                report_lines.extend([
                    f"ê²€ì¦ ì˜¤ë¥˜ ({len(update['validation_errors'])}ê±´):",
                    *[f"  - {error['error']}" for error in update['validation_errors'][:5]],
                    ""
                ])
        
        # ì´ìŠˆ
        if sync_result.get('issues'):
            report_lines.extend([
                "ë°œìƒí•œ ì´ìŠˆ:",
                *[f"  - {issue}" for issue in sync_result['issues']],
                ""
            ])
        
        report_lines.append("=" * 60)
        
        return "\n".join(report_lines)
