diff --git a/hvdc_pipeline/config/pipeline_config.yaml b/hvdc_pipeline/config/pipeline_config.yaml
index 355087bf5b184628b7c95c3a461f6da7d8355d7c..167380067c4a99877450c22b093414f8eb74af9b 100644
--- a/hvdc_pipeline/config/pipeline_config.yaml
+++ b/hvdc_pipeline/config/pipeline_config.yaml
@@ -12,30 +12,32 @@ stages:
     description: "원본 데이터 동기화 및 정제"
     enabled: true

   stage2:
     name: "Derived Columns"
     description: "13개 파생 컬럼 계산 및 추가"
     enabled: true

   stage3:
     name: "Report Generation"
     description: "종합 보고서 생성"
     enabled: true

   stage4:
     name: "Anomaly Detection"
     description: "이상치 탐지 및 분석"
     enabled: true

 paths:
   data_root: "data"
   scripts_root: "scripts"
   config_root: "config"
   docs_root: "docs"
   logs_root: "logs"
   temp_root: "temp"
+  synced_dir: "data/processed/synced"
+  derived_dir: "data/processed/derived"

 logging:
   level: "INFO"
   format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
   file: "logs/pipeline.log"
diff --git a/hvdc_pipeline/run_pipeline.py b/hvdc_pipeline/run_pipeline.py
index 3c29950c91e3c2d6a457114d531bb72e9d8286c0..9cbb2ace7e3410b173b82c5f73552a9bfadac32d 100644
--- a/hvdc_pipeline/run_pipeline.py
+++ b/hvdc_pipeline/run_pipeline.py
@@ -1,109 +1,118 @@
 #!/usr/bin/env python3
 """
 HVDC 파이프라인 통합 실행 스크립트
 HVDC Pipeline Integrated Execution Script

 전체 파이프라인을 하나의 명령으로 실행할 수 있는 통합 스크립트입니다.
 """

 import argparse
 import sys
 import time
 from pathlib import Path
 from typing import List, Optional

 import yaml

 # 프로젝트 루트 경로 추가
 sys.path.append(str(Path(__file__).parent))

 # 각 Stage 임포트
 try:
-    from scripts.stage2_derived.derived_columns_processor import process_derived_columns
+    from scripts.stage2_derived.derived_columns_processor import (
+        process_derived_columns,
+        resolve_synced_input_path as resolve_stage2_synced_input_path,
+    )

     # 다른 모듈들은 필요시 개별적으로 임포트
 except ImportError as e:
     print(f"ERROR: 모듈 임포트 실패: {e}")
     print("requirements.txt의 패키지들이 설치되었는지 확인하세요.")
     sys.exit(1)


+PROJECT_ROOT = Path(__file__).parent
+PIPELINE_CONFIG_PATH = PROJECT_ROOT / "config" / "pipeline_config.yaml"
+STAGE2_CONFIG_PATH = PROJECT_ROOT / "config" / "stage2_derived_config.yaml"
+
+
 def load_config() -> dict:
     """파이프라인 설정을 로드합니다."""
-    config_path = Path(__file__).parent / "config" / "pipeline_config.yaml"
+    config_path = PIPELINE_CONFIG_PATH
     try:
         with open(config_path, "r", encoding="utf-8") as f:
             return yaml.safe_load(f)
     except FileNotFoundError:
         print(f"WARNING: 설정 파일을 찾을 수 없습니다: {config_path}")
         return {}


 def print_banner():
     """파이프라인 시작 배너를 출력합니다."""
     print("\n" + "=" * 80)
     print("HVDC PIPELINE v2.0 - Integration Execution")
     print("   Samsung C&T Logistics | ADNOC-DSV Partnership")
     print("=" * 80)
     print("Execution Stages:")
     print("   Stage 1: Data Synchronization")
     print("   Stage 2: Derived Columns")
     print("   Stage 3: Report Generation")
     print("   Stage 4: Anomaly Detection")
     print("=" * 80 + "\n")


 def run_stage(stage_num: int, config: dict) -> bool:
     """특정 Stage를 실행합니다."""
     stage_start_time = time.time()

     try:
         if stage_num == 1:
             print("[Stage 1] Data Synchronization...")
+            shared_synced_path = resolve_stage2_synced_input_path(
+                pipeline_config_path=PIPELINE_CONFIG_PATH,
+                stage2_config_path=STAGE2_CONFIG_PATH,
+                project_root=PROJECT_ROOT,
+            )
             print("INFO: Stage 1 requires separate script execution.")
             print("      python scripts/stage1_sync/data_synchronizer.py")
+            print(f"      Output Path: {shared_synced_path}")

         elif stage_num == 2:
             print("[Stage 2] Derived Columns Generation...")
-            # 설정에서 입력 파일 경로 가져오기
-            stage2_config_path = (
-                Path(__file__).parent / "config" / "stage2_derived_config.yaml"
+            shared_synced_path = resolve_stage2_synced_input_path(
+                pipeline_config_path=PIPELINE_CONFIG_PATH,
+                stage2_config_path=STAGE2_CONFIG_PATH,
+                project_root=PROJECT_ROOT,
+            )
+            print(f"INFO: Stage 2 uses synced file: {shared_synced_path}")
+
+            success = process_derived_columns(
+                pipeline_config_path=PIPELINE_CONFIG_PATH,
+                stage2_config_path=STAGE2_CONFIG_PATH,
+                project_root=PROJECT_ROOT,
             )
-            if stage2_config_path.exists():
-                with open(stage2_config_path, "r", encoding="utf-8") as f:
-                    stage2_config = yaml.safe_load(f)
-                input_file = stage2_config.get("input", {}).get(
-                    "synced_file",
-                    "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx",
-                )
-            else:
-                input_file = (
-                    "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
-                )
-
-            success = process_derived_columns(input_file)
             if not success:
                 return False

         elif stage_num == 3:
             print("[Stage 3] Report Generation...")
             print("INFO: Stage 3 requires separate script execution.")
             print("      python scripts/stage3_report/report_generator.py")

         elif stage_num == 4:
             print("[Stage 4] Anomaly Detection...")
             print("INFO: Stage 4 requires separate script execution.")
             print("      python scripts/stage4_anomaly/anomaly_detector.py")

         else:
             print(f"ERROR: 알 수 없는 Stage 번호: {stage_num}")
             return False

         stage_duration = time.time() - stage_start_time
         print(f"[OK] Stage {stage_num} completed (Duration: {stage_duration:.2f}s)\n")
         return True

     except Exception as e:
         print(f"[ERROR] Stage {stage_num} failed: {e}")
         return False

diff --git a/hvdc_pipeline/scripts/stage1_sync/data_synchronizer.py b/hvdc_pipeline/scripts/stage1_sync/data_synchronizer.py
index c967d79a238bc13196bbbf88b8093241217d2091..6f18b8c5d5ad8ec3920029f899882ad2e389eff7 100644
--- a/hvdc_pipeline/scripts/stage1_sync/data_synchronizer.py
+++ b/hvdc_pipeline/scripts/stage1_sync/data_synchronizer.py
@@ -1,73 +1,121 @@
 """
 DataSynchronizer v2.9 (Hard-override Date + Colorize)
 - Master always takes precedence
 - Date columns: always write when Master has value; highlight only when logical date changed
 - Non-date columns: overwrite when Master has non-null (configurable)
 - New cases appended; entire row highlighted yellow
 - ExcelFormatter is called after save to color cells/rows
 """

 from __future__ import annotations
 from dataclasses import dataclass, field
-from typing import Any, Dict, List, Optional, Tuple
 from datetime import datetime
-import pandas as pd
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple
+
 import numpy as np
+import pandas as pd
 import re
-from pathlib import Path
+import yaml
 from openpyxl import load_workbook
-from openpyxl.utils import get_column_letter
 from openpyxl.styles import PatternFill
+from openpyxl.utils import get_column_letter

 # ===== Config =====
 ORANGE = "FFC000"  # changed date cell
 YELLOW = "FFFF00"  # new row
 DATE_KEYS = [
     "ETD/ATD",
     "ETA/ATA",
     "DHL Warehouse",
     "DSV Indoor",
     "DSV Al Markaz",
     "DSV Outdoor",
     "AAA  Storage",
     "Hauler Indoor",
     "DSV MZP",
     "MOSB",
     "Shifting",
     "MIR",
     "SHU",
     "DAS",
     "AGI",
 ]
 ALWAYS_OVERWRITE_NONDATE = True  # Master non-null overwrites
+PROJECT_ROOT = Path(__file__).resolve().parents[2]
+PIPELINE_CONFIG_PATH = PROJECT_ROOT / "config" / "pipeline_config.yaml"
+DEFAULT_SYNCED_DIR = Path("data/processed/synced")
+SYNCED_SUFFIX = "_synced.xlsx"


 def _norm_header(h: str) -> str:
     return re.sub(r"[^a-z0-9]+", "_", str(h).strip().lower())


+def _load_pipeline_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
+    """파이프라인 설정을 로드합니다. / Load the pipeline configuration."""
+
+    target_path = config_path or PIPELINE_CONFIG_PATH
+    if not target_path.exists():
+        return {}
+    with open(target_path, "r", encoding="utf-8") as handle:
+        return yaml.safe_load(handle) or {}
+
+
+def resolve_synced_output_path(
+    warehouse_xlsx: str | Path,
+    *,
+    config_path: Optional[Path] = None,
+    project_root: Optional[Path] = None,
+) -> Path:
+    """동기화 결과 경로를 계산합니다. / Resolve synchronized output path."""
+
+    root = project_root or PROJECT_ROOT
+    config = _load_pipeline_config(config_path=config_path)
+    synced_dir_value = (
+        config.get("paths", {}).get("synced_dir")
+        if isinstance(config, dict)
+        else None
+    )
+    synced_dir = Path(synced_dir_value) if synced_dir_value else DEFAULT_SYNCED_DIR
+    if not synced_dir.is_absolute():
+        synced_dir = root / synced_dir
+
+    if not synced_dir.exists():
+        print(f"INFO: 생성되지 않은 동기화 디렉터리를 생성합니다: {synced_dir}")
+        synced_dir.mkdir(parents=True, exist_ok=True)
+
+    warehouse_name = Path(warehouse_xlsx).stem
+    if warehouse_name.endswith("_synced"):
+        output_name = f"{warehouse_name}.xlsx"
+    else:
+        output_name = f"{warehouse_name}{SYNCED_SUFFIX}"
+
+    return synced_dir / output_name
+
+
 def _is_date_col(col_name: str) -> bool:
     def norm(s: str) -> str:
         # 대소문자/공백/슬래시/하이픈 차이 제거 (예: "ETD/ATD" ≒ "etd atd")
         return re.sub(r"[^a-z0-9]", "", str(s).strip().lower())

     cn = norm(col_name)
     return any(norm(k) == cn for k in DATE_KEYS)


 def _to_date(val) -> Optional[pd.Timestamp]:
     if val is None or (isinstance(val, float) and np.isnan(val)):
         return None
     try:
         if isinstance(val, pd.Timestamp):
             return val
         return pd.to_datetime(val, errors="coerce")
     except Exception:
         return None


 @dataclass
 class Change:
     row_index: int
     column_name: str
     old_value: Any
@@ -209,78 +257,87 @@ class DataSynchronizerV29:
                                 old_value=wval,
                                 new_value=mval,
                                 change_type="date_update",
                             )
                         else:
                             # equal logically — ensure consistent format but don't log
                             wh.at[wi, wcol] = mval
                     # if master is NaN: do nothing
                 else:
                     if ALWAYS_OVERWRITE_NONDATE and pd.notna(mval):
                         if (wval is None) or (str(mval) != str(wval)):
                             stats["updates"] += 1
                             stats["field_updates"] += 1
                             wh.at[wi, wcol] = mval
                             self.change_tracker.add_change(
                                 row_index=wi,
                                 column_name=wcol,
                                 old_value=wval,
                                 new_value=mval,
                                 change_type="field_update",
                             )

         return wh, stats

     def synchronize(
-        self, master_xlsx: str, warehouse_xlsx: str, output_path: Optional[str] = None
+        self,
+        master_xlsx: str,
+        warehouse_xlsx: str,
+        output_path: Optional[str | Path] = None,
     ) -> SyncResult:
         try:
             # Load
             m_xl = pd.ExcelFile(master_xlsx)
             w_xl = pd.ExcelFile(warehouse_xlsx)
             m_df = pd.read_excel(m_xl, sheet_name=m_xl.sheet_names[0])
             w_df = pd.read_excel(w_xl, sheet_name=w_xl.sheet_names[0])

             m_case = self._case_col(m_df)
             w_case = self._case_col(w_df)
             if not (m_case and w_case):
                 return SyncResult(
                     False,
                     "CASE NO column not found.",
                     output_path or warehouse_xlsx,
                     {},
                 )

             updated_w_df, stats = self._apply_updates(m_df, w_df, m_case, w_case)

             # Save to output
-            out = output_path or str(
-                Path(warehouse_xlsx).with_name(
-                    Path(warehouse_xlsx).stem + ".synced.xlsx"
-                )
-            )
-            with pd.ExcelWriter(out, engine="openpyxl") as writer:
+            if output_path is None:
+                out_path = resolve_synced_output_path(warehouse_xlsx)
+            else:
+                out_path = Path(output_path)
+                if not out_path.parent.exists():
+                    print(
+                        "INFO: 지정한 출력 디렉터리가 없어 생성합니다: "
+                        f"{out_path.parent}"
+                    )
+                    out_path.parent.mkdir(parents=True, exist_ok=True)
+
+            with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
                 updated_w_df.to_excel(
                     writer, sheet_name=w_xl.sheet_names[0], index=False
                 )

             # Colorize
             try:
                 from excel_formatter import (
                     ExcelFormatter,
                 )  # expects same folder or PYTHONPATH
             except Exception:
                 try:
                     from .excel_formatter import (
                         ExcelFormatter,
                     )  # package-style import fallback
                 except Exception:
                     # Fallback: create minimal ExcelFormatter
                     class ExcelFormatter:
                         def __init__(
                             self,
                             change_tracker,
                             orange_hex="FFC000",
                             yellow_hex="FFFF00",
                         ):
                             self.ct = change_tracker
                             self.orange = PatternFill(
@@ -348,49 +405,54 @@ class DataSynchronizerV29:

                                 # Apply new records (YELLOW)
                                 painted_rows = set()
                                 for ch in getattr(self.ct, "changes", []) or []:
                                     if (
                                         str(getattr(ch, "change_type", ""))
                                         == "new_record"
                                     ):
                                         row_index = getattr(ch, "row_index", None)
                                         if row_index is None:
                                             continue
                                         excel_row = int(row_index) + header_row + 1
                                         for c in ws[excel_row]:
                                             c.fill = self.yellow
                                         painted_rows.add(excel_row)

                                 wb.save(excel_file_path)
                                 return True
                             except Exception:
                                 return False

             fmt = ExcelFormatter(
                 self.change_tracker, orange_hex=ORANGE, yellow_hex=YELLOW
             )
             fmt.apply_formatting_inplace(
-                out, sheet_name=w_xl.sheet_names[0], header_row=1
+                out_path, sheet_name=w_xl.sheet_names[0], header_row=1
             )

-            stats["output_file"] = out
-            return SyncResult(True, "Sync & colorize done.", out, stats)
+            stats["output_file"] = str(out_path)
+            return SyncResult(True, "Sync & colorize done.", str(out_path), stats)
         except Exception as e:
-            return SyncResult(False, f"Error: {e}", output_path or warehouse_xlsx, {})
+            error_output = (
+                str(output_path)
+                if output_path is not None
+                else str(warehouse_xlsx)
+            )
+            return SyncResult(False, f"Error: {e}", error_output, {})


 if __name__ == "__main__":
     import argparse

     ap = argparse.ArgumentParser()
     ap.add_argument("--master", required=True)
     ap.add_argument("--warehouse", required=True)
     ap.add_argument("--out", default="")
     args = ap.parse_args()

     sync = DataSynchronizerV29()
     res = sync.synchronize(args.master, args.warehouse, args.out or None)
     print("success:", res.success)
     print("message:", res.message)
     print("output:", res.output_path)
     print("stats:", res.stats)
diff --git a/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py b/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
index 30548b8c256a751f6e53acb16e4da17fdd614d0d..be917cd9d314321eaf3dc8f6e50c8cd93cd1c1d7 100644
--- a/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
+++ b/hvdc_pipeline/scripts/stage2_derived/derived_columns_processor.py
@@ -1,70 +1,161 @@
 """
 파생 컬럼 처리기 (Derived Columns Processor)

 동기화된 데이터에서 13개의 파생 컬럼을 자동으로 계산하는 최적화된 스크립트입니다.
 Excel 공식을 Python pandas 벡터화 연산으로 변환하여 고성능 처리를 제공합니다.

 주요 기능:
 - 13개 파생 컬럼 자동 계산
 - 벡터화 연산으로 고성능 처리 (10배 속도 향상)
 - 원본 컬럼명 보존 (site  handling - 공백 2개)
 - 색상 보존 전략 지원

 작성자: AI Development Team
 버전: v2.0
 작성일: 2025-10-19
 """

 from __future__ import annotations

 from pathlib import Path
-from typing import Iterable, Tuple
+from typing import Iterable, Optional, Tuple

 import pandas as pd  # type: ignore[import-untyped]
+import yaml

 from .column_definitions import (
     DERIVED_COLUMNS,
     FINAL_HANDLING_COLUMN,
     MINUS_COLUMN,
     SITE_COLUMNS,
     SITE_HANDLING_COLUMN,
     SQM_COLUMN,
     STACK_STATUS_COLUMN,
     STATUS_CURRENT_COLUMN,
     STATUS_LOCATION_COLUMN,
     STATUS_LOCATION_DATE_COLUMN,
     STATUS_SITE_COLUMN,
     STATUS_STORAGE_COLUMN,
     STATUS_WAREHOUSE_COLUMN,
     TOTAL_HANDLING_COLUMN,
     WAREHOUSE_COLUMNS,
     WH_HANDLING_COLUMN,
 )

 SITE_COLUMN_LOOKUP = {col.lower() for col in SITE_COLUMNS}
 WAREHOUSE_COLUMN_LOOKUP = {col.lower() for col in WAREHOUSE_COLUMNS}
+PROJECT_ROOT = Path(__file__).resolve().parents[2]
+PIPELINE_CONFIG_PATH = PROJECT_ROOT / "config" / "pipeline_config.yaml"
+STAGE2_CONFIG_PATH = PROJECT_ROOT / "config" / "stage2_derived_config.yaml"
+
+
+def _load_yaml_config(path: Path) -> dict:
+    """YAML 설정을 로드합니다. / Load YAML configuration file."""
+
+    if not path.exists():
+        return {}
+    with open(path, "r", encoding="utf-8") as handle:
+        return yaml.safe_load(handle) or {}
+
+
+def load_stage2_config(config_path: Optional[Path] = None) -> dict:
+    """Stage 2 설정을 반환합니다. / Return Stage 2 configuration."""
+
+    target_path = config_path or STAGE2_CONFIG_PATH
+    return _load_yaml_config(target_path)
+
+
+def resolve_synced_input_path(
+    *,
+    pipeline_config_path: Optional[Path] = None,
+    stage2_config_path: Optional[Path] = None,
+    project_root: Optional[Path] = None,
+) -> Path:
+    """Stage 2 동기화 입력 경로를 계산합니다. / Resolve Stage 2 synced input path."""
+
+    root = project_root or PROJECT_ROOT
+    stage2_config = load_stage2_config(config_path=stage2_config_path)
+    candidate = (
+        stage2_config.get("input", {}).get("synced_file")
+        if isinstance(stage2_config, dict)
+        else None
+    )
+
+    if not candidate:
+        pipeline_config = _load_yaml_config(
+            pipeline_config_path or PIPELINE_CONFIG_PATH
+        )
+        synced_dir_value = (
+            pipeline_config.get("paths", {}).get("synced_dir")
+            if isinstance(pipeline_config, dict)
+            else None
+        )
+        synced_dir = Path(synced_dir_value) if synced_dir_value else Path("data/processed/synced")
+        if not synced_dir.is_absolute():
+            synced_dir = root / synced_dir
+        candidate = synced_dir / "HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
+    path = Path(candidate)
+    if not path.is_absolute():
+        path = root / path
+
+    if not path.parent.exists():
+        print(
+            "INFO: Stage 2 입력 폴더가 존재하지 않아 생성합니다: "
+            f"{path.parent}"
+        )
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+    return path
+
+
+def resolve_derived_output_path(
+    *,
+    stage2_config: Optional[dict] = None,
+    project_root: Optional[Path] = None,
+) -> Path:
+    """파생 컬럼 결과 경로를 계산합니다. / Resolve derived output path."""
+
+    root = project_root or PROJECT_ROOT
+    config = stage2_config or load_stage2_config()
+    candidate = (
+        config.get("output", {}).get("derived_file")
+        if isinstance(config, dict)
+        else None
+    )
+    path = Path(candidate) if candidate else Path("data/processed/derived/derived_output.xlsx")
+    if not path.is_absolute():
+        path = root / path
+
+    if not path.parent.exists():
+        print(
+            "INFO: Stage 2 결과 폴더가 존재하지 않아 생성합니다: "
+            f"{path.parent}"
+        )
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+    return path


 def _latest_location_and_date(
     row: pd.Series,
 ) -> Tuple[str | None, pd.Timestamp | pd.NaT]:
     """최근 위치와 날짜를 계산합니다. / Compute the latest location and date."""
     non_null = row.dropna()
     if non_null.empty:
         return None, pd.NaT
     latest_date = non_null.max()
     latest_columns = non_null[non_null == latest_date].index
     return latest_columns[0], latest_date


 def _classify_storage(location: str | None) -> str:
     """위치 기반 보관 유형을 분류합니다. / Classify storage based on location."""
     if location is None or location == "":
         return ""
     if location == "Pre Arrival":
         return "Pre Arrival"
     lowered = location.lower()
     if lowered in SITE_COLUMN_LOOKUP:
         return "site"
     if lowered in WAREHOUSE_COLUMN_LOOKUP:
         return "warehouse"
@@ -184,84 +275,110 @@ def calculate_derived_columns(df: pd.DataFrame) -> pd.DataFrame:
         working_df[SITE_HANDLING_COLUMN] = site_handling
     else:
         working_df[SITE_HANDLING_COLUMN] = 0
     working_df[TOTAL_HANDLING_COLUMN] = (
         working_df[WH_HANDLING_COLUMN] + working_df[SITE_HANDLING_COLUMN]
     )
     working_df[MINUS_COLUMN] = (
         working_df[SITE_HANDLING_COLUMN] - working_df[WH_HANDLING_COLUMN]
     )
     working_df[FINAL_HANDLING_COLUMN] = (
         working_df[TOTAL_HANDLING_COLUMN] + working_df[MINUS_COLUMN]
     )

     if "규격" in working_df.columns and "수량" in working_df.columns:
         working_df[SQM_COLUMN] = (working_df["규격"] * working_df["수량"]) / 10000
     else:
         working_df[SQM_COLUMN] = ""
         print("WARNING: '규격' 또는 '수량' 컬럼이 없어 SQM 계산을 건너뜁니다.")

     working_df[STACK_STATUS_COLUMN] = ""

     return working_df


 def process_derived_columns(
-    input_file: str = "HVDC WAREHOUSE_HITACHI(HE).synced.xlsx",
+    input_file: Optional[str | Path] = None,
+    *,
+    pipeline_config_path: Optional[Path] = None,
+    stage2_config_path: Optional[Path] = None,
+    project_root: Optional[Path] = None,
 ) -> bool:
     """파생 컬럼을 계산합니다. / Process derived columns."""
+    resolved_input_path = (
+        resolve_synced_input_path(
+            pipeline_config_path=pipeline_config_path,
+            stage2_config_path=stage2_config_path,
+            project_root=project_root,
+        )
+        if input_file is None
+        else Path(input_file)
+    )
+
+    root = project_root or PROJECT_ROOT
+    if not resolved_input_path.is_absolute():
+        resolved_input_path = root / resolved_input_path
+
     print("=== 파생 컬럼 처리 시작 ===")
-    print(f"입력 파일: {input_file}")
+    print(f"입력 파일: {resolved_input_path}")

     # 파일 존재 확인
-    if not Path(input_file).exists():
-        raise FileNotFoundError(f"입력 파일을 찾을 수 없습니다: {input_file}")
+    if not resolved_input_path.exists():
+        raise FileNotFoundError(
+            f"입력 파일을 찾을 수 없습니다: {resolved_input_path}"
+        )

     # 데이터 로드
-    df = pd.read_excel(input_file)
+    df = pd.read_excel(resolved_input_path)
     print(f"원본 데이터 로드 완료: {len(df)}행, {len(df.columns)}컬럼")

     df = calculate_derived_columns(df)

     wh_cols = [c for c in WAREHOUSE_COLUMNS if c in df.columns]
     st_cols = [c for c in SITE_COLUMNS if c in df.columns]

     print(f"Warehouse 컬럼: {len(wh_cols)}개 - {wh_cols}")
     print(f"Site 컬럼: {len(st_cols)}개 - {st_cols}")

     print(
         "SUCCESS: 파생 컬럼 %s개 계산 완료 (행: %s, 컬럼: %s)"
         % (len(DERIVED_COLUMNS), len(df), len(df.columns))
     )

-    # 결과 저장
-    output_file = "HVDC WAREHOUSE_HITACHI(HE).xlsx"
-    df.to_excel(output_file, index=False)
-    print(f"SUCCESS: 파일 저장 완료: {output_file}")
+    stage2_config = load_stage2_config(config_path=stage2_config_path)
+    output_path = resolve_derived_output_path(
+        stage2_config=stage2_config, project_root=root
+    )
+    df.to_excel(output_path, index=False)
+    print(f"SUCCESS: 파일 저장 완료: {output_path}")

     return True


 def main() -> int:
     """메인 실행을 수행합니다. / Execute script entry point."""
     try:
         success = process_derived_columns()
         if success:
+            stage2_config = load_stage2_config()
+            derived_path = resolve_derived_output_path(
+                stage2_config=stage2_config
+            )
             print("\n" + "=" * 60)
             print("SUCCESS: 파생 컬럼 처리 완료!")
-            print("FILE: 결과 파일: HVDC WAREHOUSE_HITACHI(HE).xlsx")
+            print(f"FILE: 결과 파일: {derived_path}")
             print("INFO: 색상은 Step 1에서 이미 적용되었습니다.")
             print("=" * 60)
         else:
             print("ERROR: 처리 실패")
             return 1
     except Exception as e:
         print(f"ERROR: 오류 발생: {e}")
         return 1

     return 0


 if __name__ == "__main__":
     exit(main())

     exit(main())
diff --git a/tests/test_synced_paths.py b/tests/test_synced_paths.py
new file mode 100644
index 0000000000000000000000000000000000000000..5b3ad33b27ed50adb370b7aa11387f19e2485a23
--- /dev/null
+++ b/tests/test_synced_paths.py
@@ -0,0 +1,139 @@
+from pathlib import Path
+import importlib.util
+import sys
+import textwrap
+import types
+
+import pytest
+
+REPO_ROOT = Path(__file__).resolve().parents[1]
+HVDC_ROOT = REPO_ROOT / "hvdc_pipeline"
+
+scripts_pkg = types.ModuleType("scripts")
+scripts_pkg.__path__ = [str(HVDC_ROOT / "scripts")]
+sys.modules.setdefault("scripts", scripts_pkg)
+
+stage1_pkg = types.ModuleType("scripts.stage1_sync")
+stage1_pkg.__path__ = [str(HVDC_ROOT / "scripts/stage1_sync")]
+sys.modules.setdefault("scripts.stage1_sync", stage1_pkg)
+
+stage2_pkg = types.ModuleType("scripts.stage2_derived")
+stage2_pkg.__path__ = [str(HVDC_ROOT / "scripts/stage2_derived")]
+sys.modules.setdefault("scripts.stage2_derived", stage2_pkg)
+
+
+def _load_module(module_name: str, module_path: Path):
+    spec = importlib.util.spec_from_file_location(module_name, module_path)
+    if spec is None or spec.loader is None:
+        raise ImportError(f"Cannot load module {module_name} from {module_path}")
+    module = importlib.util.module_from_spec(spec)
+    sys.modules[module_name] = module
+    spec.loader.exec_module(module)
+    return module
+
+
+stage1_sync = _load_module(
+    "scripts.stage1_sync.data_synchronizer",
+    HVDC_ROOT / "scripts/stage1_sync/data_synchronizer.py",
+)
+stage2_derived = _load_module(
+    "scripts.stage2_derived.derived_columns_processor",
+    HVDC_ROOT / "scripts/stage2_derived/derived_columns_processor.py",
+)
+run_pipeline = _load_module(
+    "run_pipeline",
+    HVDC_ROOT / "run_pipeline.py",
+)
+
+
+@pytest.fixture()
+def pipeline_and_stage2_config(tmp_path: Path) -> tuple[Path, Path, Path]:
+    project_root = tmp_path
+    pipeline_config_path = project_root / "pipeline_config.yaml"
+    stage2_config_path = project_root / "stage2_config.yaml"
+
+    pipeline_config_path.write_text(
+        textwrap.dedent(
+            """
+            paths:
+              synced_dir: "data/processed/synced"
+              derived_dir: "data/processed/derived"
+            """
+        ).strip()
+        + "\n",
+        encoding="utf-8",
+    )
+
+    stage2_config_path.write_text(
+        textwrap.dedent(
+            """
+            input:
+              synced_file: "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
+            output:
+              derived_file: "data/processed/derived/HVDC_WAREHOUSE_HITACHI_HE_derived.xlsx"
+            """
+        ).strip()
+        + "\n",
+        encoding="utf-8",
+    )
+
+    return project_root, pipeline_config_path, stage2_config_path
+
+
+def test_stage1_and_stage2_share_synced_path(pipeline_and_stage2_config: tuple[Path, Path, Path], monkeypatch: pytest.MonkeyPatch) -> None:
+    project_root, pipeline_config_path, stage2_config_path = pipeline_and_stage2_config
+
+    warehouse_dir = project_root / "warehouse"
+    warehouse_dir.mkdir(parents=True, exist_ok=True)
+    warehouse_file = warehouse_dir / "HVDC_WAREHOUSE_HITACHI_HE.xlsx"
+    warehouse_file.touch()
+
+    monkeypatch.setattr(stage1_sync, "PROJECT_ROOT", project_root)
+    monkeypatch.setattr(stage1_sync, "PIPELINE_CONFIG_PATH", pipeline_config_path)
+
+    monkeypatch.setattr(stage2_derived, "PROJECT_ROOT", project_root)
+    monkeypatch.setattr(stage2_derived, "PIPELINE_CONFIG_PATH", pipeline_config_path)
+    monkeypatch.setattr(stage2_derived, "STAGE2_CONFIG_PATH", stage2_config_path)
+
+    stage1_path = stage1_sync.resolve_synced_output_path(
+        warehouse_file,
+        config_path=pipeline_config_path,
+        project_root=project_root,
+    )
+    stage2_path = stage2_derived.resolve_synced_input_path(
+        pipeline_config_path=pipeline_config_path,
+        stage2_config_path=stage2_config_path,
+        project_root=project_root,
+    )
+
+    assert stage1_path == stage2_path
+    assert stage1_path.name.endswith("_synced.xlsx")
+    assert stage1_path.parent.exists()
+
+
+def test_run_pipeline_uses_shared_synced_path(
+    pipeline_and_stage2_config: tuple[Path, Path, Path], monkeypatch: pytest.MonkeyPatch
+) -> None:
+    project_root, pipeline_config_path, stage2_config_path = pipeline_and_stage2_config
+
+    shared_path = project_root / "data/processed/synced/HVDC_WAREHOUSE_HITACHI_HE_synced.xlsx"
+    shared_path.parent.mkdir(parents=True, exist_ok=True)
+
+    monkeypatch.setattr(run_pipeline, "PROJECT_ROOT", project_root)
+    monkeypatch.setattr(run_pipeline, "PIPELINE_CONFIG_PATH", pipeline_config_path)
+    monkeypatch.setattr(run_pipeline, "STAGE2_CONFIG_PATH", stage2_config_path)
+
+    calls: list[tuple[Path, Path, Path]] = []
+
+    def fake_resolve(*, pipeline_config_path: Path, stage2_config_path: Path, project_root: Path) -> Path:
+        calls.append((pipeline_config_path, stage2_config_path, project_root))
+        return shared_path
+
+    monkeypatch.setattr(run_pipeline, "resolve_stage2_synced_input_path", fake_resolve)
+
+    monkeypatch.setattr(run_pipeline, "process_derived_columns", lambda **_: True)
+
+    assert run_pipeline.run_stage(1, {}) is True
+    assert run_pipeline.run_stage(2, {}) is True
+    assert calls[0] == calls[1]
+    assert calls[0] == (pipeline_config_path, stage2_config_path, project_root)
